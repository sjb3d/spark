//! Generated from vk.xml version 1.4.344

#![allow(
    clippy::too_many_arguments,
    clippy::trivially_copy_pass_by_ref,
    clippy::missing_safety_doc,
    clippy::unnecessary_cast
)]
#![allow(unsafe_op_in_unsafe_fn)]

pub mod builder;
pub mod vk;

use lazy_static::lazy_static;
use shared_library::dynamic_library::DynamicLibrary;
use std::{
    ffi::CStr,
    mem::{self, MaybeUninit},
    os::raw::{c_int, c_void},
    path::Path,
    ptr, result, slice,
};

#[doc(no_inline)]
pub use self::builder::*;

pub type Result<T> = result::Result<T, vk::Result>;

struct Lib {
    _lib: DynamicLibrary,
    fp_get_instance_proc_addr: vk::FnGetInstanceProcAddr,
}

#[derive(Debug, Clone)]
pub enum LoadError {
    DynamicLibrary(String),
    MissingSymbol(&'static CStr),
    Vulkan(vk::Result),
}

impl From<vk::Result> for LoadError {
    fn from(err: vk::Result) -> Self {
        LoadError::Vulkan(err)
    }
}

pub type LoadResult<T> = result::Result<T, LoadError>;

#[cfg(target_os = "linux")]
const DL_PATH: &str = "libvulkan.so.1";

#[cfg(target_os = "windows")]
const DL_PATH: &str = "vulkan-1.dll";

#[cfg(target_os = "android")]
const DL_PATH: &str = "libvulkan.so";

#[cfg(target_os = "macos")]
const DL_PATH: &str = "libvulkan.1.dylib";

impl Lib {
    pub fn new() -> LoadResult<Self> {
        match DynamicLibrary::open(Some(Path::new(&DL_PATH))) {
            Ok(lib) => match unsafe {
                lib.symbol("vkGetInstanceProcAddr")
                    .map(|f: *mut c_void| mem::transmute(f))
            } {
                Ok(fp_get_instance_proc_addr) => Ok(Self {
                    _lib: lib,
                    fp_get_instance_proc_addr,
                }),
                Err(s) => Err(LoadError::DynamicLibrary(s)),
            },
            Err(s) => Err(LoadError::DynamicLibrary(s)),
        }
    }
}

lazy_static! {
    static ref LIB: LoadResult<Lib> = Lib::new();
}

#[derive(Debug, Copy, Clone, PartialOrd, Ord, PartialEq, Eq, Hash)]
pub enum EnumerateResult {
    Success,
    Incomplete,
}

unsafe fn enumerate_generic_to_vec<T>(
    enumerator: impl Fn(&mut u32, *mut T) -> Result<EnumerateResult>,
) -> Result<Vec<T>> {
    let mut v = Vec::new();
    loop {
        let mut capacity = 0;
        match enumerator(&mut capacity, ptr::null_mut())? {
            EnumerateResult::Success => {}
            EnumerateResult::Incomplete => return Err(vk::Result::ERROR_UNKNOWN),
        };
        v.reserve(capacity as usize);

        let mut len = capacity;
        let values_res = enumerator(&mut len, v.as_mut_ptr())?;
        match values_res {
            EnumerateResult::Success => {
                v.set_len(len as usize);
                return Ok(v);
            }
            EnumerateResult::Incomplete => {}
        }
    }
}

unsafe fn enumerate_generic_unchecked_to_vec<T>(enumerator: impl Fn(&mut u32, *mut T)) -> Vec<T> {
    let mut len = 0;
    enumerator(&mut len, ptr::null_mut());

    let mut v = Vec::with_capacity(len as usize);
    enumerator(&mut len, v.as_mut_ptr());
    v.set_len(len as usize);
    v
}

#[derive(Copy, Clone)]
pub struct Globals {
    pub fp_create_instance: vk::FnCreateInstance,
    pub fp_get_instance_proc_addr: vk::FnGetInstanceProcAddr,
    pub fp_enumerate_instance_version: Option<vk::FnEnumerateInstanceVersion>,
    pub fp_enumerate_instance_layer_properties: vk::FnEnumerateInstanceLayerProperties,
    pub fp_enumerate_instance_extension_properties: vk::FnEnumerateInstanceExtensionProperties,
}
impl Globals {
    pub unsafe fn enumerate_instance_version(&self) -> Result<vk::Version> {
        if let Some(fp) = self.fp_enumerate_instance_version {
            let mut p_api_version = MaybeUninit::<_>::uninit();
            let err = (fp)(p_api_version.as_mut_ptr());
            match err {
                vk::Result::SUCCESS => Ok(p_api_version.assume_init()),
                _ => Err(err),
            }
        } else {
            Ok(vk::Version::default())
        }
    }
    pub unsafe fn create_instance_commands(
        &self,
        p_create_info: &vk::InstanceCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> LoadResult<Instance> {
        let instance = self.create_instance(p_create_info, p_allocator)?;
        Instance::load(self, instance, p_create_info)
    }
    pub fn new() -> LoadResult<Self> {
        let lib = LIB.as_ref().map_err(|e| e.clone())?;
        unsafe { Self::load(lib.fp_get_instance_proc_addr) }
    }
    pub unsafe fn load(get_instance_proc_addr: vk::FnGetInstanceProcAddr) -> LoadResult<Self> {
        Ok(Self {
            fp_create_instance: mem::transmute(
                get_instance_proc_addr(vk::Instance::null(), c"vkCreateInstance".as_ptr())
                    .ok_or(LoadError::MissingSymbol(c"vkCreateInstance"))?,
            ),
            fp_get_instance_proc_addr: get_instance_proc_addr,
            fp_enumerate_instance_version: get_instance_proc_addr(
                vk::Instance::null(),
                c"vkEnumerateInstanceVersion".as_ptr(),
            )
            .map(|f| mem::transmute(f)),
            fp_enumerate_instance_layer_properties: mem::transmute(
                get_instance_proc_addr(vk::Instance::null(), c"vkEnumerateInstanceLayerProperties".as_ptr())
                    .ok_or(LoadError::MissingSymbol(c"vkEnumerateInstanceLayerProperties"))?,
            ),
            fp_enumerate_instance_extension_properties: mem::transmute(
                get_instance_proc_addr(vk::Instance::null(), c"vkEnumerateInstanceExtensionProperties".as_ptr())
                    .ok_or(LoadError::MissingSymbol(c"vkEnumerateInstanceExtensionProperties"))?,
            ),
        })
    }
    pub unsafe fn create_instance(
        &self,
        p_create_info: &vk::InstanceCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Instance> {
        let fp = self.fp_create_instance;
        let mut p_instance = MaybeUninit::<_>::uninit();
        let err = (fp)(
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_instance.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_instance.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_instance_proc_addr(&self, instance: vk::Instance, p_name: &CStr) -> Option<vk::FnVoidFunction> {
        let fp = self.fp_get_instance_proc_addr;
        (fp)(instance, p_name.as_ptr())
    }
    pub unsafe fn enumerate_instance_layer_properties(
        &self,
        p_property_count: &mut u32,
        p_properties: *mut vk::LayerProperties,
    ) -> Result<EnumerateResult> {
        let fp = self.fp_enumerate_instance_layer_properties;
        let err = (fp)(p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_instance_layer_properties_to_vec(&self) -> Result<Vec<vk::LayerProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_instance_layer_properties(len, ptr))
    }
    pub unsafe fn enumerate_instance_extension_properties(
        &self,
        p_layer_name: Option<&CStr>,
        p_property_count: &mut u32,
        p_properties: *mut vk::ExtensionProperties,
    ) -> Result<EnumerateResult> {
        let fp = self.fp_enumerate_instance_extension_properties;
        let err = (fp)(
            p_layer_name.map_or(ptr::null(), |s| s.as_ptr()),
            p_property_count,
            p_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_instance_extension_properties_to_vec(
        &self,
        p_layer_name: Option<&CStr>,
    ) -> Result<Vec<vk::ExtensionProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_instance_extension_properties(p_layer_name, len, ptr))
    }
}
#[derive(Debug, Copy, Clone)]
pub struct InstanceExtensions {
    pub core_version: vk::Version,
    pub khr_surface: bool,
    pub khr_display: bool,
    pub khr_xlib_surface: bool,
    pub khr_xcb_surface: bool,
    pub khr_wayland_surface: bool,
    pub khr_android_surface: bool,
    pub khr_win32_surface: bool,
    pub ext_debug_report: bool,
    pub nv_external_memory_capabilities: bool,
    pub khr_get_physical_device_properties2: bool,
    pub ext_validation_flags: bool,
    pub nn_vi_surface: bool,
    pub khr_device_group_creation: bool,
    pub khr_external_memory_capabilities: bool,
    pub khr_external_semaphore_capabilities: bool,
    pub ext_direct_mode_display: bool,
    pub ext_acquire_xlib_display: bool,
    pub ext_display_surface_counter: bool,
    pub ext_swapchain_colorspace: bool,
    pub khr_external_fence_capabilities: bool,
    pub khr_get_surface_capabilities2: bool,
    pub khr_get_display_properties2: bool,
    pub mvk_ios_surface: bool,
    pub mvk_macos_surface: bool,
    pub ext_debug_utils: bool,
    pub fuchsia_imagepipe_surface: bool,
    pub ext_metal_surface: bool,
    pub khr_surface_protected_capabilities: bool,
    pub ext_validation_features: bool,
    pub ext_headless_surface: bool,
    pub ext_surface_maintenance1: bool,
    pub ext_acquire_drm_display: bool,
    pub ext_directfb_surface: bool,
    pub khr_portability_enumeration: bool,
    pub google_surfaceless_query: bool,
    pub lunarg_direct_driver_loading: bool,
    pub khr_surface_maintenance1: bool,
    pub ext_layer_settings: bool,
    pub nv_display_stereo: bool,
    pub ohos_surface: bool,
    pub sec_ubm_surface: bool,
}
impl InstanceExtensions {
    fn enable_by_name(&mut self, name: &CStr) {
        if name == c"VK_KHR_surface" {
            self.khr_surface = true;
        } else if name == c"VK_KHR_display" {
            self.khr_display = true;
        } else if name == c"VK_KHR_xlib_surface" {
            self.khr_xlib_surface = true;
        } else if name == c"VK_KHR_xcb_surface" {
            self.khr_xcb_surface = true;
        } else if name == c"VK_KHR_wayland_surface" {
            self.khr_wayland_surface = true;
        } else if name == c"VK_KHR_android_surface" {
            self.khr_android_surface = true;
        } else if name == c"VK_KHR_win32_surface" {
            self.khr_win32_surface = true;
        } else if name == c"VK_EXT_debug_report" {
            self.ext_debug_report = true;
        } else if name == c"VK_NV_external_memory_capabilities" {
            self.nv_external_memory_capabilities = true;
        } else if name == c"VK_KHR_get_physical_device_properties2" {
            self.khr_get_physical_device_properties2 = true;
        } else if name == c"VK_EXT_validation_flags" {
            self.ext_validation_flags = true;
        } else if name == c"VK_NN_vi_surface" {
            self.nn_vi_surface = true;
        } else if name == c"VK_KHR_device_group_creation" {
            self.khr_device_group_creation = true;
        } else if name == c"VK_KHR_external_memory_capabilities" {
            self.khr_external_memory_capabilities = true;
        } else if name == c"VK_KHR_external_semaphore_capabilities" {
            self.khr_external_semaphore_capabilities = true;
        } else if name == c"VK_EXT_direct_mode_display" {
            self.ext_direct_mode_display = true;
        } else if name == c"VK_EXT_acquire_xlib_display" {
            self.ext_acquire_xlib_display = true;
        } else if name == c"VK_EXT_display_surface_counter" {
            self.ext_display_surface_counter = true;
        } else if name == c"VK_EXT_swapchain_colorspace" {
            self.ext_swapchain_colorspace = true;
        } else if name == c"VK_KHR_external_fence_capabilities" {
            self.khr_external_fence_capabilities = true;
        } else if name == c"VK_KHR_get_surface_capabilities2" {
            self.khr_get_surface_capabilities2 = true;
        } else if name == c"VK_KHR_get_display_properties2" {
            self.khr_get_display_properties2 = true;
        } else if name == c"VK_MVK_ios_surface" {
            self.mvk_ios_surface = true;
        } else if name == c"VK_MVK_macos_surface" {
            self.mvk_macos_surface = true;
        } else if name == c"VK_EXT_debug_utils" {
            self.ext_debug_utils = true;
        } else if name == c"VK_FUCHSIA_imagepipe_surface" {
            self.fuchsia_imagepipe_surface = true;
        } else if name == c"VK_EXT_metal_surface" {
            self.ext_metal_surface = true;
        } else if name == c"VK_KHR_surface_protected_capabilities" {
            self.khr_surface_protected_capabilities = true;
        } else if name == c"VK_EXT_validation_features" {
            self.ext_validation_features = true;
        } else if name == c"VK_EXT_headless_surface" {
            self.ext_headless_surface = true;
        } else if name == c"VK_EXT_surface_maintenance1" {
            self.ext_surface_maintenance1 = true;
        } else if name == c"VK_EXT_acquire_drm_display" {
            self.ext_acquire_drm_display = true;
        } else if name == c"VK_EXT_directfb_surface" {
            self.ext_directfb_surface = true;
        } else if name == c"VK_KHR_portability_enumeration" {
            self.khr_portability_enumeration = true;
        } else if name == c"VK_GOOGLE_surfaceless_query" {
            self.google_surfaceless_query = true;
        } else if name == c"VK_LUNARG_direct_driver_loading" {
            self.lunarg_direct_driver_loading = true;
        } else if name == c"VK_KHR_surface_maintenance1" {
            self.khr_surface_maintenance1 = true;
        } else if name == c"VK_EXT_layer_settings" {
            self.ext_layer_settings = true;
        } else if name == c"VK_NV_display_stereo" {
            self.nv_display_stereo = true;
        } else if name == c"VK_OHOS_surface" {
            self.ohos_surface = true;
        } else if name == c"VK_SEC_ubm_surface" {
            self.sec_ubm_surface = true;
        }
    }
    pub fn new(core_version: vk::Version) -> Self {
        Self {
            core_version,
            khr_surface: false,
            khr_display: false,
            khr_xlib_surface: false,
            khr_xcb_surface: false,
            khr_wayland_surface: false,
            khr_android_surface: false,
            khr_win32_surface: false,
            ext_debug_report: false,
            nv_external_memory_capabilities: false,
            khr_get_physical_device_properties2: false,
            ext_validation_flags: false,
            nn_vi_surface: false,
            khr_device_group_creation: false,
            khr_external_memory_capabilities: false,
            khr_external_semaphore_capabilities: false,
            ext_direct_mode_display: false,
            ext_acquire_xlib_display: false,
            ext_display_surface_counter: false,
            ext_swapchain_colorspace: false,
            khr_external_fence_capabilities: false,
            khr_get_surface_capabilities2: false,
            khr_get_display_properties2: false,
            mvk_ios_surface: false,
            mvk_macos_surface: false,
            ext_debug_utils: false,
            fuchsia_imagepipe_surface: false,
            ext_metal_surface: false,
            khr_surface_protected_capabilities: false,
            ext_validation_features: false,
            ext_headless_surface: false,
            ext_surface_maintenance1: false,
            ext_acquire_drm_display: false,
            ext_directfb_surface: false,
            khr_portability_enumeration: false,
            google_surfaceless_query: false,
            lunarg_direct_driver_loading: false,
            khr_surface_maintenance1: false,
            ext_layer_settings: false,
            nv_display_stereo: false,
            ohos_surface: false,
            sec_ubm_surface: false,
        }
    }
    pub fn from_properties(core_version: vk::Version, properties: &[vk::ExtensionProperties]) -> Self {
        let mut ext = Self::new(core_version);
        for ep in properties.iter() {
            if ep.extension_name.iter().any(|&c| c == 0) {
                let name = unsafe { CStr::from_ptr(ep.extension_name.as_ptr()) };
                ext.enable_by_name(name);
            }
        }
        ext
    }
    pub fn supports_khr_surface(&self) -> bool {
        self.khr_surface
    }
    pub fn enable_khr_surface(&mut self) {
        self.khr_surface = true;
    }
    pub fn supports_khr_swapchain(&self) -> bool {
        self.supports_khr_surface()
    }
    pub fn enable_khr_swapchain(&mut self) {
        self.enable_khr_surface();
    }
    pub fn supports_khr_display(&self) -> bool {
        self.khr_display && self.supports_khr_surface()
    }
    pub fn enable_khr_display(&mut self) {
        self.khr_display = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_display_swapchain(&self) -> bool {
        self.supports_khr_swapchain() && self.supports_khr_display()
    }
    pub fn enable_khr_display_swapchain(&mut self) {
        self.enable_khr_swapchain();
        self.enable_khr_display();
    }
    pub fn supports_khr_xlib_surface(&self) -> bool {
        self.khr_xlib_surface && self.supports_khr_surface()
    }
    pub fn enable_khr_xlib_surface(&mut self) {
        self.khr_xlib_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_xcb_surface(&self) -> bool {
        self.khr_xcb_surface && self.supports_khr_surface()
    }
    pub fn enable_khr_xcb_surface(&mut self) {
        self.khr_xcb_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_wayland_surface(&self) -> bool {
        self.khr_wayland_surface && self.supports_khr_surface()
    }
    pub fn enable_khr_wayland_surface(&mut self) {
        self.khr_wayland_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_android_surface(&self) -> bool {
        self.khr_android_surface && self.supports_khr_surface()
    }
    pub fn enable_khr_android_surface(&mut self) {
        self.khr_android_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_win32_surface(&self) -> bool {
        self.khr_win32_surface && self.supports_khr_surface()
    }
    pub fn enable_khr_win32_surface(&mut self) {
        self.khr_win32_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_debug_report(&self) -> bool {
        self.ext_debug_report
    }
    pub fn enable_ext_debug_report(&mut self) {
        self.ext_debug_report = true;
    }
    pub fn supports_ext_debug_marker(&self) -> bool {
        self.supports_ext_debug_report()
    }
    pub fn enable_ext_debug_marker(&mut self) {
        self.enable_ext_debug_report();
    }
    pub fn supports_ext_transform_feedback(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_transform_feedback(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amd_texture_gather_bias_lod(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_amd_texture_gather_bias_lod(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_dynamic_rendering(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_depth_stencil_resolve())
    }
    pub fn enable_khr_dynamic_rendering(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_depth_stencil_resolve();
        }
    }
    pub fn supports_nv_corner_sampled_image(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_corner_sampled_image(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_multiview(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_multiview(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_external_memory_capabilities(&self) -> bool {
        self.nv_external_memory_capabilities
    }
    pub fn enable_nv_external_memory_capabilities(&mut self) {
        self.nv_external_memory_capabilities = true;
    }
    pub fn supports_nv_external_memory(&self) -> bool {
        self.supports_nv_external_memory_capabilities()
    }
    pub fn enable_nv_external_memory(&mut self) {
        self.enable_nv_external_memory_capabilities();
    }
    pub fn supports_nv_external_memory_win32(&self) -> bool {
        self.supports_nv_external_memory()
    }
    pub fn enable_nv_external_memory_win32(&mut self) {
        self.enable_nv_external_memory();
    }
    pub fn supports_nv_win32_keyed_mutex(&self) -> bool {
        self.supports_nv_external_memory_win32()
    }
    pub fn enable_nv_win32_keyed_mutex(&mut self) {
        self.enable_nv_external_memory_win32();
    }
    pub fn supports_khr_get_physical_device_properties2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_get_physical_device_properties2
    }
    pub fn enable_khr_get_physical_device_properties2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_get_physical_device_properties2 = true;
        }
    }
    pub fn supports_khr_device_group(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_device_group_creation()
    }
    pub fn enable_khr_device_group(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_device_group_creation();
        }
    }
    pub fn supports_ext_validation_flags(&self) -> bool {
        self.ext_validation_flags
    }
    pub fn enable_ext_validation_flags(&mut self) {
        self.ext_validation_flags = true;
    }
    pub fn supports_nn_vi_surface(&self) -> bool {
        self.nn_vi_surface && self.supports_khr_surface()
    }
    pub fn enable_nn_vi_surface(&mut self) {
        self.nn_vi_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_texture_compression_astc_hdr(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_texture_compression_astc_hdr(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_astc_decode_mode(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_astc_decode_mode(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_pipeline_robustness(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_pipeline_robustness(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_device_group_creation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_device_group_creation
    }
    pub fn enable_khr_device_group_creation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_device_group_creation = true;
        }
    }
    pub fn supports_khr_external_memory_capabilities(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_external_memory_capabilities
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_get_physical_device_properties2()))
    }
    pub fn enable_khr_external_memory_capabilities(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_memory_capabilities = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
        }
    }
    pub fn supports_khr_external_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory_capabilities()
    }
    pub fn enable_khr_external_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory_capabilities();
        }
    }
    pub fn supports_khr_external_memory_win32(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_khr_external_memory_win32(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_external_memory_fd(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_khr_external_memory_fd(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_win32_keyed_mutex(&self) -> bool {
        self.supports_khr_external_memory_win32()
    }
    pub fn enable_khr_win32_keyed_mutex(&mut self) {
        self.enable_khr_external_memory_win32();
    }
    pub fn supports_khr_external_semaphore_capabilities(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_external_semaphore_capabilities
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_get_physical_device_properties2()))
    }
    pub fn enable_khr_external_semaphore_capabilities(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_semaphore_capabilities = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
        }
    }
    pub fn supports_khr_external_semaphore(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_semaphore_capabilities()
    }
    pub fn enable_khr_external_semaphore(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_semaphore_capabilities();
        }
    }
    pub fn supports_khr_external_semaphore_win32(&self) -> bool {
        self.supports_khr_external_semaphore()
    }
    pub fn enable_khr_external_semaphore_win32(&mut self) {
        self.enable_khr_external_semaphore();
    }
    pub fn supports_khr_external_semaphore_fd(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_semaphore()
    }
    pub fn enable_khr_external_semaphore_fd(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_semaphore();
        }
    }
    pub fn supports_khr_push_descriptor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_push_descriptor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_conditional_rendering(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_conditional_rendering(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_float16_int8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_float16_int8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_16bit_storage(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_16bit_storage(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_incremental_present(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_khr_incremental_present(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_direct_mode_display(&self) -> bool {
        self.ext_direct_mode_display && self.supports_khr_display()
    }
    pub fn enable_ext_direct_mode_display(&mut self) {
        self.ext_direct_mode_display = true;
        self.enable_khr_display();
    }
    pub fn supports_ext_acquire_xlib_display(&self) -> bool {
        self.ext_acquire_xlib_display && self.supports_ext_direct_mode_display()
    }
    pub fn enable_ext_acquire_xlib_display(&mut self) {
        self.ext_acquire_xlib_display = true;
        self.enable_ext_direct_mode_display();
    }
    pub fn supports_ext_display_surface_counter(&self) -> bool {
        self.ext_display_surface_counter && self.supports_khr_display()
    }
    pub fn enable_ext_display_surface_counter(&mut self) {
        self.ext_display_surface_counter = true;
        self.enable_khr_display();
    }
    pub fn supports_ext_display_control(&self) -> bool {
        self.supports_ext_display_surface_counter() && self.supports_khr_swapchain()
    }
    pub fn enable_ext_display_control(&mut self) {
        self.enable_ext_display_surface_counter();
        self.enable_khr_swapchain();
    }
    pub fn supports_google_display_timing(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_google_display_timing(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_nvx_multiview_per_view_attributes(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_multiview()
    }
    pub fn enable_nvx_multiview_per_view_attributes(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_multiview();
        }
    }
    pub fn supports_ext_discard_rectangles(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_discard_rectangles(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_conservative_rasterization(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_conservative_rasterization(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_depth_clip_enable(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_depth_clip_enable(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_swapchain_colorspace(&self) -> bool {
        self.ext_swapchain_colorspace && self.supports_khr_surface()
    }
    pub fn enable_ext_swapchain_colorspace(&mut self) {
        self.ext_swapchain_colorspace = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_hdr_metadata(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_ext_hdr_metadata(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_imageless_framebuffer(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_imageless_framebuffer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_create_renderpass2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_multiview()
    }
    pub fn enable_khr_create_renderpass2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_multiview();
        }
    }
    pub fn supports_img_relaxed_line_rasterization(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_img_relaxed_line_rasterization(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shared_presentable_image(&self) -> bool {
        self.supports_khr_swapchain()
            && self.supports_khr_get_surface_capabilities2()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
    }
    pub fn enable_khr_shared_presentable_image(&mut self) {
        self.enable_khr_swapchain();
        self.enable_khr_get_surface_capabilities2();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_external_fence_capabilities(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_external_fence_capabilities
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_get_physical_device_properties2()))
    }
    pub fn enable_khr_external_fence_capabilities(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_fence_capabilities = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
        }
    }
    pub fn supports_khr_external_fence(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_fence_capabilities()
    }
    pub fn enable_khr_external_fence(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_fence_capabilities();
        }
    }
    pub fn supports_khr_external_fence_win32(&self) -> bool {
        self.supports_khr_external_fence()
    }
    pub fn enable_khr_external_fence_win32(&mut self) {
        self.enable_khr_external_fence();
    }
    pub fn supports_khr_external_fence_fd(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_fence()
    }
    pub fn enable_khr_external_fence_fd(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_fence();
        }
    }
    pub fn supports_khr_performance_query(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_performance_query(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_get_surface_capabilities2(&self) -> bool {
        self.khr_get_surface_capabilities2 && self.supports_khr_surface()
    }
    pub fn enable_khr_get_surface_capabilities2(&mut self) {
        self.khr_get_surface_capabilities2 = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_variable_pointers(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_variable_pointers(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_get_display_properties2(&self) -> bool {
        self.khr_get_display_properties2 && self.supports_khr_display()
    }
    pub fn enable_khr_get_display_properties2(&mut self) {
        self.khr_get_display_properties2 = true;
        self.enable_khr_display();
    }
    pub fn supports_mvk_ios_surface(&self) -> bool {
        self.mvk_ios_surface && self.supports_khr_surface()
    }
    pub fn enable_mvk_ios_surface(&mut self) {
        self.mvk_ios_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_mvk_macos_surface(&self) -> bool {
        self.mvk_macos_surface && self.supports_khr_surface()
    }
    pub fn enable_mvk_macos_surface(&mut self) {
        self.mvk_macos_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_external_memory_dma_buf(&self) -> bool {
        self.supports_khr_external_memory_fd()
    }
    pub fn enable_ext_external_memory_dma_buf(&mut self) {
        self.enable_khr_external_memory_fd();
    }
    pub fn supports_ext_queue_family_foreign(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_ext_queue_family_foreign(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_ext_debug_utils(&self) -> bool {
        self.ext_debug_utils
    }
    pub fn enable_ext_debug_utils(&mut self) {
        self.ext_debug_utils = true;
    }
    pub fn supports_android_external_memory_android_hardware_buffer(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_sampler_ycbcr_conversion() && self.supports_khr_external_memory()))
            && self.supports_ext_queue_family_foreign()
    }
    pub fn enable_android_external_memory_android_hardware_buffer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
            self.enable_khr_external_memory();
        }
        self.enable_ext_queue_family_foreign();
    }
    pub fn supports_ext_sampler_filter_minmax(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_sampler_filter_minmax(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amdx_shader_enqueue(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.supports_khr_synchronization2()
                && self.supports_khr_spirv_1_4()
                && self.supports_ext_extended_dynamic_state()))
            && self.supports_khr_maintenance5()
    }
    pub fn enable_amdx_shader_enqueue(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
            self.enable_khr_spirv_1_4();
            self.enable_ext_extended_dynamic_state();
        }
        self.enable_khr_maintenance5();
    }
    pub fn supports_ext_descriptor_heap(&self) -> bool {
        self.supports_khr_maintenance5()
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_buffer_device_address())
    }
    pub fn enable_ext_descriptor_heap(&mut self) {
        self.enable_khr_maintenance5();
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_ext_inline_uniform_block(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_inline_uniform_block(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_bfloat16(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_bfloat16(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_sample_locations(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_sample_locations(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_blend_operation_advanced(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_blend_operation_advanced(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_acceleration_structure(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                && self.supports_ext_descriptor_indexing()
                && self.supports_khr_buffer_device_address())
    }
    pub fn enable_khr_acceleration_structure(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_ext_descriptor_indexing();
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_khr_ray_tracing_pipeline(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4())
            && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_pipeline(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_khr_ray_query(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4())
            && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_query(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_khr_sampler_ycbcr_conversion(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_sampler_ycbcr_conversion(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_image_drm_format_modifier(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_get_physical_device_properties2() && self.supports_khr_sampler_ycbcr_conversion())
    }
    pub fn enable_ext_image_drm_format_modifier(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_ext_descriptor_indexing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_get_physical_device_properties2() && self.supports_khr_maintenance3())
    }
    pub fn enable_ext_descriptor_indexing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
            self.enable_khr_maintenance3();
        }
    }
    pub fn supports_khr_portability_subset(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_portability_subset(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_shading_rate_image(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_shading_rate_image(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_ray_tracing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_ray_tracing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_representative_fragment_test(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_representative_fragment_test(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_maintenance3(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_maintenance3(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_cooperative_matrix_conversion(&self) -> bool {
        self.supports_khr_cooperative_matrix()
    }
    pub fn enable_qcom_cooperative_matrix_conversion(&mut self) {
        self.enable_khr_cooperative_matrix();
    }
    pub fn supports_khr_8bit_storage(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_8bit_storage(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_external_memory_host(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_ext_external_memory_host(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_shader_atomic_int64(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_atomic_int64(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_clock(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_clock(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_calibrated_timestamps(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_calibrated_timestamps(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amd_shader_core_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_amd_shader_core_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_global_priority(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_global_priority(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_vertex_attribute_divisor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_vertex_attribute_divisor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_driver_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_driver_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_float_controls(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_float_controls(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_depth_stencil_resolve(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_create_renderpass2()
    }
    pub fn enable_khr_depth_stencil_resolve(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
        }
    }
    pub fn supports_khr_swapchain_mutable_format(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_khr_swapchain_mutable_format(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_nv_compute_shader_derivatives(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_compute_shader_derivatives(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_mesh_shader(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_mesh_shader(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_fragment_shader_barycentric(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_fragment_shader_barycentric(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_shader_image_footprint(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_shader_image_footprint(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_scissor_exclusive(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_scissor_exclusive(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_device_diagnostic_checkpoints(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_device_diagnostic_checkpoints(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_timeline_semaphore(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_timeline_semaphore(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_present_timing(&self) -> bool {
        self.supports_khr_swapchain()
            && self.supports_khr_present_id2()
            && self.supports_khr_get_surface_capabilities2()
            && self.supports_khr_calibrated_timestamps()
    }
    pub fn enable_ext_present_timing(&mut self) {
        self.enable_khr_swapchain();
        self.enable_khr_present_id2();
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_calibrated_timestamps();
    }
    pub fn supports_intel_shader_integer_functions2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_intel_shader_integer_functions2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_vulkan_memory_model(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_vulkan_memory_model(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_pci_bus_info(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_pci_bus_info(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amd_display_native_hdr(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_khr_get_surface_capabilities2()
            && self.supports_khr_swapchain()
    }
    pub fn enable_amd_display_native_hdr(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_swapchain();
    }
    pub fn supports_fuchsia_imagepipe_surface(&self) -> bool {
        self.fuchsia_imagepipe_surface && self.supports_khr_surface()
    }
    pub fn enable_fuchsia_imagepipe_surface(&mut self) {
        self.fuchsia_imagepipe_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_shader_terminate_invocation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_terminate_invocation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_metal_surface(&self) -> bool {
        self.ext_metal_surface && self.supports_khr_surface()
    }
    pub fn enable_ext_metal_surface(&mut self) {
        self.ext_metal_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_fragment_density_map(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_fragment_density_map(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_scalar_block_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_scalar_block_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_fragment_shading_rate(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_create_renderpass2())
    }
    pub fn enable_khr_fragment_shading_rate(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_create_renderpass2();
        }
    }
    pub fn supports_amd_shader_core_properties2(&self) -> bool {
        self.supports_amd_shader_core_properties()
    }
    pub fn enable_amd_shader_core_properties2(&mut self) {
        self.enable_amd_shader_core_properties();
    }
    pub fn supports_amd_device_coherent_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_amd_device_coherent_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_dynamic_rendering_local_read(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering()
    }
    pub fn enable_khr_dynamic_rendering_local_read(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_ext_shader_image_atomic_int64(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_image_atomic_int64(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_quad_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) && self.supports_khr_vulkan_memory_model())
    }
    pub fn enable_khr_shader_quad_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_vulkan_memory_model();
        }
    }
    pub fn supports_khr_spirv_1_4(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) && self.supports_khr_shader_float_controls())
    }
    pub fn enable_khr_spirv_1_4(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_shader_float_controls();
        }
    }
    pub fn supports_ext_memory_budget(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_memory_budget(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_memory_priority(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_memory_priority(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_surface_protected_capabilities(&self) -> bool {
        self.khr_surface_protected_capabilities
            && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            && self.supports_khr_get_surface_capabilities2()
    }
    pub fn enable_khr_surface_protected_capabilities(&mut self) {
        self.khr_surface_protected_capabilities = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
        self.enable_khr_get_surface_capabilities2();
    }
    pub fn supports_nv_dedicated_allocation_image_aliasing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_dedicated_allocation_image_aliasing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_separate_depth_stencil_layouts(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_create_renderpass2())
    }
    pub fn enable_khr_separate_depth_stencil_layouts(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_create_renderpass2();
        }
    }
    pub fn supports_ext_buffer_device_address(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_buffer_device_address(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_validation_features(&self) -> bool {
        self.ext_validation_features
    }
    pub fn enable_ext_validation_features(&mut self) {
        self.ext_validation_features = true;
    }
    pub fn supports_khr_present_wait(&self) -> bool {
        self.supports_khr_swapchain() && self.supports_khr_present_id()
    }
    pub fn enable_khr_present_wait(&mut self) {
        self.enable_khr_swapchain();
        self.enable_khr_present_id();
    }
    pub fn supports_nv_cooperative_matrix(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_cooperative_matrix(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_coverage_reduction_mode(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_coverage_reduction_mode(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_fragment_shader_interlock(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_fragment_shader_interlock(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_ycbcr_image_arrays(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_sampler_ycbcr_conversion()
    }
    pub fn enable_ext_ycbcr_image_arrays(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_khr_uniform_buffer_standard_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_uniform_buffer_standard_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_provoking_vertex(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_provoking_vertex(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_full_screen_exclusive(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_khr_surface()
            && self.supports_khr_get_surface_capabilities2()
            && self.supports_khr_swapchain()
    }
    pub fn enable_ext_full_screen_exclusive(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_khr_surface();
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_headless_surface(&self) -> bool {
        self.ext_headless_surface && self.supports_khr_surface()
    }
    pub fn enable_ext_headless_surface(&mut self) {
        self.ext_headless_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_khr_buffer_device_address(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_get_physical_device_properties2() && self.supports_khr_device_group())
    }
    pub fn enable_khr_buffer_device_address(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
            self.enable_khr_device_group();
        }
    }
    pub fn supports_ext_line_rasterization(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_line_rasterization(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_atomic_float(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_atomic_float(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_host_query_reset(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_host_query_reset(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_index_type_uint8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_index_type_uint8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_extended_dynamic_state(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_extended_dynamic_state(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_pipeline_executable_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_pipeline_executable_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_host_image_copy(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_copy_commands2()
                && self.supports_khr_format_feature_flags2())
    }
    pub fn enable_ext_host_image_copy(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_copy_commands2();
            self.enable_khr_format_feature_flags2();
        }
    }
    pub fn supports_ext_shader_atomic_float2(&self) -> bool {
        self.supports_ext_shader_atomic_float()
    }
    pub fn enable_ext_shader_atomic_float2(&mut self) {
        self.enable_ext_shader_atomic_float();
    }
    pub fn supports_ext_surface_maintenance1(&self) -> bool {
        self.ext_surface_maintenance1 && self.supports_khr_surface() && self.supports_khr_get_surface_capabilities2()
    }
    pub fn enable_ext_surface_maintenance1(&mut self) {
        self.ext_surface_maintenance1 = true;
        self.enable_khr_surface();
        self.enable_khr_get_surface_capabilities2();
    }
    pub fn supports_ext_swapchain_maintenance1(&self) -> bool {
        self.supports_khr_swapchain()
            && self.supports_ext_surface_maintenance1()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
    }
    pub fn enable_ext_swapchain_maintenance1(&mut self) {
        self.enable_khr_swapchain();
        self.enable_ext_surface_maintenance1();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_demote_to_helper_invocation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_demote_to_helper_invocation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_device_generated_commands(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) && self.supports_khr_buffer_device_address())
    }
    pub fn enable_nv_device_generated_commands(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_inherited_viewport_scissor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_inherited_viewport_scissor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_integer_dot_product(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_integer_dot_product(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_texel_buffer_alignment(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_texel_buffer_alignment(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_depth_bias_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_depth_bias_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_device_memory_report(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_device_memory_report(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_acquire_drm_display(&self) -> bool {
        self.ext_acquire_drm_display && self.supports_ext_direct_mode_display()
    }
    pub fn enable_ext_acquire_drm_display(&mut self) {
        self.ext_acquire_drm_display = true;
        self.enable_ext_direct_mode_display();
    }
    pub fn supports_ext_robustness2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_robustness2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_custom_border_color(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_custom_border_color(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_texture_compression_astc_3d(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_texture_compression_astc_3d(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_present_barrier(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_khr_surface()
            && self.supports_khr_get_surface_capabilities2()
            && self.supports_khr_swapchain()
    }
    pub fn enable_nv_present_barrier(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_khr_surface();
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_present_id(&self) -> bool {
        self.supports_khr_swapchain()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
    }
    pub fn enable_khr_present_id(&mut self) {
        self.enable_khr_swapchain();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_private_data(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_private_data(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_pipeline_creation_cache_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_pipeline_creation_cache_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_device_diagnostics_config(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_device_diagnostics_config(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_cuda_kernel_launch(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_cuda_kernel_launch(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_tile_shading(&self) -> bool {
        self.supports_qcom_tile_properties()
    }
    pub fn enable_qcom_tile_shading(&mut self) {
        self.enable_qcom_tile_properties();
    }
    pub fn supports_khr_synchronization2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_synchronization2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_descriptor_buffer(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_get_physical_device_properties2())
                    && self.supports_khr_buffer_device_address()
                    && self.supports_ext_descriptor_indexing()))
                && self.supports_khr_synchronization2())
    }
    pub fn enable_ext_descriptor_buffer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                    self.enable_khr_get_physical_device_properties2();
                }
                self.enable_khr_buffer_device_address();
                self.enable_ext_descriptor_indexing();
            }
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_ext_graphics_pipeline_library(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_graphics_pipeline_library(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amd_shader_early_and_late_fragment_tests(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_amd_shader_early_and_late_fragment_tests(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_fragment_shader_barycentric(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_fragment_shader_barycentric(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_zero_initialize_workgroup_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_zero_initialize_workgroup_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_fragment_shading_rate_enums(&self) -> bool {
        self.supports_khr_fragment_shading_rate()
    }
    pub fn enable_nv_fragment_shading_rate_enums(&mut self) {
        self.enable_khr_fragment_shading_rate();
    }
    pub fn supports_nv_ray_tracing_motion_blur(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_motion_blur(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_ext_mesh_shader(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4()
    }
    pub fn enable_ext_mesh_shader(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
    }
    pub fn supports_ext_ycbcr_2plane_444_formats(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_sampler_ycbcr_conversion()
    }
    pub fn enable_ext_ycbcr_2plane_444_formats(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_ext_fragment_density_map2(&self) -> bool {
        self.supports_ext_fragment_density_map()
    }
    pub fn enable_ext_fragment_density_map2(&mut self) {
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_qcom_rotated_copy_commands(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_copy_commands2()
    }
    pub fn enable_qcom_rotated_copy_commands(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_copy_commands2();
        }
    }
    pub fn supports_ext_image_robustness(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_image_robustness(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_workgroup_memory_explicit_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_workgroup_memory_explicit_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_copy_commands2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_copy_commands2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_image_compression_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_image_compression_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_attachment_feedback_loop_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_attachment_feedback_loop_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_4444_formats(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_4444_formats(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_device_fault(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_device_fault(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_rasterization_order_attachment_access(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_arm_rasterization_order_attachment_access(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_rgba10x6_formats(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_sampler_ycbcr_conversion()
    }
    pub fn enable_ext_rgba10x6_formats(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_nv_acquire_winrt_display(&self) -> bool {
        self.supports_ext_direct_mode_display()
    }
    pub fn enable_nv_acquire_winrt_display(&mut self) {
        self.enable_ext_direct_mode_display();
    }
    pub fn supports_ext_directfb_surface(&self) -> bool {
        self.ext_directfb_surface && self.supports_khr_surface()
    }
    pub fn enable_ext_directfb_surface(&mut self) {
        self.ext_directfb_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_valve_mutable_descriptor_type(&self) -> bool {
        self.supports_khr_maintenance3()
    }
    pub fn enable_valve_mutable_descriptor_type(&mut self) {
        self.enable_khr_maintenance3();
    }
    pub fn supports_ext_vertex_input_dynamic_state(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_vertex_input_dynamic_state(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_physical_device_drm(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_physical_device_drm(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_device_address_binding_report(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_ext_debug_utils()
    }
    pub fn enable_ext_device_address_binding_report(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_ext_debug_utils();
    }
    pub fn supports_ext_depth_clip_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_depth_clip_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_primitive_topology_list_restart(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_primitive_topology_list_restart(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_format_feature_flags2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_format_feature_flags2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_present_mode_fifo_latest_ready(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_ext_present_mode_fifo_latest_ready(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_fuchsia_external_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_external_memory_capabilities() && self.supports_khr_external_memory())
    }
    pub fn enable_fuchsia_external_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory_capabilities();
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_fuchsia_external_semaphore(&self) -> bool {
        self.supports_khr_external_semaphore_capabilities() && self.supports_khr_external_semaphore()
    }
    pub fn enable_fuchsia_external_semaphore(&mut self) {
        self.enable_khr_external_semaphore_capabilities();
        self.enable_khr_external_semaphore();
    }
    pub fn supports_fuchsia_buffer_collection(&self) -> bool {
        self.supports_fuchsia_external_memory()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_sampler_ycbcr_conversion())
    }
    pub fn enable_fuchsia_buffer_collection(&mut self) {
        self.enable_fuchsia_external_memory();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_huawei_subpass_shading(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_create_renderpass2())
                && self.supports_khr_synchronization2())
    }
    pub fn enable_huawei_subpass_shading(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_create_renderpass2();
            }
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_huawei_invocation_mask(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_synchronization2())
    }
    pub fn enable_huawei_invocation_mask(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_nv_external_memory_rdma(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_nv_external_memory_rdma(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_ext_pipeline_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_pipeline_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_frame_boundary(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_frame_boundary(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_multisampled_render_to_single_sampled(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.supports_khr_create_renderpass2() && self.supports_khr_depth_stencil_resolve())
    }
    pub fn enable_ext_multisampled_render_to_single_sampled(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
            self.enable_khr_depth_stencil_resolve();
        }
    }
    pub fn supports_ext_extended_dynamic_state2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_extended_dynamic_state2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_color_write_enable(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_color_write_enable(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_primitives_generated_query(&self) -> bool {
        self.supports_ext_transform_feedback()
    }
    pub fn enable_ext_primitives_generated_query(&mut self) {
        self.enable_ext_transform_feedback();
    }
    pub fn supports_khr_ray_tracing_maintenance1(&self) -> bool {
        self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_maintenance1(&mut self) {
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_khr_shader_untyped_pointers(&self) -> bool {
        self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_untyped_pointers(&mut self) {
        self.enable_khr_get_physical_device_properties2();
    }
    pub fn supports_ext_global_priority_query(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_global_priority_query(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_image_view_min_lod(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_image_view_min_lod(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_multi_draw(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_multi_draw(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_image_2d_view_of_3d(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_image_2d_view_of_3d(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_portability_enumeration(&self) -> bool {
        self.khr_portability_enumeration
    }
    pub fn enable_khr_portability_enumeration(&mut self) {
        self.khr_portability_enumeration = true;
    }
    pub fn supports_ext_opacity_micromap(&self) -> bool {
        self.supports_khr_acceleration_structure()
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_synchronization2())
    }
    pub fn enable_ext_opacity_micromap(&mut self) {
        self.enable_khr_acceleration_structure();
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_nv_displacement_micromap(&self) -> bool {
        self.supports_ext_opacity_micromap()
    }
    pub fn enable_nv_displacement_micromap(&mut self) {
        self.enable_ext_opacity_micromap();
    }
    pub fn supports_huawei_cluster_culling_shader(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_huawei_cluster_culling_shader(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_border_color_swizzle(&self) -> bool {
        self.supports_ext_custom_border_color()
    }
    pub fn enable_ext_border_color_swizzle(&mut self) {
        self.enable_ext_custom_border_color();
    }
    pub fn supports_ext_pageable_device_local_memory(&self) -> bool {
        self.supports_ext_memory_priority()
    }
    pub fn enable_ext_pageable_device_local_memory(&mut self) {
        self.enable_ext_memory_priority();
    }
    pub fn supports_khr_shader_subgroup_rotate(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_subgroup_rotate(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_scheduling_controls(&self) -> bool {
        self.supports_arm_shader_core_builtins()
    }
    pub fn enable_arm_scheduling_controls(&mut self) {
        self.enable_arm_shader_core_builtins();
    }
    pub fn supports_ext_image_sliced_view_of_3d(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_image_sliced_view_of_3d(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_valve_descriptor_set_host_mapping(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_valve_descriptor_set_host_mapping(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_depth_clamp_zero_one(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_depth_clamp_zero_one(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_non_seamless_cube_map(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_non_seamless_cube_map(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_render_pass_striped(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_synchronization2())
    }
    pub fn enable_arm_render_pass_striped(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_qcom_fragment_density_map_offset(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_ext_fragment_density_map()
    }
    pub fn enable_qcom_fragment_density_map_offset(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_nv_copy_memory_indirect(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_buffer_device_address())
    }
    pub fn enable_nv_copy_memory_indirect(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_memory_decompression(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_buffer_device_address())
    }
    pub fn enable_nv_memory_decompression(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_device_generated_commands_compute(&self) -> bool {
        self.supports_nv_device_generated_commands()
    }
    pub fn enable_nv_device_generated_commands_compute(&mut self) {
        self.enable_nv_device_generated_commands();
    }
    pub fn supports_nv_ray_tracing_linear_swept_spheres(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_linear_swept_spheres(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_nv_linear_color_attachment(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_linear_color_attachment(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_google_surfaceless_query(&self) -> bool {
        self.google_surfaceless_query && self.supports_khr_surface()
    }
    pub fn enable_google_surfaceless_query(&mut self) {
        self.google_surfaceless_query = true;
        self.enable_khr_surface();
    }
    pub fn supports_ext_image_compression_control_swapchain(&self) -> bool {
        self.supports_ext_image_compression_control()
    }
    pub fn enable_ext_image_compression_control_swapchain(&mut self) {
        self.enable_ext_image_compression_control();
    }
    pub fn supports_qcom_image_processing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_format_feature_flags2()
    }
    pub fn enable_qcom_image_processing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_format_feature_flags2();
        }
    }
    pub fn supports_ext_nested_command_buffer(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_nested_command_buffer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ohos_external_memory(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.supports_khr_sampler_ycbcr_conversion() && self.supports_khr_external_memory()))
            && self.supports_ext_queue_family_foreign()
    }
    pub fn enable_ohos_external_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
            self.enable_khr_external_memory();
        }
        self.enable_ext_queue_family_foreign();
    }
    pub fn supports_ext_external_memory_acquire_unmodified(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_ext_external_memory_acquire_unmodified(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_ext_extended_dynamic_state3(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_extended_dynamic_state3(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_subpass_merge_feedback(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_subpass_merge_feedback(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_lunarg_direct_driver_loading(&self) -> bool {
        self.lunarg_direct_driver_loading
    }
    pub fn enable_lunarg_direct_driver_loading(&mut self) {
        self.lunarg_direct_driver_loading = true;
    }
    pub fn supports_ext_shader_module_identifier(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_ext_pipeline_creation_cache_control())
    }
    pub fn enable_ext_shader_module_identifier(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_ext_pipeline_creation_cache_control();
        }
    }
    pub fn supports_ext_rasterization_order_attachment_access(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_rasterization_order_attachment_access(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_optical_flow(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_format_feature_flags2()
                && self.supports_khr_synchronization2())
    }
    pub fn enable_nv_optical_flow(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_format_feature_flags2();
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_ext_legacy_dithering(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_legacy_dithering(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_pipeline_protected_access(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_pipeline_protected_access(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_android_external_format_resolve(&self) -> bool {
        self.supports_android_external_memory_android_hardware_buffer()
    }
    pub fn enable_android_external_format_resolve(&mut self) {
        self.enable_android_external_memory_android_hardware_buffer();
    }
    pub fn supports_khr_maintenance5(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) && self.supports_khr_dynamic_rendering())
    }
    pub fn enable_khr_maintenance5(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_amd_anti_lag(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_amd_anti_lag(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_amdx_dense_geometry_format(&self) -> bool {
        self.supports_khr_acceleration_structure()
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5())
    }
    pub fn enable_amdx_dense_geometry_format(&mut self) {
        self.enable_khr_acceleration_structure();
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_khr_present_id2(&self) -> bool {
        self.supports_khr_get_surface_capabilities2() && self.supports_khr_surface() && self.supports_khr_swapchain()
    }
    pub fn enable_khr_present_id2(&mut self) {
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_surface();
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_present_wait2(&self) -> bool {
        self.supports_khr_get_surface_capabilities2()
            && self.supports_khr_surface()
            && self.supports_khr_swapchain()
            && self.supports_khr_present_id2()
    }
    pub fn enable_khr_present_wait2(&mut self) {
        self.enable_khr_get_surface_capabilities2();
        self.enable_khr_surface();
        self.enable_khr_swapchain();
        self.enable_khr_present_id2();
    }
    pub fn supports_khr_ray_tracing_position_fetch(&self) -> bool {
        self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_position_fetch(&mut self) {
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_ext_shader_object(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_shader_object(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_khr_pipeline_binary(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5()
    }
    pub fn enable_khr_pipeline_binary(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_qcom_tile_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_qcom_tile_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_sec_amigo_profiling(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_sec_amigo_profiling(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_surface_maintenance1(&self) -> bool {
        self.khr_surface_maintenance1 && self.supports_khr_surface() && self.supports_khr_get_surface_capabilities2()
    }
    pub fn enable_khr_surface_maintenance1(&mut self) {
        self.khr_surface_maintenance1 = true;
        self.enable_khr_surface();
        self.enable_khr_get_surface_capabilities2();
    }
    pub fn supports_khr_swapchain_maintenance1(&self) -> bool {
        self.supports_khr_swapchain()
            && self.supports_khr_surface_maintenance1()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
    }
    pub fn enable_khr_swapchain_maintenance1(&mut self) {
        self.enable_khr_swapchain();
        self.enable_khr_surface_maintenance1();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_multiview_per_view_viewports(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_qcom_multiview_per_view_viewports(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_ray_tracing_invocation_reorder(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_invocation_reorder(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_nv_cooperative_vector(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_cooperative_vector(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_extended_sparse_address_space(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_extended_sparse_address_space(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_mutable_descriptor_type(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance3()
    }
    pub fn enable_ext_mutable_descriptor_type(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_maintenance3();
        }
    }
    pub fn supports_ext_legacy_vertex_attributes(&self) -> bool {
        self.supports_ext_vertex_input_dynamic_state()
    }
    pub fn enable_ext_legacy_vertex_attributes(&mut self) {
        self.enable_ext_vertex_input_dynamic_state();
    }
    pub fn supports_ext_layer_settings(&self) -> bool {
        self.ext_layer_settings
    }
    pub fn enable_ext_layer_settings(&mut self) {
        self.ext_layer_settings = true;
    }
    pub fn supports_arm_shader_core_builtins(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_arm_shader_core_builtins(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_pipeline_library_group_handles(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_ext_pipeline_library_group_handles(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_ext_dynamic_rendering_unused_attachments(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_physical_device_properties2())
                && self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_dynamic_rendering_unused_attachments(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_physical_device_properties2();
            }
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_nv_low_latency2(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_timeline_semaphore())
            && (self.supports_khr_present_id() || self.supports_khr_present_id2())
    }
    pub fn enable_nv_low_latency2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_timeline_semaphore();
        }
        // ambiguous dependency, caller must enable one explicitly
        debug_assert!(self.supports_khr_present_id() || self.supports_khr_present_id2());
    }
    pub fn supports_khr_cooperative_matrix(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_cooperative_matrix(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_data_graph(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) && self.supports_khr_maintenance5()
    }
    pub fn enable_arm_data_graph(&mut self) {
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 3, 0));
        self.enable_khr_maintenance5();
    }
    pub fn supports_qcom_multiview_per_view_render_areas(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_qcom_multiview_per_view_render_areas(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_compute_shader_derivatives(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_compute_shader_derivatives(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_image_processing2(&self) -> bool {
        self.supports_qcom_image_processing()
    }
    pub fn enable_qcom_image_processing2(&mut self) {
        self.enable_qcom_image_processing();
    }
    pub fn supports_qcom_ycbcr_degamma(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_qcom_ycbcr_degamma(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_filter_cubic_clamp(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_ext_sampler_filter_minmax()
    }
    pub fn enable_qcom_filter_cubic_clamp(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_ext_sampler_filter_minmax();
        }
    }
    pub fn supports_ext_attachment_feedback_loop_dynamic_state(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_ext_attachment_feedback_loop_layout()
    }
    pub fn enable_ext_attachment_feedback_loop_dynamic_state(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_ext_attachment_feedback_loop_layout();
    }
    pub fn supports_khr_vertex_attribute_divisor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_vertex_attribute_divisor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_unified_image_layouts(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_unified_image_layouts(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_float_controls2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) && self.supports_khr_shader_float_controls())
    }
    pub fn enable_khr_shader_float_controls2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_shader_float_controls();
        }
    }
    pub fn supports_msft_layered_driver(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_msft_layered_driver(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_index_type_uint8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_index_type_uint8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_line_rasterization(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_line_rasterization(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_calibrated_timestamps(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_calibrated_timestamps(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_expect_assume(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_expect_assume(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_tile_memory_heap(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_qcom_tile_memory_heap(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_copy_memory_indirect(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.supports_khr_get_physical_device_properties2() && self.supports_khr_buffer_device_address())
    }
    pub fn enable_khr_copy_memory_indirect(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_get_physical_device_properties2();
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_ext_memory_decompression(&self) -> bool {
        self.supports_khr_get_physical_device_properties2() && self.supports_khr_buffer_device_address()
    }
    pub fn enable_ext_memory_decompression(&mut self) {
        self.enable_khr_get_physical_device_properties2();
        self.enable_khr_buffer_device_address();
    }
    pub fn supports_nv_display_stereo(&self) -> bool {
        self.nv_display_stereo && self.supports_khr_display() && self.supports_khr_get_display_properties2()
    }
    pub fn enable_nv_display_stereo(&mut self) {
        self.nv_display_stereo = true;
        self.enable_khr_display();
        self.enable_khr_get_display_properties2();
    }
    pub fn supports_nv_raw_access_chains(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_raw_access_chains(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_relaxed_extended_instruction(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_relaxed_extended_instruction(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_command_buffer_inheritance(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_command_buffer_inheritance(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_shader_atomic_float16_vector(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_shader_atomic_float16_vector(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_replicated_composites(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_replicated_composites(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_float8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_float8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_ray_tracing_validation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_ray_tracing_validation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_cluster_acceleration_structure(&self) -> bool {
        self.supports_khr_acceleration_structure()
    }
    pub fn enable_nv_cluster_acceleration_structure(&mut self) {
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_nv_partitioned_acceleration_structure(&self) -> bool {
        self.supports_khr_acceleration_structure()
    }
    pub fn enable_nv_partitioned_acceleration_structure(&mut self) {
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_ext_device_generated_commands(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || self.supports_khr_buffer_device_address())
                && self.supports_khr_maintenance5())
    }
    pub fn enable_ext_device_generated_commands(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_buffer_device_address();
            }
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_mesa_image_alignment_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_mesa_image_alignment_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_shader_fma(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_shader_fma(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_ray_tracing_invocation_reorder(&self) -> bool {
        self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_ext_ray_tracing_invocation_reorder(&mut self) {
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_ext_depth_clamp_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_depth_clamp_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_maintenance9(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_maintenance9(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ohos_surface(&self) -> bool {
        self.ohos_surface && self.supports_khr_surface()
    }
    pub fn enable_ohos_surface(&mut self) {
        self.ohos_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_huawei_hdr_vivid(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_khr_swapchain()
            && self.supports_ext_hdr_metadata()
    }
    pub fn enable_huawei_hdr_vivid(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_khr_swapchain();
        self.enable_ext_hdr_metadata();
    }
    pub fn supports_nv_cooperative_matrix2(&self) -> bool {
        self.supports_khr_cooperative_matrix()
    }
    pub fn enable_nv_cooperative_matrix2(&mut self) {
        self.enable_khr_cooperative_matrix();
    }
    pub fn supports_arm_pipeline_opacity_micromap(&self) -> bool {
        self.supports_ext_opacity_micromap()
    }
    pub fn enable_arm_pipeline_opacity_micromap(&mut self) {
        self.enable_ext_opacity_micromap();
    }
    pub fn supports_ext_external_memory_metal(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory()
    }
    pub fn enable_ext_external_memory_metal(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_depth_clamp_zero_one(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_depth_clamp_zero_one(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_performance_counters_by_region(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_arm_performance_counters_by_region(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_vertex_attribute_robustness(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_vertex_attribute_robustness(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_arm_format_pack(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_arm_format_pack(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_valve_fragment_density_map_layered(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5())
            && self.supports_ext_fragment_density_map()
    }
    pub fn enable_valve_fragment_density_map_layered(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_khr_robustness2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_robustness2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_present_metering(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_present_metering(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_fragment_density_map_offset(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && self.supports_ext_fragment_density_map()
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_create_renderpass2())
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_fragment_density_map_offset(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        self.enable_ext_fragment_density_map();
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
        }
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_ext_zero_initialize_device_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_zero_initialize_device_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_khr_present_mode_fifo_latest_ready(&self) -> bool {
        self.supports_khr_swapchain()
    }
    pub fn enable_khr_present_mode_fifo_latest_ready(&mut self) {
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_shader_64bit_indexing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_64bit_indexing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_custom_resolve(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_custom_resolve(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_qcom_data_graph_model(&self) -> bool {
        self.supports_arm_data_graph()
    }
    pub fn enable_qcom_data_graph_model(&mut self) {
        self.enable_arm_data_graph();
    }
    pub fn supports_khr_maintenance10(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_khr_maintenance10(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_sec_pipeline_cache_incremental_mode(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_sec_pipeline_cache_incremental_mode(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_uniform_buffer_unsized_array(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_uniform_buffer_unsized_array(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_nv_compute_occupancy_priority(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_nv_compute_occupancy_priority(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_ext_shader_subgroup_partitioned(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_get_physical_device_properties2()
    }
    pub fn enable_ext_shader_subgroup_partitioned(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
    }
    pub fn supports_sec_ubm_surface(&self) -> bool {
        self.sec_ubm_surface && self.supports_khr_surface()
    }
    pub fn enable_sec_ubm_surface(&mut self) {
        self.sec_ubm_surface = true;
        self.enable_khr_surface();
    }
    pub fn supports_valve_shader_mixed_float_dot_product(&self) -> bool {
        (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || self.supports_khr_get_physical_device_properties2())
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_shader_float16_int8())
    }
    pub fn enable_valve_shader_mixed_float_dot_product(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_physical_device_properties2();
        }
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_shader_float16_int8();
        }
    }
    pub fn to_name_vec(&self) -> Vec<&'static CStr> {
        let mut v = Vec::new();
        if self.khr_surface {
            v.push(c"VK_KHR_surface");
        }
        if self.khr_display {
            v.push(c"VK_KHR_display");
        }
        if self.khr_xlib_surface {
            v.push(c"VK_KHR_xlib_surface");
        }
        if self.khr_xcb_surface {
            v.push(c"VK_KHR_xcb_surface");
        }
        if self.khr_wayland_surface {
            v.push(c"VK_KHR_wayland_surface");
        }
        if self.khr_android_surface {
            v.push(c"VK_KHR_android_surface");
        }
        if self.khr_win32_surface {
            v.push(c"VK_KHR_win32_surface");
        }
        if self.ext_debug_report {
            v.push(c"VK_EXT_debug_report");
        }
        if self.nv_external_memory_capabilities {
            v.push(c"VK_NV_external_memory_capabilities");
        }
        if self.khr_get_physical_device_properties2 {
            v.push(c"VK_KHR_get_physical_device_properties2");
        }
        if self.ext_validation_flags {
            v.push(c"VK_EXT_validation_flags");
        }
        if self.nn_vi_surface {
            v.push(c"VK_NN_vi_surface");
        }
        if self.khr_device_group_creation {
            v.push(c"VK_KHR_device_group_creation");
        }
        if self.khr_external_memory_capabilities {
            v.push(c"VK_KHR_external_memory_capabilities");
        }
        if self.khr_external_semaphore_capabilities {
            v.push(c"VK_KHR_external_semaphore_capabilities");
        }
        if self.ext_direct_mode_display {
            v.push(c"VK_EXT_direct_mode_display");
        }
        if self.ext_acquire_xlib_display {
            v.push(c"VK_EXT_acquire_xlib_display");
        }
        if self.ext_display_surface_counter {
            v.push(c"VK_EXT_display_surface_counter");
        }
        if self.ext_swapchain_colorspace {
            v.push(c"VK_EXT_swapchain_colorspace");
        }
        if self.khr_external_fence_capabilities {
            v.push(c"VK_KHR_external_fence_capabilities");
        }
        if self.khr_get_surface_capabilities2 {
            v.push(c"VK_KHR_get_surface_capabilities2");
        }
        if self.khr_get_display_properties2 {
            v.push(c"VK_KHR_get_display_properties2");
        }
        if self.mvk_ios_surface {
            v.push(c"VK_MVK_ios_surface");
        }
        if self.mvk_macos_surface {
            v.push(c"VK_MVK_macos_surface");
        }
        if self.ext_debug_utils {
            v.push(c"VK_EXT_debug_utils");
        }
        if self.fuchsia_imagepipe_surface {
            v.push(c"VK_FUCHSIA_imagepipe_surface");
        }
        if self.ext_metal_surface {
            v.push(c"VK_EXT_metal_surface");
        }
        if self.khr_surface_protected_capabilities {
            v.push(c"VK_KHR_surface_protected_capabilities");
        }
        if self.ext_validation_features {
            v.push(c"VK_EXT_validation_features");
        }
        if self.ext_headless_surface {
            v.push(c"VK_EXT_headless_surface");
        }
        if self.ext_surface_maintenance1 {
            v.push(c"VK_EXT_surface_maintenance1");
        }
        if self.ext_acquire_drm_display {
            v.push(c"VK_EXT_acquire_drm_display");
        }
        if self.ext_directfb_surface {
            v.push(c"VK_EXT_directfb_surface");
        }
        if self.khr_portability_enumeration {
            v.push(c"VK_KHR_portability_enumeration");
        }
        if self.google_surfaceless_query {
            v.push(c"VK_GOOGLE_surfaceless_query");
        }
        if self.lunarg_direct_driver_loading {
            v.push(c"VK_LUNARG_direct_driver_loading");
        }
        if self.khr_surface_maintenance1 {
            v.push(c"VK_KHR_surface_maintenance1");
        }
        if self.ext_layer_settings {
            v.push(c"VK_EXT_layer_settings");
        }
        if self.nv_display_stereo {
            v.push(c"VK_NV_display_stereo");
        }
        if self.ohos_surface {
            v.push(c"VK_OHOS_surface");
        }
        if self.sec_ubm_surface {
            v.push(c"VK_SEC_ubm_surface");
        }
        v
    }
}

#[derive(Copy, Clone)]
pub struct Instance {
    pub handle: vk::Instance,
    pub extensions: InstanceExtensions,
    pub fp_destroy_instance: vk::FnDestroyInstance,
    pub fp_enumerate_physical_devices: vk::FnEnumeratePhysicalDevices,
    pub fp_get_device_proc_addr: vk::FnGetDeviceProcAddr,
    pub fp_get_physical_device_properties: vk::FnGetPhysicalDeviceProperties,
    pub fp_get_physical_device_queue_family_properties: vk::FnGetPhysicalDeviceQueueFamilyProperties,
    pub fp_get_physical_device_memory_properties: vk::FnGetPhysicalDeviceMemoryProperties,
    pub fp_get_physical_device_features: vk::FnGetPhysicalDeviceFeatures,
    pub fp_get_physical_device_format_properties: vk::FnGetPhysicalDeviceFormatProperties,
    pub fp_get_physical_device_image_format_properties: vk::FnGetPhysicalDeviceImageFormatProperties,
    pub fp_create_device: vk::FnCreateDevice,
    pub fp_enumerate_device_layer_properties: vk::FnEnumerateDeviceLayerProperties,
    pub fp_enumerate_device_extension_properties: vk::FnEnumerateDeviceExtensionProperties,
    pub fp_get_physical_device_sparse_image_format_properties: vk::FnGetPhysicalDeviceSparseImageFormatProperties,
    pub fp_create_android_surface_khr: Option<vk::FnCreateAndroidSurfaceKHR>,
    pub fp_create_surface_ohos: Option<vk::FnCreateSurfaceOHOS>,
    pub fp_get_physical_device_display_properties_khr: Option<vk::FnGetPhysicalDeviceDisplayPropertiesKHR>,
    pub fp_get_physical_device_display_plane_properties_khr: Option<vk::FnGetPhysicalDeviceDisplayPlanePropertiesKHR>,
    pub fp_get_display_plane_supported_displays_khr: Option<vk::FnGetDisplayPlaneSupportedDisplaysKHR>,
    pub fp_get_display_mode_properties_khr: Option<vk::FnGetDisplayModePropertiesKHR>,
    pub fp_create_display_mode_khr: Option<vk::FnCreateDisplayModeKHR>,
    pub fp_get_display_plane_capabilities_khr: Option<vk::FnGetDisplayPlaneCapabilitiesKHR>,
    pub fp_create_display_plane_surface_khr: Option<vk::FnCreateDisplayPlaneSurfaceKHR>,
    pub fp_destroy_surface_khr: Option<vk::FnDestroySurfaceKHR>,
    pub fp_get_physical_device_surface_support_khr: Option<vk::FnGetPhysicalDeviceSurfaceSupportKHR>,
    pub fp_get_physical_device_surface_capabilities_khr: Option<vk::FnGetPhysicalDeviceSurfaceCapabilitiesKHR>,
    pub fp_get_physical_device_surface_formats_khr: Option<vk::FnGetPhysicalDeviceSurfaceFormatsKHR>,
    pub fp_get_physical_device_surface_present_modes_khr: Option<vk::FnGetPhysicalDeviceSurfacePresentModesKHR>,
    pub fp_create_vi_surface_nn: Option<vk::FnCreateViSurfaceNN>,
    pub fp_create_wayland_surface_khr: Option<vk::FnCreateWaylandSurfaceKHR>,
    pub fp_get_physical_device_wayland_presentation_support_khr:
        Option<vk::FnGetPhysicalDeviceWaylandPresentationSupportKHR>,
    pub fp_create_ubm_surface_sec: Option<vk::FnCreateUbmSurfaceSEC>,
    pub fp_get_physical_device_ubm_presentation_support_sec: Option<vk::FnGetPhysicalDeviceUbmPresentationSupportSEC>,
    pub fp_create_win32_surface_khr: Option<vk::FnCreateWin32SurfaceKHR>,
    pub fp_get_physical_device_win32_presentation_support_khr:
        Option<vk::FnGetPhysicalDeviceWin32PresentationSupportKHR>,
    pub fp_create_xlib_surface_khr: Option<vk::FnCreateXlibSurfaceKHR>,
    pub fp_get_physical_device_xlib_presentation_support_khr: Option<vk::FnGetPhysicalDeviceXlibPresentationSupportKHR>,
    pub fp_create_xcb_surface_khr: Option<vk::FnCreateXcbSurfaceKHR>,
    pub fp_get_physical_device_xcb_presentation_support_khr: Option<vk::FnGetPhysicalDeviceXcbPresentationSupportKHR>,
    pub fp_create_direct_fb_surface_ext: Option<vk::FnCreateDirectFBSurfaceEXT>,
    pub fp_get_physical_device_direct_fb_presentation_support_ext:
        Option<vk::FnGetPhysicalDeviceDirectFBPresentationSupportEXT>,
    pub fp_create_image_pipe_surface_fuchsia: Option<vk::FnCreateImagePipeSurfaceFUCHSIA>,
    pub fp_create_debug_report_callback_ext: Option<vk::FnCreateDebugReportCallbackEXT>,
    pub fp_destroy_debug_report_callback_ext: Option<vk::FnDestroyDebugReportCallbackEXT>,
    pub fp_debug_report_message_ext: Option<vk::FnDebugReportMessageEXT>,
    pub fp_get_physical_device_external_image_format_properties_nv:
        Option<vk::FnGetPhysicalDeviceExternalImageFormatPropertiesNV>,
    pub fp_get_physical_device_features2: Option<vk::FnGetPhysicalDeviceFeatures2>,
    pub fp_get_physical_device_properties2: Option<vk::FnGetPhysicalDeviceProperties2>,
    pub fp_get_physical_device_format_properties2: Option<vk::FnGetPhysicalDeviceFormatProperties2>,
    pub fp_get_physical_device_image_format_properties2: Option<vk::FnGetPhysicalDeviceImageFormatProperties2>,
    pub fp_get_physical_device_queue_family_properties2: Option<vk::FnGetPhysicalDeviceQueueFamilyProperties2>,
    pub fp_get_physical_device_memory_properties2: Option<vk::FnGetPhysicalDeviceMemoryProperties2>,
    pub fp_get_physical_device_sparse_image_format_properties2:
        Option<vk::FnGetPhysicalDeviceSparseImageFormatProperties2>,
    pub fp_get_physical_device_external_buffer_properties: Option<vk::FnGetPhysicalDeviceExternalBufferProperties>,
    pub fp_get_physical_device_external_semaphore_properties:
        Option<vk::FnGetPhysicalDeviceExternalSemaphoreProperties>,
    pub fp_get_physical_device_external_fence_properties: Option<vk::FnGetPhysicalDeviceExternalFenceProperties>,
    pub fp_release_display_ext: Option<vk::FnReleaseDisplayEXT>,
    pub fp_acquire_xlib_display_ext: Option<vk::FnAcquireXlibDisplayEXT>,
    pub fp_get_rand_r_output_display_ext: Option<vk::FnGetRandROutputDisplayEXT>,
    pub fp_get_physical_device_surface_capabilities2_ext: Option<vk::FnGetPhysicalDeviceSurfaceCapabilities2EXT>,
    pub fp_enumerate_physical_device_groups: Option<vk::FnEnumeratePhysicalDeviceGroups>,
    pub fp_create_ios_surface_mvk: Option<vk::FnCreateIOSSurfaceMVK>,
    pub fp_create_mac_os_surface_mvk: Option<vk::FnCreateMacOSSurfaceMVK>,
    pub fp_create_metal_surface_ext: Option<vk::FnCreateMetalSurfaceEXT>,
    pub fp_get_physical_device_surface_capabilities2_khr: Option<vk::FnGetPhysicalDeviceSurfaceCapabilities2KHR>,
    pub fp_get_physical_device_surface_formats2_khr: Option<vk::FnGetPhysicalDeviceSurfaceFormats2KHR>,
    pub fp_get_physical_device_display_properties2_khr: Option<vk::FnGetPhysicalDeviceDisplayProperties2KHR>,
    pub fp_get_physical_device_display_plane_properties2_khr: Option<vk::FnGetPhysicalDeviceDisplayPlaneProperties2KHR>,
    pub fp_get_display_mode_properties2_khr: Option<vk::FnGetDisplayModeProperties2KHR>,
    pub fp_get_display_plane_capabilities2_khr: Option<vk::FnGetDisplayPlaneCapabilities2KHR>,
    pub fp_set_debug_utils_object_name_ext: Option<vk::FnSetDebugUtilsObjectNameEXT>,
    pub fp_set_debug_utils_object_tag_ext: Option<vk::FnSetDebugUtilsObjectTagEXT>,
    pub fp_queue_begin_debug_utils_label_ext: Option<vk::FnQueueBeginDebugUtilsLabelEXT>,
    pub fp_queue_end_debug_utils_label_ext: Option<vk::FnQueueEndDebugUtilsLabelEXT>,
    pub fp_queue_insert_debug_utils_label_ext: Option<vk::FnQueueInsertDebugUtilsLabelEXT>,
    pub fp_cmd_begin_debug_utils_label_ext: Option<vk::FnCmdBeginDebugUtilsLabelEXT>,
    pub fp_cmd_end_debug_utils_label_ext: Option<vk::FnCmdEndDebugUtilsLabelEXT>,
    pub fp_cmd_insert_debug_utils_label_ext: Option<vk::FnCmdInsertDebugUtilsLabelEXT>,
    pub fp_create_debug_utils_messenger_ext: Option<vk::FnCreateDebugUtilsMessengerEXT>,
    pub fp_destroy_debug_utils_messenger_ext: Option<vk::FnDestroyDebugUtilsMessengerEXT>,
    pub fp_submit_debug_utils_message_ext: Option<vk::FnSubmitDebugUtilsMessageEXT>,
    pub fp_create_headless_surface_ext: Option<vk::FnCreateHeadlessSurfaceEXT>,
    pub fp_acquire_drm_display_ext: Option<vk::FnAcquireDrmDisplayEXT>,
    pub fp_get_drm_display_ext: Option<vk::FnGetDrmDisplayEXT>,
}
impl Instance {
    pub unsafe fn create_device_commands(
        &self,
        globals: &Globals,
        physical_device: vk::PhysicalDevice,
        p_create_info: &vk::DeviceCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> LoadResult<Device> {
        let device = self.create_device(physical_device, p_create_info, p_allocator)?;
        Device::load(globals, self, device, p_create_info)
    }
    #[allow(clippy::cognitive_complexity, clippy::nonminimal_bool)]
    pub unsafe fn load(
        globals: &Globals,
        instance: vk::Instance,
        create_info: &vk::InstanceCreateInfo,
    ) -> LoadResult<Self> {
        let version = create_info
            .p_application_info
            .as_ref()
            .map(|app_info| app_info.api_version)
            .unwrap_or_default();
        let mut extensions = InstanceExtensions::new(version);
        if create_info.enabled_extension_count != 0 {
            for &name_ptr in slice::from_raw_parts(
                create_info.pp_enabled_extension_names,
                create_info.enabled_extension_count as usize,
            ) {
                extensions.enable_by_name(CStr::from_ptr(name_ptr));
            }
        }
        Ok(Self {
            handle: instance,
            extensions,
            fp_destroy_instance: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkDestroyInstance")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyInstance"))?,
            ),
            fp_enumerate_physical_devices: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkEnumeratePhysicalDevices")
                    .ok_or(LoadError::MissingSymbol(c"vkEnumeratePhysicalDevices"))?,
            ),
            fp_get_device_proc_addr: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetDeviceProcAddr")
                    .ok_or(LoadError::MissingSymbol(c"vkGetDeviceProcAddr"))?,
            ),
            fp_get_physical_device_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceProperties"))?,
            ),
            fp_get_physical_device_queue_family_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceQueueFamilyProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceQueueFamilyProperties"))?,
            ),
            fp_get_physical_device_memory_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceMemoryProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceMemoryProperties"))?,
            ),
            fp_get_physical_device_features: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFeatures")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceFeatures"))?,
            ),
            fp_get_physical_device_format_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFormatProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceFormatProperties"))?,
            ),
            fp_get_physical_device_image_format_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceImageFormatProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPhysicalDeviceImageFormatProperties"))?,
            ),
            fp_create_device: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDevice")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateDevice"))?,
            ),
            fp_enumerate_device_layer_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkEnumerateDeviceLayerProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkEnumerateDeviceLayerProperties"))?,
            ),
            fp_enumerate_device_extension_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkEnumerateDeviceExtensionProperties")
                    .ok_or(LoadError::MissingSymbol(c"vkEnumerateDeviceExtensionProperties"))?,
            ),
            fp_get_physical_device_sparse_image_format_properties: mem::transmute(
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSparseImageFormatProperties")
                    .ok_or(LoadError::MissingSymbol(
                        c"vkGetPhysicalDeviceSparseImageFormatProperties",
                    ))?,
            ),
            fp_create_android_surface_khr: if extensions.khr_android_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateAndroidSurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_surface_ohos: if extensions.ohos_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateSurfaceOHOS")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_display_properties_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceDisplayPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_display_plane_properties_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceDisplayPlanePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_display_plane_supported_displays_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDisplayPlaneSupportedDisplaysKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_display_mode_properties_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDisplayModePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_display_mode_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDisplayModeKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_display_plane_capabilities_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDisplayPlaneCapabilitiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_display_plane_surface_khr: if extensions.khr_display {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDisplayPlaneSurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_surface_khr: if extensions.khr_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkDestroySurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_support_khr: if extensions.khr_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_capabilities_khr: if extensions.khr_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceCapabilitiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_formats_khr: if extensions.khr_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceFormatsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_present_modes_khr: if extensions.khr_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfacePresentModesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_vi_surface_nn: if extensions.nn_vi_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateViSurfaceNN")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_wayland_surface_khr: if extensions.khr_wayland_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateWaylandSurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_wayland_presentation_support_khr: if extensions.khr_wayland_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceWaylandPresentationSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_ubm_surface_sec: if extensions.sec_ubm_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateUbmSurfaceSEC")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_ubm_presentation_support_sec: if extensions.sec_ubm_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceUbmPresentationSupportSEC")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_win32_surface_khr: if extensions.khr_win32_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateWin32SurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_win32_presentation_support_khr: if extensions.khr_win32_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceWin32PresentationSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_xlib_surface_khr: if extensions.khr_xlib_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateXlibSurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_xlib_presentation_support_khr: if extensions.khr_xlib_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceXlibPresentationSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_xcb_surface_khr: if extensions.khr_xcb_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateXcbSurfaceKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_xcb_presentation_support_khr: if extensions.khr_xcb_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceXcbPresentationSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_direct_fb_surface_ext: if extensions.ext_directfb_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDirectFBSurfaceEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_direct_fb_presentation_support_ext: if extensions.ext_directfb_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceDirectFBPresentationSupportEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_image_pipe_surface_fuchsia: if extensions.fuchsia_imagepipe_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateImagePipeSurfaceFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_debug_report_callback_ext: if extensions.ext_debug_report {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDebugReportCallbackEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_debug_report_callback_ext: if extensions.ext_debug_report {
                globals
                    .get_instance_proc_addr(instance, c"vkDestroyDebugReportCallbackEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_debug_report_message_ext: if extensions.ext_debug_report {
                globals
                    .get_instance_proc_addr(instance, c"vkDebugReportMessageEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_external_image_format_properties_nv: if extensions.nv_external_memory_capabilities {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalImageFormatPropertiesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_features2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFeatures2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFeatures2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_properties2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_format_properties2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFormatProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceFormatProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_image_format_properties2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceImageFormatProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceImageFormatProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_queue_family_properties2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceQueueFamilyProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceQueueFamilyProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_memory_properties2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceMemoryProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceMemoryProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_sparse_image_format_properties2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSparseImageFormatProperties2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_physical_device_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSparseImageFormatProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_external_buffer_properties: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalBufferProperties")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_external_memory_capabilities {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalBufferPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_external_semaphore_properties: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalSemaphoreProperties")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_external_semaphore_capabilities {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalSemaphorePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_external_fence_properties: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 1, 0)
            {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalFenceProperties")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_external_fence_capabilities {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceExternalFencePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_display_ext: if extensions.ext_direct_mode_display {
                globals
                    .get_instance_proc_addr(instance, c"vkReleaseDisplayEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_xlib_display_ext: if extensions.ext_acquire_xlib_display {
                globals
                    .get_instance_proc_addr(instance, c"vkAcquireXlibDisplayEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_rand_r_output_display_ext: if extensions.ext_acquire_xlib_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetRandROutputDisplayEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_capabilities2_ext: if extensions.ext_display_surface_counter {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceCapabilities2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_enumerate_physical_device_groups: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                globals
                    .get_instance_proc_addr(instance, c"vkEnumeratePhysicalDeviceGroups")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_device_group_creation {
                globals
                    .get_instance_proc_addr(instance, c"vkEnumeratePhysicalDeviceGroupsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_ios_surface_mvk: if extensions.mvk_ios_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateIOSSurfaceMVK")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_mac_os_surface_mvk: if extensions.mvk_macos_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateMacOSSurfaceMVK")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_metal_surface_ext: if extensions.ext_metal_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateMetalSurfaceEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_capabilities2_khr: if extensions.khr_get_surface_capabilities2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceCapabilities2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_formats2_khr: if extensions.khr_get_surface_capabilities2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceSurfaceFormats2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_display_properties2_khr: if extensions.khr_get_display_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceDisplayProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_display_plane_properties2_khr: if extensions.khr_get_display_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetPhysicalDeviceDisplayPlaneProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_display_mode_properties2_khr: if extensions.khr_get_display_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDisplayModeProperties2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_display_plane_capabilities2_khr: if extensions.khr_get_display_properties2 {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDisplayPlaneCapabilities2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_debug_utils_object_name_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkSetDebugUtilsObjectNameEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_debug_utils_object_tag_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkSetDebugUtilsObjectTagEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_begin_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkQueueBeginDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_end_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkQueueEndDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_insert_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkQueueInsertDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkCmdBeginDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkCmdEndDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_insert_debug_utils_label_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkCmdInsertDebugUtilsLabelEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_debug_utils_messenger_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateDebugUtilsMessengerEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_debug_utils_messenger_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkDestroyDebugUtilsMessengerEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_submit_debug_utils_message_ext: if extensions.ext_debug_utils {
                globals
                    .get_instance_proc_addr(instance, c"vkSubmitDebugUtilsMessageEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_headless_surface_ext: if extensions.ext_headless_surface {
                globals
                    .get_instance_proc_addr(instance, c"vkCreateHeadlessSurfaceEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_drm_display_ext: if extensions.ext_acquire_drm_display {
                globals
                    .get_instance_proc_addr(instance, c"vkAcquireDrmDisplayEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_drm_display_ext: if extensions.ext_acquire_drm_display {
                globals
                    .get_instance_proc_addr(instance, c"vkGetDrmDisplayEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
        })
    }
    pub unsafe fn destroy_instance(&self, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_instance;
        (fp)(self.handle, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn enumerate_physical_devices(
        &self,
        p_physical_device_count: &mut u32,
        p_physical_devices: *mut vk::PhysicalDevice,
    ) -> Result<EnumerateResult> {
        let fp = self.fp_enumerate_physical_devices;
        let err = (fp)(self.handle, p_physical_device_count, p_physical_devices);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_devices_to_vec(&self) -> Result<Vec<vk::PhysicalDevice>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_physical_devices(len, ptr))
    }
    pub unsafe fn get_device_proc_addr(&self, device: vk::Device, p_name: &CStr) -> Option<vk::FnVoidFunction> {
        let fp = self.fp_get_device_proc_addr;
        (fp)(device, p_name.as_ptr())
    }
    pub unsafe fn get_physical_device_properties(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> vk::PhysicalDeviceProperties {
        let fp = self.fp_get_physical_device_properties;
        let mut p_properties = MaybeUninit::<_>::uninit();
        (fp)(physical_device, p_properties.as_mut_ptr());
        p_properties.assume_init()
    }
    pub unsafe fn get_physical_device_queue_family_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_queue_family_property_count: &mut u32,
        p_queue_family_properties: *mut vk::QueueFamilyProperties,
    ) {
        let fp = self.fp_get_physical_device_queue_family_properties;
        (fp)(
            physical_device,
            p_queue_family_property_count,
            p_queue_family_properties,
        );
    }
    pub unsafe fn get_physical_device_queue_family_properties_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Vec<vk::QueueFamilyProperties> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_queue_family_properties(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_memory_properties(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> vk::PhysicalDeviceMemoryProperties {
        let fp = self.fp_get_physical_device_memory_properties;
        let mut p_memory_properties = MaybeUninit::<_>::uninit();
        (fp)(physical_device, p_memory_properties.as_mut_ptr());
        p_memory_properties.assume_init()
    }
    pub unsafe fn get_physical_device_features(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> vk::PhysicalDeviceFeatures {
        let fp = self.fp_get_physical_device_features;
        let mut p_features = MaybeUninit::<_>::uninit();
        (fp)(physical_device, p_features.as_mut_ptr());
        p_features.assume_init()
    }
    pub unsafe fn get_physical_device_format_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
    ) -> vk::FormatProperties {
        let fp = self.fp_get_physical_device_format_properties;
        let mut p_format_properties = MaybeUninit::<_>::uninit();
        (fp)(physical_device, format, p_format_properties.as_mut_ptr());
        p_format_properties.assume_init()
    }
    pub unsafe fn get_physical_device_image_format_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        ty: vk::ImageType,
        tiling: vk::ImageTiling,
        usage: vk::ImageUsageFlags,
        flags: vk::ImageCreateFlags,
    ) -> Result<vk::ImageFormatProperties> {
        let fp = self.fp_get_physical_device_image_format_properties;
        let mut p_image_format_properties = MaybeUninit::<_>::uninit();
        let err = (fp)(
            physical_device,
            format,
            ty,
            tiling,
            usage,
            flags,
            p_image_format_properties.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_image_format_properties.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_device(
        &self,
        physical_device: vk::PhysicalDevice,
        p_create_info: &vk::DeviceCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Device> {
        let fp = self.fp_create_device;
        let mut p_device = MaybeUninit::<_>::uninit();
        let err = (fp)(
            physical_device,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_device.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_device.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_device_layer_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::LayerProperties,
    ) -> Result<EnumerateResult> {
        let fp = self.fp_enumerate_device_layer_properties;
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_device_layer_properties_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::LayerProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_device_layer_properties(physical_device, len, ptr))
    }
    pub unsafe fn enumerate_device_extension_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_layer_name: Option<&CStr>,
        p_property_count: &mut u32,
        p_properties: *mut vk::ExtensionProperties,
    ) -> Result<EnumerateResult> {
        let fp = self.fp_enumerate_device_extension_properties;
        let err = (fp)(
            physical_device,
            p_layer_name.map_or(ptr::null(), |s| s.as_ptr()),
            p_property_count,
            p_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_device_extension_properties_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_layer_name: Option<&CStr>,
    ) -> Result<Vec<vk::ExtensionProperties>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.enumerate_device_extension_properties(physical_device, p_layer_name, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        ty: vk::ImageType,
        samples: vk::SampleCountFlags,
        usage: vk::ImageUsageFlags,
        tiling: vk::ImageTiling,
        p_property_count: &mut u32,
        p_properties: *mut vk::SparseImageFormatProperties,
    ) {
        let fp = self.fp_get_physical_device_sparse_image_format_properties;
        (fp)(
            physical_device,
            format,
            ty,
            samples,
            usage,
            tiling,
            p_property_count,
            p_properties,
        );
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        ty: vk::ImageType,
        samples: vk::SampleCountFlags,
        usage: vk::ImageUsageFlags,
        tiling: vk::ImageTiling,
    ) -> Vec<vk::SparseImageFormatProperties> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_sparse_image_format_properties(
                physical_device,
                format,
                ty,
                samples,
                usage,
                tiling,
                len,
                ptr,
            )
        })
    }
    pub unsafe fn create_android_surface_khr(
        &self,
        p_create_info: &vk::AndroidSurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_android_surface_khr
            .expect("vkCreateAndroidSurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_surface_ohos(
        &self,
        p_create_info: &vk::SurfaceCreateInfoOHOS,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self.fp_create_surface_ohos.expect("vkCreateSurfaceOHOS is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_display_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayPropertiesKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_display_properties_khr
            .expect("vkGetPhysicalDeviceDisplayPropertiesKHR is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_display_properties_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::DisplayPropertiesKHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_physical_device_display_properties_khr(physical_device, len, ptr))
    }
    pub unsafe fn get_physical_device_display_plane_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayPlanePropertiesKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_display_plane_properties_khr
            .expect("vkGetPhysicalDeviceDisplayPlanePropertiesKHR is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_display_plane_properties_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::DisplayPlanePropertiesKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_display_plane_properties_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_display_plane_supported_displays_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        plane_index: u32,
        p_display_count: &mut u32,
        p_displays: *mut vk::DisplayKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_display_plane_supported_displays_khr
            .expect("vkGetDisplayPlaneSupportedDisplaysKHR is not loaded");
        let err = (fp)(physical_device, plane_index, p_display_count, p_displays);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_display_plane_supported_displays_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        plane_index: u32,
    ) -> Result<Vec<vk::DisplayKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_display_plane_supported_displays_khr(physical_device, plane_index, len, ptr)
        })
    }
    pub unsafe fn get_display_mode_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayModePropertiesKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_display_mode_properties_khr
            .expect("vkGetDisplayModePropertiesKHR is not loaded");
        let err = (fp)(physical_device, display, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_display_mode_properties_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
    ) -> Result<Vec<vk::DisplayModePropertiesKHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_display_mode_properties_khr(physical_device, display, len, ptr))
    }
    pub unsafe fn create_display_mode_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
        p_create_info: &vk::DisplayModeCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DisplayModeKHR> {
        let fp = self
            .fp_create_display_mode_khr
            .expect("vkCreateDisplayModeKHR is not loaded");
        let mut p_mode = MaybeUninit::<_>::uninit();
        let err = (fp)(
            physical_device,
            display,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_mode.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_mode.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_display_plane_capabilities_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        mode: vk::DisplayModeKHR,
        plane_index: u32,
    ) -> Result<vk::DisplayPlaneCapabilitiesKHR> {
        let fp = self
            .fp_get_display_plane_capabilities_khr
            .expect("vkGetDisplayPlaneCapabilitiesKHR is not loaded");
        let mut p_capabilities = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, mode, plane_index, p_capabilities.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_capabilities.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_display_plane_surface_khr(
        &self,
        p_create_info: &vk::DisplaySurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_display_plane_surface_khr
            .expect("vkCreateDisplayPlaneSurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_surface_khr(&self, surface: vk::SurfaceKHR, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_surface_khr.expect("vkDestroySurfaceKHR is not loaded");
        (fp)(self.handle, surface, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_physical_device_surface_support_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        surface: vk::SurfaceKHR,
    ) -> Result<bool> {
        let fp = self
            .fp_get_physical_device_surface_support_khr
            .expect("vkGetPhysicalDeviceSurfaceSupportKHR is not loaded");
        let mut p_supported = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, queue_family_index, surface, p_supported.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_supported.assume_init() != vk::FALSE),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_capabilities_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
    ) -> Result<vk::SurfaceCapabilitiesKHR> {
        let fp = self
            .fp_get_physical_device_surface_capabilities_khr
            .expect("vkGetPhysicalDeviceSurfaceCapabilitiesKHR is not loaded");
        let mut p_surface_capabilities = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, surface, p_surface_capabilities.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_surface_capabilities.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_formats_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
        p_surface_format_count: &mut u32,
        p_surface_formats: *mut vk::SurfaceFormatKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_surface_formats_khr
            .expect("vkGetPhysicalDeviceSurfaceFormatsKHR is not loaded");
        let err = (fp)(physical_device, surface, p_surface_format_count, p_surface_formats);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_formats_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
    ) -> Result<Vec<vk::SurfaceFormatKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_surface_formats_khr(physical_device, surface, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_surface_present_modes_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
        p_present_mode_count: &mut u32,
        p_present_modes: *mut vk::PresentModeKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_surface_present_modes_khr
            .expect("vkGetPhysicalDeviceSurfacePresentModesKHR is not loaded");
        let err = (fp)(physical_device, surface, p_present_mode_count, p_present_modes);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_present_modes_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
    ) -> Result<Vec<vk::PresentModeKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_surface_present_modes_khr(physical_device, surface, len, ptr)
        })
    }
    pub unsafe fn create_vi_surface_nn(
        &self,
        p_create_info: &vk::ViSurfaceCreateInfoNN,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self.fp_create_vi_surface_nn.expect("vkCreateViSurfaceNN is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_wayland_surface_khr(
        &self,
        p_create_info: &vk::WaylandSurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_wayland_surface_khr
            .expect("vkCreateWaylandSurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_wayland_presentation_support_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        display: &mut vk::wl_display,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_wayland_presentation_support_khr
            .expect("vkGetPhysicalDeviceWaylandPresentationSupportKHR is not loaded");
        (fp)(physical_device, queue_family_index, display) != vk::FALSE
    }
    pub unsafe fn create_ubm_surface_sec(
        &self,
        p_create_info: &vk::UbmSurfaceCreateInfoSEC,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_ubm_surface_sec
            .expect("vkCreateUbmSurfaceSEC is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_ubm_presentation_support_sec(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        device: &mut vk::ubm_device,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_ubm_presentation_support_sec
            .expect("vkGetPhysicalDeviceUbmPresentationSupportSEC is not loaded");
        (fp)(physical_device, queue_family_index, device) != vk::FALSE
    }
    pub unsafe fn create_win32_surface_khr(
        &self,
        p_create_info: &vk::Win32SurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_win32_surface_khr
            .expect("vkCreateWin32SurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_win32_presentation_support_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_win32_presentation_support_khr
            .expect("vkGetPhysicalDeviceWin32PresentationSupportKHR is not loaded");
        (fp)(physical_device, queue_family_index) != vk::FALSE
    }
    pub unsafe fn create_xlib_surface_khr(
        &self,
        p_create_info: &vk::XlibSurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_xlib_surface_khr
            .expect("vkCreateXlibSurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_xlib_presentation_support_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        dpy: &mut vk::Display,
        visual_id: vk::VisualID,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_xlib_presentation_support_khr
            .expect("vkGetPhysicalDeviceXlibPresentationSupportKHR is not loaded");
        (fp)(physical_device, queue_family_index, dpy, visual_id) != vk::FALSE
    }
    pub unsafe fn create_xcb_surface_khr(
        &self,
        p_create_info: &vk::XcbSurfaceCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_xcb_surface_khr
            .expect("vkCreateXcbSurfaceKHR is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_xcb_presentation_support_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        connection: &mut vk::xcb_connection_t,
        visual_id: vk::xcb_visualid_t,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_xcb_presentation_support_khr
            .expect("vkGetPhysicalDeviceXcbPresentationSupportKHR is not loaded");
        (fp)(physical_device, queue_family_index, connection, visual_id) != vk::FALSE
    }
    pub unsafe fn create_direct_fb_surface_ext(
        &self,
        p_create_info: &vk::DirectFBSurfaceCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_direct_fb_surface_ext
            .expect("vkCreateDirectFBSurfaceEXT is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_direct_fb_presentation_support_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        dfb: &mut vk::IDirectFB,
    ) -> bool {
        let fp = self
            .fp_get_physical_device_direct_fb_presentation_support_ext
            .expect("vkGetPhysicalDeviceDirectFBPresentationSupportEXT is not loaded");
        (fp)(physical_device, queue_family_index, dfb) != vk::FALSE
    }
    pub unsafe fn create_image_pipe_surface_fuchsia(
        &self,
        p_create_info: &vk::ImagePipeSurfaceCreateInfoFUCHSIA,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_image_pipe_surface_fuchsia
            .expect("vkCreateImagePipeSurfaceFUCHSIA is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_debug_report_callback_ext(
        &self,
        p_create_info: &vk::DebugReportCallbackCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DebugReportCallbackEXT> {
        let fp = self
            .fp_create_debug_report_callback_ext
            .expect("vkCreateDebugReportCallbackEXT is not loaded");
        let mut p_callback = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_callback.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_callback.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_debug_report_callback_ext(
        &self,
        callback: vk::DebugReportCallbackEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_debug_report_callback_ext
            .expect("vkDestroyDebugReportCallbackEXT is not loaded");
        (fp)(self.handle, callback, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn debug_report_message_ext(
        &self,
        flags: vk::DebugReportFlagsEXT,
        object_type: vk::DebugReportObjectTypeEXT,
        object: u64,
        location: usize,
        message_code: i32,
        p_layer_prefix: &CStr,
        p_message: &CStr,
    ) {
        let fp = self
            .fp_debug_report_message_ext
            .expect("vkDebugReportMessageEXT is not loaded");
        (fp)(
            self.handle,
            flags,
            object_type,
            object,
            location,
            message_code,
            p_layer_prefix.as_ptr(),
            p_message.as_ptr(),
        )
    }
    pub unsafe fn get_physical_device_external_image_format_properties_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        ty: vk::ImageType,
        tiling: vk::ImageTiling,
        usage: vk::ImageUsageFlags,
        flags: vk::ImageCreateFlags,
        external_handle_type: vk::ExternalMemoryHandleTypeFlagsNV,
    ) -> Result<vk::ExternalImageFormatPropertiesNV> {
        let fp = self
            .fp_get_physical_device_external_image_format_properties_nv
            .expect("vkGetPhysicalDeviceExternalImageFormatPropertiesNV is not loaded");
        let mut p_external_image_format_properties = MaybeUninit::<_>::uninit();
        let err = (fp)(
            physical_device,
            format,
            ty,
            tiling,
            usage,
            flags,
            external_handle_type,
            p_external_image_format_properties.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_external_image_format_properties.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_features2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_features: &mut vk::PhysicalDeviceFeatures2,
    ) {
        let fp = self
            .fp_get_physical_device_features2
            .expect("vkGetPhysicalDeviceFeatures2 is not loaded");
        (fp)(physical_device, p_features)
    }
    pub unsafe fn get_physical_device_features2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_features: &mut vk::PhysicalDeviceFeatures2,
    ) {
        let fp = self
            .fp_get_physical_device_features2
            .expect("vkGetPhysicalDeviceFeatures2KHR is not loaded");
        (fp)(physical_device, p_features)
    }
    pub unsafe fn get_physical_device_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_properties: &mut vk::PhysicalDeviceProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_properties2
            .expect("vkGetPhysicalDeviceProperties2 is not loaded");
        (fp)(physical_device, p_properties)
    }
    pub unsafe fn get_physical_device_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_properties: &mut vk::PhysicalDeviceProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_properties2
            .expect("vkGetPhysicalDeviceProperties2KHR is not loaded");
        (fp)(physical_device, p_properties)
    }
    pub unsafe fn get_physical_device_format_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        p_format_properties: &mut vk::FormatProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_format_properties2
            .expect("vkGetPhysicalDeviceFormatProperties2 is not loaded");
        (fp)(physical_device, format, p_format_properties)
    }
    pub unsafe fn get_physical_device_format_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        format: vk::Format,
        p_format_properties: &mut vk::FormatProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_format_properties2
            .expect("vkGetPhysicalDeviceFormatProperties2KHR is not loaded");
        (fp)(physical_device, format, p_format_properties)
    }
    pub unsafe fn get_physical_device_image_format_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_image_format_info: &vk::PhysicalDeviceImageFormatInfo2,
        p_image_format_properties: &mut vk::ImageFormatProperties2,
    ) -> Result<()> {
        let fp = self
            .fp_get_physical_device_image_format_properties2
            .expect("vkGetPhysicalDeviceImageFormatProperties2 is not loaded");
        let err = (fp)(physical_device, p_image_format_info, p_image_format_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_image_format_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_image_format_info: &vk::PhysicalDeviceImageFormatInfo2,
        p_image_format_properties: &mut vk::ImageFormatProperties2,
    ) -> Result<()> {
        let fp = self
            .fp_get_physical_device_image_format_properties2
            .expect("vkGetPhysicalDeviceImageFormatProperties2KHR is not loaded");
        let err = (fp)(physical_device, p_image_format_info, p_image_format_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_queue_family_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_queue_family_property_count: &mut u32,
        p_queue_family_properties: *mut vk::QueueFamilyProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_queue_family_properties2
            .expect("vkGetPhysicalDeviceQueueFamilyProperties2 is not loaded");
        (fp)(
            physical_device,
            p_queue_family_property_count,
            p_queue_family_properties,
        );
    }
    pub unsafe fn get_physical_device_queue_family_properties2_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Vec<vk::QueueFamilyProperties2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_queue_family_properties2(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_queue_family_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_queue_family_property_count: &mut u32,
        p_queue_family_properties: *mut vk::QueueFamilyProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_queue_family_properties2
            .expect("vkGetPhysicalDeviceQueueFamilyProperties2KHR is not loaded");
        (fp)(
            physical_device,
            p_queue_family_property_count,
            p_queue_family_properties,
        );
    }
    pub unsafe fn get_physical_device_queue_family_properties2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Vec<vk::QueueFamilyProperties2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_queue_family_properties2_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_memory_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_memory_properties: &mut vk::PhysicalDeviceMemoryProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_memory_properties2
            .expect("vkGetPhysicalDeviceMemoryProperties2 is not loaded");
        (fp)(physical_device, p_memory_properties)
    }
    pub unsafe fn get_physical_device_memory_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_memory_properties: &mut vk::PhysicalDeviceMemoryProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_memory_properties2
            .expect("vkGetPhysicalDeviceMemoryProperties2KHR is not loaded");
        (fp)(physical_device, p_memory_properties)
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties2(
        &self,
        physical_device: vk::PhysicalDevice,
        p_format_info: &vk::PhysicalDeviceSparseImageFormatInfo2,
        p_property_count: &mut u32,
        p_properties: *mut vk::SparseImageFormatProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_sparse_image_format_properties2
            .expect("vkGetPhysicalDeviceSparseImageFormatProperties2 is not loaded");
        (fp)(physical_device, p_format_info, p_property_count, p_properties);
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties2_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_format_info: &vk::PhysicalDeviceSparseImageFormatInfo2,
    ) -> Vec<vk::SparseImageFormatProperties2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_sparse_image_format_properties2(physical_device, p_format_info, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_format_info: &vk::PhysicalDeviceSparseImageFormatInfo2,
        p_property_count: &mut u32,
        p_properties: *mut vk::SparseImageFormatProperties2,
    ) {
        let fp = self
            .fp_get_physical_device_sparse_image_format_properties2
            .expect("vkGetPhysicalDeviceSparseImageFormatProperties2KHR is not loaded");
        (fp)(physical_device, p_format_info, p_property_count, p_properties);
    }
    pub unsafe fn get_physical_device_sparse_image_format_properties2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_format_info: &vk::PhysicalDeviceSparseImageFormatInfo2,
    ) -> Vec<vk::SparseImageFormatProperties2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_physical_device_sparse_image_format_properties2_khr(physical_device, p_format_info, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_external_buffer_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_buffer_info: &vk::PhysicalDeviceExternalBufferInfo,
        p_external_buffer_properties: &mut vk::ExternalBufferProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_buffer_properties
            .expect("vkGetPhysicalDeviceExternalBufferProperties is not loaded");
        (fp)(physical_device, p_external_buffer_info, p_external_buffer_properties)
    }
    pub unsafe fn get_physical_device_external_buffer_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_buffer_info: &vk::PhysicalDeviceExternalBufferInfo,
        p_external_buffer_properties: &mut vk::ExternalBufferProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_buffer_properties
            .expect("vkGetPhysicalDeviceExternalBufferPropertiesKHR is not loaded");
        (fp)(physical_device, p_external_buffer_info, p_external_buffer_properties)
    }
    pub unsafe fn get_physical_device_external_semaphore_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_semaphore_info: &vk::PhysicalDeviceExternalSemaphoreInfo,
        p_external_semaphore_properties: &mut vk::ExternalSemaphoreProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_semaphore_properties
            .expect("vkGetPhysicalDeviceExternalSemaphoreProperties is not loaded");
        (fp)(
            physical_device,
            p_external_semaphore_info,
            p_external_semaphore_properties,
        )
    }
    pub unsafe fn get_physical_device_external_semaphore_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_semaphore_info: &vk::PhysicalDeviceExternalSemaphoreInfo,
        p_external_semaphore_properties: &mut vk::ExternalSemaphoreProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_semaphore_properties
            .expect("vkGetPhysicalDeviceExternalSemaphorePropertiesKHR is not loaded");
        (fp)(
            physical_device,
            p_external_semaphore_info,
            p_external_semaphore_properties,
        )
    }
    pub unsafe fn get_physical_device_external_fence_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_fence_info: &vk::PhysicalDeviceExternalFenceInfo,
        p_external_fence_properties: &mut vk::ExternalFenceProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_fence_properties
            .expect("vkGetPhysicalDeviceExternalFenceProperties is not loaded");
        (fp)(physical_device, p_external_fence_info, p_external_fence_properties)
    }
    pub unsafe fn get_physical_device_external_fence_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_fence_info: &vk::PhysicalDeviceExternalFenceInfo,
        p_external_fence_properties: &mut vk::ExternalFenceProperties,
    ) {
        let fp = self
            .fp_get_physical_device_external_fence_properties
            .expect("vkGetPhysicalDeviceExternalFencePropertiesKHR is not loaded");
        (fp)(physical_device, p_external_fence_info, p_external_fence_properties)
    }
    pub unsafe fn release_display_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
    ) -> Result<()> {
        let fp = self.fp_release_display_ext.expect("vkReleaseDisplayEXT is not loaded");
        let err = (fp)(physical_device, display);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_xlib_display_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        dpy: &mut vk::Display,
        display: vk::DisplayKHR,
    ) -> Result<()> {
        let fp = self
            .fp_acquire_xlib_display_ext
            .expect("vkAcquireXlibDisplayEXT is not loaded");
        let err = (fp)(physical_device, dpy, display);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_rand_r_output_display_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        dpy: &mut vk::Display,
        rr_output: vk::RROutput,
    ) -> Result<vk::DisplayKHR> {
        let fp = self
            .fp_get_rand_r_output_display_ext
            .expect("vkGetRandROutputDisplayEXT is not loaded");
        let mut p_display = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, dpy, rr_output, p_display.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_display.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_capabilities2_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
        p_surface_capabilities: &mut vk::SurfaceCapabilities2EXT,
    ) -> Result<()> {
        let fp = self
            .fp_get_physical_device_surface_capabilities2_ext
            .expect("vkGetPhysicalDeviceSurfaceCapabilities2EXT is not loaded");
        let err = (fp)(physical_device, surface, p_surface_capabilities);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_device_groups(
        &self,
        p_physical_device_group_count: &mut u32,
        p_physical_device_group_properties: *mut vk::PhysicalDeviceGroupProperties,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_enumerate_physical_device_groups
            .expect("vkEnumeratePhysicalDeviceGroups is not loaded");
        let err = (fp)(
            self.handle,
            p_physical_device_group_count,
            p_physical_device_group_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_device_groups_to_vec(&self) -> Result<Vec<vk::PhysicalDeviceGroupProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_physical_device_groups(len, ptr))
    }
    pub unsafe fn enumerate_physical_device_groups_khr(
        &self,
        p_physical_device_group_count: &mut u32,
        p_physical_device_group_properties: *mut vk::PhysicalDeviceGroupProperties,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_enumerate_physical_device_groups
            .expect("vkEnumeratePhysicalDeviceGroupsKHR is not loaded");
        let err = (fp)(
            self.handle,
            p_physical_device_group_count,
            p_physical_device_group_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_device_groups_khr_to_vec(&self) -> Result<Vec<vk::PhysicalDeviceGroupProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.enumerate_physical_device_groups_khr(len, ptr))
    }
    pub unsafe fn create_ios_surface_mvk(
        &self,
        p_create_info: &vk::IOSSurfaceCreateInfoMVK,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_ios_surface_mvk
            .expect("vkCreateIOSSurfaceMVK is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_mac_os_surface_mvk(
        &self,
        p_create_info: &vk::MacOSSurfaceCreateInfoMVK,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_mac_os_surface_mvk
            .expect("vkCreateMacOSSurfaceMVK is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_metal_surface_ext(
        &self,
        p_create_info: &vk::MetalSurfaceCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_metal_surface_ext
            .expect("vkCreateMetalSurfaceEXT is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_capabilities2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
        p_surface_capabilities: &mut vk::SurfaceCapabilities2KHR,
    ) -> Result<()> {
        let fp = self
            .fp_get_physical_device_surface_capabilities2_khr
            .expect("vkGetPhysicalDeviceSurfaceCapabilities2KHR is not loaded");
        let err = (fp)(physical_device, p_surface_info, p_surface_capabilities);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_formats2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
        p_surface_format_count: &mut u32,
        p_surface_formats: *mut vk::SurfaceFormat2KHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_surface_formats2_khr
            .expect("vkGetPhysicalDeviceSurfaceFormats2KHR is not loaded");
        let err = (fp)(
            physical_device,
            p_surface_info,
            p_surface_format_count,
            p_surface_formats,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_formats2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
    ) -> Result<Vec<vk::SurfaceFormat2KHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_surface_formats2_khr(physical_device, p_surface_info, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_display_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayProperties2KHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_display_properties2_khr
            .expect("vkGetPhysicalDeviceDisplayProperties2KHR is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_display_properties2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::DisplayProperties2KHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_physical_device_display_properties2_khr(physical_device, len, ptr))
    }
    pub unsafe fn get_physical_device_display_plane_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayPlaneProperties2KHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_display_plane_properties2_khr
            .expect("vkGetPhysicalDeviceDisplayPlaneProperties2KHR is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_display_plane_properties2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::DisplayPlaneProperties2KHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_display_plane_properties2_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_display_mode_properties2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
        p_property_count: &mut u32,
        p_properties: *mut vk::DisplayModeProperties2KHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_display_mode_properties2_khr
            .expect("vkGetDisplayModeProperties2KHR is not loaded");
        let err = (fp)(physical_device, display, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_display_mode_properties2_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
    ) -> Result<Vec<vk::DisplayModeProperties2KHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_display_mode_properties2_khr(physical_device, display, len, ptr))
    }
    pub unsafe fn get_display_plane_capabilities2_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_display_plane_info: &vk::DisplayPlaneInfo2KHR,
        p_capabilities: &mut vk::DisplayPlaneCapabilities2KHR,
    ) -> Result<()> {
        let fp = self
            .fp_get_display_plane_capabilities2_khr
            .expect("vkGetDisplayPlaneCapabilities2KHR is not loaded");
        let err = (fp)(physical_device, p_display_plane_info, p_capabilities);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_debug_utils_object_name_ext(
        &self,
        device: vk::Device,
        p_name_info: &vk::DebugUtilsObjectNameInfoEXT,
    ) -> Result<()> {
        let fp = self
            .fp_set_debug_utils_object_name_ext
            .expect("vkSetDebugUtilsObjectNameEXT is not loaded");
        let err = (fp)(device, p_name_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_debug_utils_object_tag_ext(
        &self,
        device: vk::Device,
        p_tag_info: &vk::DebugUtilsObjectTagInfoEXT,
    ) -> Result<()> {
        let fp = self
            .fp_set_debug_utils_object_tag_ext
            .expect("vkSetDebugUtilsObjectTagEXT is not loaded");
        let err = (fp)(device, p_tag_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn queue_begin_debug_utils_label_ext(&self, queue: vk::Queue, p_label_info: &vk::DebugUtilsLabelEXT) {
        let fp = self
            .fp_queue_begin_debug_utils_label_ext
            .expect("vkQueueBeginDebugUtilsLabelEXT is not loaded");
        (fp)(queue, p_label_info)
    }
    pub unsafe fn queue_end_debug_utils_label_ext(&self, queue: vk::Queue) {
        let fp = self
            .fp_queue_end_debug_utils_label_ext
            .expect("vkQueueEndDebugUtilsLabelEXT is not loaded");
        (fp)(queue)
    }
    pub unsafe fn queue_insert_debug_utils_label_ext(&self, queue: vk::Queue, p_label_info: &vk::DebugUtilsLabelEXT) {
        let fp = self
            .fp_queue_insert_debug_utils_label_ext
            .expect("vkQueueInsertDebugUtilsLabelEXT is not loaded");
        (fp)(queue, p_label_info)
    }
    pub unsafe fn cmd_begin_debug_utils_label_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_label_info: &vk::DebugUtilsLabelEXT,
    ) {
        let fp = self
            .fp_cmd_begin_debug_utils_label_ext
            .expect("vkCmdBeginDebugUtilsLabelEXT is not loaded");
        (fp)(command_buffer, p_label_info)
    }
    pub unsafe fn cmd_end_debug_utils_label_ext(&self, command_buffer: vk::CommandBuffer) {
        let fp = self
            .fp_cmd_end_debug_utils_label_ext
            .expect("vkCmdEndDebugUtilsLabelEXT is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_insert_debug_utils_label_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_label_info: &vk::DebugUtilsLabelEXT,
    ) {
        let fp = self
            .fp_cmd_insert_debug_utils_label_ext
            .expect("vkCmdInsertDebugUtilsLabelEXT is not loaded");
        (fp)(command_buffer, p_label_info)
    }
    pub unsafe fn create_debug_utils_messenger_ext(
        &self,
        p_create_info: &vk::DebugUtilsMessengerCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DebugUtilsMessengerEXT> {
        let fp = self
            .fp_create_debug_utils_messenger_ext
            .expect("vkCreateDebugUtilsMessengerEXT is not loaded");
        let mut p_messenger = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_messenger.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_messenger.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_debug_utils_messenger_ext(
        &self,
        messenger: vk::DebugUtilsMessengerEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_debug_utils_messenger_ext
            .expect("vkDestroyDebugUtilsMessengerEXT is not loaded");
        (fp)(self.handle, messenger, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn submit_debug_utils_message_ext(
        &self,
        message_severity: vk::DebugUtilsMessageSeverityFlagsEXT,
        message_types: vk::DebugUtilsMessageTypeFlagsEXT,
        p_callback_data: &vk::DebugUtilsMessengerCallbackDataEXT,
    ) {
        let fp = self
            .fp_submit_debug_utils_message_ext
            .expect("vkSubmitDebugUtilsMessageEXT is not loaded");
        (fp)(self.handle, message_severity, message_types, p_callback_data)
    }
    pub unsafe fn create_headless_surface_ext(
        &self,
        p_create_info: &vk::HeadlessSurfaceCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SurfaceKHR> {
        let fp = self
            .fp_create_headless_surface_ext
            .expect("vkCreateHeadlessSurfaceEXT is not loaded");
        let mut p_surface = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_surface.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_surface.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_drm_display_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        drm_fd: i32,
        display: vk::DisplayKHR,
    ) -> Result<()> {
        let fp = self
            .fp_acquire_drm_display_ext
            .expect("vkAcquireDrmDisplayEXT is not loaded");
        let err = (fp)(physical_device, drm_fd, display);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_drm_display_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        drm_fd: i32,
        connector_id: u32,
    ) -> Result<vk::DisplayKHR> {
        let fp = self.fp_get_drm_display_ext.expect("vkGetDrmDisplayEXT is not loaded");
        let mut display = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, drm_fd, connector_id, display.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(display.assume_init()),
            _ => Err(err),
        }
    }
}
#[derive(Debug, Copy, Clone)]
pub struct DeviceExtensions {
    pub core_version: vk::Version,
    pub khr_swapchain: bool,
    pub khr_display_swapchain: bool,
    pub nv_glsl_shader: bool,
    pub ext_depth_range_unrestricted: bool,
    pub khr_sampler_mirror_clamp_to_edge: bool,
    pub img_filter_cubic: bool,
    pub amd_rasterization_order: bool,
    pub amd_shader_trinary_minmax: bool,
    pub amd_shader_explicit_vertex_parameter: bool,
    pub ext_debug_marker: bool,
    pub amd_gcn_shader: bool,
    pub nv_dedicated_allocation: bool,
    pub ext_transform_feedback: bool,
    pub nvx_binary_import: bool,
    pub nvx_image_view_handle: bool,
    pub amd_draw_indirect_count: bool,
    pub amd_negative_viewport_height: bool,
    pub amd_gpu_shader_half_float: bool,
    pub amd_shader_ballot: bool,
    pub amd_texture_gather_bias_lod: bool,
    pub amd_shader_info: bool,
    pub khr_dynamic_rendering: bool,
    pub amd_shader_image_load_store_lod: bool,
    pub nv_corner_sampled_image: bool,
    pub khr_multiview: bool,
    pub img_format_pvrtc: bool,
    pub nv_external_memory: bool,
    pub nv_external_memory_win32: bool,
    pub nv_win32_keyed_mutex: bool,
    pub khr_device_group: bool,
    pub khr_shader_draw_parameters: bool,
    pub ext_shader_subgroup_ballot: bool,
    pub ext_shader_subgroup_vote: bool,
    pub ext_texture_compression_astc_hdr: bool,
    pub ext_astc_decode_mode: bool,
    pub ext_pipeline_robustness: bool,
    pub khr_maintenance1: bool,
    pub khr_external_memory: bool,
    pub khr_external_memory_win32: bool,
    pub khr_external_memory_fd: bool,
    pub khr_win32_keyed_mutex: bool,
    pub khr_external_semaphore: bool,
    pub khr_external_semaphore_win32: bool,
    pub khr_external_semaphore_fd: bool,
    pub khr_push_descriptor: bool,
    pub ext_conditional_rendering: bool,
    pub khr_shader_float16_int8: bool,
    pub khr_16bit_storage: bool,
    pub khr_incremental_present: bool,
    pub khr_descriptor_update_template: bool,
    pub nv_clip_space_w_scaling: bool,
    pub ext_display_control: bool,
    pub google_display_timing: bool,
    pub nv_sample_mask_override_coverage: bool,
    pub nv_geometry_shader_passthrough: bool,
    pub nv_viewport_array2: bool,
    pub nvx_multiview_per_view_attributes: bool,
    pub nv_viewport_swizzle: bool,
    pub ext_discard_rectangles: bool,
    pub ext_conservative_rasterization: bool,
    pub ext_depth_clip_enable: bool,
    pub ext_hdr_metadata: bool,
    pub khr_imageless_framebuffer: bool,
    pub khr_create_renderpass2: bool,
    pub img_relaxed_line_rasterization: bool,
    pub khr_shared_presentable_image: bool,
    pub khr_external_fence: bool,
    pub khr_external_fence_win32: bool,
    pub khr_external_fence_fd: bool,
    pub khr_performance_query: bool,
    pub khr_maintenance2: bool,
    pub khr_variable_pointers: bool,
    pub ext_external_memory_dma_buf: bool,
    pub ext_queue_family_foreign: bool,
    pub khr_dedicated_allocation: bool,
    pub android_external_memory_android_hardware_buffer: bool,
    pub ext_sampler_filter_minmax: bool,
    pub khr_storage_buffer_storage_class: bool,
    pub amd_gpu_shader_int16: bool,
    pub amdx_shader_enqueue: bool,
    pub ext_descriptor_heap: bool,
    pub amd_mixed_attachment_samples: bool,
    pub amd_shader_fragment_mask: bool,
    pub ext_inline_uniform_block: bool,
    pub ext_shader_stencil_export: bool,
    pub khr_shader_bfloat16: bool,
    pub ext_sample_locations: bool,
    pub khr_relaxed_block_layout: bool,
    pub khr_get_memory_requirements2: bool,
    pub khr_image_format_list: bool,
    pub ext_blend_operation_advanced: bool,
    pub nv_fragment_coverage_to_color: bool,
    pub khr_acceleration_structure: bool,
    pub khr_ray_tracing_pipeline: bool,
    pub khr_ray_query: bool,
    pub nv_framebuffer_mixed_samples: bool,
    pub nv_fill_rectangle: bool,
    pub nv_shader_sm_builtins: bool,
    pub ext_post_depth_coverage: bool,
    pub khr_sampler_ycbcr_conversion: bool,
    pub khr_bind_memory2: bool,
    pub ext_image_drm_format_modifier: bool,
    pub ext_validation_cache: bool,
    pub ext_descriptor_indexing: bool,
    pub ext_shader_viewport_index_layer: bool,
    pub khr_portability_subset: bool,
    pub nv_shading_rate_image: bool,
    pub nv_ray_tracing: bool,
    pub nv_representative_fragment_test: bool,
    pub khr_maintenance3: bool,
    pub khr_draw_indirect_count: bool,
    pub ext_filter_cubic: bool,
    pub qcom_render_pass_shader_resolve: bool,
    pub qcom_cooperative_matrix_conversion: bool,
    pub ext_global_priority: bool,
    pub khr_shader_subgroup_extended_types: bool,
    pub khr_8bit_storage: bool,
    pub ext_external_memory_host: bool,
    pub amd_buffer_marker: bool,
    pub khr_shader_atomic_int64: bool,
    pub khr_shader_clock: bool,
    pub amd_pipeline_compiler_control: bool,
    pub ext_calibrated_timestamps: bool,
    pub amd_shader_core_properties: bool,
    pub khr_global_priority: bool,
    pub amd_memory_overallocation_behavior: bool,
    pub ext_vertex_attribute_divisor: bool,
    pub ext_pipeline_creation_feedback: bool,
    pub khr_driver_properties: bool,
    pub khr_shader_float_controls: bool,
    pub nv_shader_subgroup_partitioned: bool,
    pub khr_depth_stencil_resolve: bool,
    pub khr_swapchain_mutable_format: bool,
    pub nv_compute_shader_derivatives: bool,
    pub nv_mesh_shader: bool,
    pub nv_fragment_shader_barycentric: bool,
    pub nv_shader_image_footprint: bool,
    pub nv_scissor_exclusive: bool,
    pub nv_device_diagnostic_checkpoints: bool,
    pub khr_timeline_semaphore: bool,
    pub ext_present_timing: bool,
    pub intel_shader_integer_functions2: bool,
    pub intel_performance_query: bool,
    pub khr_vulkan_memory_model: bool,
    pub ext_pci_bus_info: bool,
    pub amd_display_native_hdr: bool,
    pub khr_shader_terminate_invocation: bool,
    pub ext_fragment_density_map: bool,
    pub ext_scalar_block_layout: bool,
    pub google_hlsl_functionality1: bool,
    pub google_decorate_string: bool,
    pub ext_subgroup_size_control: bool,
    pub khr_fragment_shading_rate: bool,
    pub amd_shader_core_properties2: bool,
    pub amd_device_coherent_memory: bool,
    pub khr_dynamic_rendering_local_read: bool,
    pub ext_shader_image_atomic_int64: bool,
    pub khr_shader_quad_control: bool,
    pub khr_spirv_1_4: bool,
    pub ext_memory_budget: bool,
    pub ext_memory_priority: bool,
    pub nv_dedicated_allocation_image_aliasing: bool,
    pub khr_separate_depth_stencil_layouts: bool,
    pub ext_buffer_device_address: bool,
    pub ext_tooling_info: bool,
    pub ext_separate_stencil_usage: bool,
    pub khr_present_wait: bool,
    pub nv_cooperative_matrix: bool,
    pub nv_coverage_reduction_mode: bool,
    pub ext_fragment_shader_interlock: bool,
    pub ext_ycbcr_image_arrays: bool,
    pub khr_uniform_buffer_standard_layout: bool,
    pub ext_provoking_vertex: bool,
    pub ext_full_screen_exclusive: bool,
    pub khr_buffer_device_address: bool,
    pub ext_line_rasterization: bool,
    pub ext_shader_atomic_float: bool,
    pub ext_host_query_reset: bool,
    pub ext_index_type_uint8: bool,
    pub ext_extended_dynamic_state: bool,
    pub khr_deferred_host_operations: bool,
    pub khr_pipeline_executable_properties: bool,
    pub ext_host_image_copy: bool,
    pub khr_map_memory2: bool,
    pub ext_map_memory_placed: bool,
    pub ext_shader_atomic_float2: bool,
    pub ext_swapchain_maintenance1: bool,
    pub ext_shader_demote_to_helper_invocation: bool,
    pub nv_device_generated_commands: bool,
    pub nv_inherited_viewport_scissor: bool,
    pub khr_shader_integer_dot_product: bool,
    pub ext_texel_buffer_alignment: bool,
    pub qcom_render_pass_transform: bool,
    pub ext_depth_bias_control: bool,
    pub ext_device_memory_report: bool,
    pub ext_robustness2: bool,
    pub ext_custom_border_color: bool,
    pub ext_texture_compression_astc_3d: bool,
    pub google_user_type: bool,
    pub khr_pipeline_library: bool,
    pub nv_present_barrier: bool,
    pub khr_shader_non_semantic_info: bool,
    pub khr_present_id: bool,
    pub ext_private_data: bool,
    pub ext_pipeline_creation_cache_control: bool,
    pub nv_device_diagnostics_config: bool,
    pub qcom_render_pass_store_ops: bool,
    pub nv_cuda_kernel_launch: bool,
    pub qcom_tile_shading: bool,
    pub nv_low_latency: bool,
    pub ext_metal_objects: bool,
    pub khr_synchronization2: bool,
    pub ext_descriptor_buffer: bool,
    pub ext_graphics_pipeline_library: bool,
    pub amd_shader_early_and_late_fragment_tests: bool,
    pub khr_fragment_shader_barycentric: bool,
    pub khr_shader_subgroup_uniform_control_flow: bool,
    pub khr_zero_initialize_workgroup_memory: bool,
    pub nv_fragment_shading_rate_enums: bool,
    pub nv_ray_tracing_motion_blur: bool,
    pub ext_mesh_shader: bool,
    pub ext_ycbcr_2plane_444_formats: bool,
    pub ext_fragment_density_map2: bool,
    pub qcom_rotated_copy_commands: bool,
    pub ext_image_robustness: bool,
    pub khr_workgroup_memory_explicit_layout: bool,
    pub khr_copy_commands2: bool,
    pub ext_image_compression_control: bool,
    pub ext_attachment_feedback_loop_layout: bool,
    pub ext_4444_formats: bool,
    pub ext_device_fault: bool,
    pub arm_rasterization_order_attachment_access: bool,
    pub ext_rgba10x6_formats: bool,
    pub nv_acquire_winrt_display: bool,
    pub valve_mutable_descriptor_type: bool,
    pub ext_vertex_input_dynamic_state: bool,
    pub ext_physical_device_drm: bool,
    pub ext_device_address_binding_report: bool,
    pub ext_depth_clip_control: bool,
    pub ext_primitive_topology_list_restart: bool,
    pub khr_format_feature_flags2: bool,
    pub ext_present_mode_fifo_latest_ready: bool,
    pub fuchsia_external_memory: bool,
    pub fuchsia_external_semaphore: bool,
    pub fuchsia_buffer_collection: bool,
    pub huawei_subpass_shading: bool,
    pub huawei_invocation_mask: bool,
    pub nv_external_memory_rdma: bool,
    pub ext_pipeline_properties: bool,
    pub ext_frame_boundary: bool,
    pub ext_multisampled_render_to_single_sampled: bool,
    pub ext_extended_dynamic_state2: bool,
    pub ext_color_write_enable: bool,
    pub ext_primitives_generated_query: bool,
    pub khr_ray_tracing_maintenance1: bool,
    pub khr_shader_untyped_pointers: bool,
    pub ext_global_priority_query: bool,
    pub ext_image_view_min_lod: bool,
    pub ext_multi_draw: bool,
    pub ext_image_2d_view_of_3d: bool,
    pub ext_shader_tile_image: bool,
    pub ext_opacity_micromap: bool,
    pub nv_displacement_micromap: bool,
    pub ext_load_store_op_none: bool,
    pub huawei_cluster_culling_shader: bool,
    pub ext_border_color_swizzle: bool,
    pub ext_pageable_device_local_memory: bool,
    pub khr_maintenance4: bool,
    pub arm_shader_core_properties: bool,
    pub khr_shader_subgroup_rotate: bool,
    pub arm_scheduling_controls: bool,
    pub ext_image_sliced_view_of_3d: bool,
    pub valve_descriptor_set_host_mapping: bool,
    pub ext_depth_clamp_zero_one: bool,
    pub ext_non_seamless_cube_map: bool,
    pub arm_render_pass_striped: bool,
    pub qcom_fragment_density_map_offset: bool,
    pub nv_copy_memory_indirect: bool,
    pub nv_memory_decompression: bool,
    pub nv_device_generated_commands_compute: bool,
    pub nv_ray_tracing_linear_swept_spheres: bool,
    pub nv_linear_color_attachment: bool,
    pub khr_shader_maximal_reconvergence: bool,
    pub ext_image_compression_control_swapchain: bool,
    pub qcom_image_processing: bool,
    pub ext_nested_command_buffer: bool,
    pub ohos_external_memory: bool,
    pub ext_external_memory_acquire_unmodified: bool,
    pub ext_extended_dynamic_state3: bool,
    pub ext_subpass_merge_feedback: bool,
    pub arm_tensors: bool,
    pub ext_shader_module_identifier: bool,
    pub ext_rasterization_order_attachment_access: bool,
    pub nv_optical_flow: bool,
    pub ext_legacy_dithering: bool,
    pub ext_pipeline_protected_access: bool,
    pub android_external_format_resolve: bool,
    pub khr_maintenance5: bool,
    pub amd_anti_lag: bool,
    pub amdx_dense_geometry_format: bool,
    pub khr_present_id2: bool,
    pub khr_present_wait2: bool,
    pub khr_ray_tracing_position_fetch: bool,
    pub ext_shader_object: bool,
    pub khr_pipeline_binary: bool,
    pub qcom_tile_properties: bool,
    pub sec_amigo_profiling: bool,
    pub khr_swapchain_maintenance1: bool,
    pub qcom_multiview_per_view_viewports: bool,
    pub nv_ray_tracing_invocation_reorder: bool,
    pub nv_cooperative_vector: bool,
    pub nv_extended_sparse_address_space: bool,
    pub ext_mutable_descriptor_type: bool,
    pub ext_legacy_vertex_attributes: bool,
    pub arm_shader_core_builtins: bool,
    pub ext_pipeline_library_group_handles: bool,
    pub ext_dynamic_rendering_unused_attachments: bool,
    pub khr_internally_synchronized_queues: bool,
    pub nv_low_latency2: bool,
    pub khr_cooperative_matrix: bool,
    pub arm_data_graph: bool,
    pub qcom_multiview_per_view_render_areas: bool,
    pub khr_compute_shader_derivatives: bool,
    pub nv_per_stage_descriptor_set: bool,
    pub qcom_image_processing2: bool,
    pub qcom_filter_cubic_weights: bool,
    pub qcom_ycbcr_degamma: bool,
    pub qcom_filter_cubic_clamp: bool,
    pub ext_attachment_feedback_loop_dynamic_state: bool,
    pub khr_vertex_attribute_divisor: bool,
    pub khr_load_store_op_none: bool,
    pub khr_unified_image_layouts: bool,
    pub khr_shader_float_controls2: bool,
    pub msft_layered_driver: bool,
    pub khr_index_type_uint8: bool,
    pub khr_line_rasterization: bool,
    pub khr_calibrated_timestamps: bool,
    pub khr_shader_expect_assume: bool,
    pub khr_maintenance6: bool,
    pub nv_descriptor_pool_overallocation: bool,
    pub qcom_tile_memory_heap: bool,
    pub khr_copy_memory_indirect: bool,
    pub ext_memory_decompression: bool,
    pub nv_raw_access_chains: bool,
    pub nv_external_compute_queue: bool,
    pub khr_shader_relaxed_extended_instruction: bool,
    pub nv_command_buffer_inheritance: bool,
    pub khr_maintenance7: bool,
    pub nv_shader_atomic_float16_vector: bool,
    pub ext_shader_replicated_composites: bool,
    pub ext_shader_float8: bool,
    pub nv_ray_tracing_validation: bool,
    pub nv_cluster_acceleration_structure: bool,
    pub nv_partitioned_acceleration_structure: bool,
    pub ext_device_generated_commands: bool,
    pub khr_maintenance8: bool,
    pub mesa_image_alignment_control: bool,
    pub khr_shader_fma: bool,
    pub nv_push_constant_bank: bool,
    pub ext_ray_tracing_invocation_reorder: bool,
    pub ext_depth_clamp_control: bool,
    pub khr_maintenance9: bool,
    pub huawei_hdr_vivid: bool,
    pub nv_cooperative_matrix2: bool,
    pub arm_pipeline_opacity_micromap: bool,
    pub ext_external_memory_metal: bool,
    pub khr_depth_clamp_zero_one: bool,
    pub arm_performance_counters_by_region: bool,
    pub ext_vertex_attribute_robustness: bool,
    pub arm_format_pack: bool,
    pub valve_fragment_density_map_layered: bool,
    pub khr_robustness2: bool,
    pub nv_present_metering: bool,
    pub ext_fragment_density_map_offset: bool,
    pub ext_zero_initialize_device_memory: bool,
    pub khr_present_mode_fifo_latest_ready: bool,
    pub ext_shader_64bit_indexing: bool,
    pub ext_custom_resolve: bool,
    pub qcom_data_graph_model: bool,
    pub khr_maintenance10: bool,
    pub ext_shader_long_vector: bool,
    pub sec_pipeline_cache_incremental_mode: bool,
    pub ext_shader_uniform_buffer_unsized_array: bool,
    pub nv_compute_occupancy_priority: bool,
    pub ext_shader_subgroup_partitioned: bool,
    pub valve_shader_mixed_float_dot_product: bool,
}
impl DeviceExtensions {
    fn enable_by_name(&mut self, name: &CStr) {
        if name == c"VK_KHR_swapchain" {
            self.khr_swapchain = true;
        } else if name == c"VK_KHR_display_swapchain" {
            self.khr_display_swapchain = true;
        } else if name == c"VK_NV_glsl_shader" {
            self.nv_glsl_shader = true;
        } else if name == c"VK_EXT_depth_range_unrestricted" {
            self.ext_depth_range_unrestricted = true;
        } else if name == c"VK_KHR_sampler_mirror_clamp_to_edge" {
            self.khr_sampler_mirror_clamp_to_edge = true;
        } else if name == c"VK_IMG_filter_cubic" {
            self.img_filter_cubic = true;
        } else if name == c"VK_AMD_rasterization_order" {
            self.amd_rasterization_order = true;
        } else if name == c"VK_AMD_shader_trinary_minmax" {
            self.amd_shader_trinary_minmax = true;
        } else if name == c"VK_AMD_shader_explicit_vertex_parameter" {
            self.amd_shader_explicit_vertex_parameter = true;
        } else if name == c"VK_EXT_debug_marker" {
            self.ext_debug_marker = true;
        } else if name == c"VK_AMD_gcn_shader" {
            self.amd_gcn_shader = true;
        } else if name == c"VK_NV_dedicated_allocation" {
            self.nv_dedicated_allocation = true;
        } else if name == c"VK_EXT_transform_feedback" {
            self.ext_transform_feedback = true;
        } else if name == c"VK_NVX_binary_import" {
            self.nvx_binary_import = true;
        } else if name == c"VK_NVX_image_view_handle" {
            self.nvx_image_view_handle = true;
        } else if name == c"VK_AMD_draw_indirect_count" {
            self.amd_draw_indirect_count = true;
        } else if name == c"VK_AMD_negative_viewport_height" {
            self.amd_negative_viewport_height = true;
        } else if name == c"VK_AMD_gpu_shader_half_float" {
            self.amd_gpu_shader_half_float = true;
        } else if name == c"VK_AMD_shader_ballot" {
            self.amd_shader_ballot = true;
        } else if name == c"VK_AMD_texture_gather_bias_lod" {
            self.amd_texture_gather_bias_lod = true;
        } else if name == c"VK_AMD_shader_info" {
            self.amd_shader_info = true;
        } else if name == c"VK_KHR_dynamic_rendering" {
            self.khr_dynamic_rendering = true;
        } else if name == c"VK_AMD_shader_image_load_store_lod" {
            self.amd_shader_image_load_store_lod = true;
        } else if name == c"VK_NV_corner_sampled_image" {
            self.nv_corner_sampled_image = true;
        } else if name == c"VK_KHR_multiview" {
            self.khr_multiview = true;
        } else if name == c"VK_IMG_format_pvrtc" {
            self.img_format_pvrtc = true;
        } else if name == c"VK_NV_external_memory" {
            self.nv_external_memory = true;
        } else if name == c"VK_NV_external_memory_win32" {
            self.nv_external_memory_win32 = true;
        } else if name == c"VK_NV_win32_keyed_mutex" {
            self.nv_win32_keyed_mutex = true;
        } else if name == c"VK_KHR_device_group" {
            self.khr_device_group = true;
        } else if name == c"VK_KHR_shader_draw_parameters" {
            self.khr_shader_draw_parameters = true;
        } else if name == c"VK_EXT_shader_subgroup_ballot" {
            self.ext_shader_subgroup_ballot = true;
        } else if name == c"VK_EXT_shader_subgroup_vote" {
            self.ext_shader_subgroup_vote = true;
        } else if name == c"VK_EXT_texture_compression_astc_hdr" {
            self.ext_texture_compression_astc_hdr = true;
        } else if name == c"VK_EXT_astc_decode_mode" {
            self.ext_astc_decode_mode = true;
        } else if name == c"VK_EXT_pipeline_robustness" {
            self.ext_pipeline_robustness = true;
        } else if name == c"VK_KHR_maintenance1" {
            self.khr_maintenance1 = true;
        } else if name == c"VK_KHR_external_memory" {
            self.khr_external_memory = true;
        } else if name == c"VK_KHR_external_memory_win32" {
            self.khr_external_memory_win32 = true;
        } else if name == c"VK_KHR_external_memory_fd" {
            self.khr_external_memory_fd = true;
        } else if name == c"VK_KHR_win32_keyed_mutex" {
            self.khr_win32_keyed_mutex = true;
        } else if name == c"VK_KHR_external_semaphore" {
            self.khr_external_semaphore = true;
        } else if name == c"VK_KHR_external_semaphore_win32" {
            self.khr_external_semaphore_win32 = true;
        } else if name == c"VK_KHR_external_semaphore_fd" {
            self.khr_external_semaphore_fd = true;
        } else if name == c"VK_KHR_push_descriptor" {
            self.khr_push_descriptor = true;
        } else if name == c"VK_EXT_conditional_rendering" {
            self.ext_conditional_rendering = true;
        } else if name == c"VK_KHR_shader_float16_int8" {
            self.khr_shader_float16_int8 = true;
        } else if name == c"VK_KHR_16bit_storage" {
            self.khr_16bit_storage = true;
        } else if name == c"VK_KHR_incremental_present" {
            self.khr_incremental_present = true;
        } else if name == c"VK_KHR_descriptor_update_template" {
            self.khr_descriptor_update_template = true;
        } else if name == c"VK_NV_clip_space_w_scaling" {
            self.nv_clip_space_w_scaling = true;
        } else if name == c"VK_EXT_display_control" {
            self.ext_display_control = true;
        } else if name == c"VK_GOOGLE_display_timing" {
            self.google_display_timing = true;
        } else if name == c"VK_NV_sample_mask_override_coverage" {
            self.nv_sample_mask_override_coverage = true;
        } else if name == c"VK_NV_geometry_shader_passthrough" {
            self.nv_geometry_shader_passthrough = true;
        } else if name == c"VK_NV_viewport_array2" {
            self.nv_viewport_array2 = true;
        } else if name == c"VK_NVX_multiview_per_view_attributes" {
            self.nvx_multiview_per_view_attributes = true;
        } else if name == c"VK_NV_viewport_swizzle" {
            self.nv_viewport_swizzle = true;
        } else if name == c"VK_EXT_discard_rectangles" {
            self.ext_discard_rectangles = true;
        } else if name == c"VK_EXT_conservative_rasterization" {
            self.ext_conservative_rasterization = true;
        } else if name == c"VK_EXT_depth_clip_enable" {
            self.ext_depth_clip_enable = true;
        } else if name == c"VK_EXT_hdr_metadata" {
            self.ext_hdr_metadata = true;
        } else if name == c"VK_KHR_imageless_framebuffer" {
            self.khr_imageless_framebuffer = true;
        } else if name == c"VK_KHR_create_renderpass2" {
            self.khr_create_renderpass2 = true;
        } else if name == c"VK_IMG_relaxed_line_rasterization" {
            self.img_relaxed_line_rasterization = true;
        } else if name == c"VK_KHR_shared_presentable_image" {
            self.khr_shared_presentable_image = true;
        } else if name == c"VK_KHR_external_fence" {
            self.khr_external_fence = true;
        } else if name == c"VK_KHR_external_fence_win32" {
            self.khr_external_fence_win32 = true;
        } else if name == c"VK_KHR_external_fence_fd" {
            self.khr_external_fence_fd = true;
        } else if name == c"VK_KHR_performance_query" {
            self.khr_performance_query = true;
        } else if name == c"VK_KHR_maintenance2" {
            self.khr_maintenance2 = true;
        } else if name == c"VK_KHR_variable_pointers" {
            self.khr_variable_pointers = true;
        } else if name == c"VK_EXT_external_memory_dma_buf" {
            self.ext_external_memory_dma_buf = true;
        } else if name == c"VK_EXT_queue_family_foreign" {
            self.ext_queue_family_foreign = true;
        } else if name == c"VK_KHR_dedicated_allocation" {
            self.khr_dedicated_allocation = true;
        } else if name == c"VK_ANDROID_external_memory_android_hardware_buffer" {
            self.android_external_memory_android_hardware_buffer = true;
        } else if name == c"VK_EXT_sampler_filter_minmax" {
            self.ext_sampler_filter_minmax = true;
        } else if name == c"VK_KHR_storage_buffer_storage_class" {
            self.khr_storage_buffer_storage_class = true;
        } else if name == c"VK_AMD_gpu_shader_int16" {
            self.amd_gpu_shader_int16 = true;
        } else if name == c"VK_AMDX_shader_enqueue" {
            self.amdx_shader_enqueue = true;
        } else if name == c"VK_EXT_descriptor_heap" {
            self.ext_descriptor_heap = true;
        } else if name == c"VK_AMD_mixed_attachment_samples" {
            self.amd_mixed_attachment_samples = true;
        } else if name == c"VK_AMD_shader_fragment_mask" {
            self.amd_shader_fragment_mask = true;
        } else if name == c"VK_EXT_inline_uniform_block" {
            self.ext_inline_uniform_block = true;
        } else if name == c"VK_EXT_shader_stencil_export" {
            self.ext_shader_stencil_export = true;
        } else if name == c"VK_KHR_shader_bfloat16" {
            self.khr_shader_bfloat16 = true;
        } else if name == c"VK_EXT_sample_locations" {
            self.ext_sample_locations = true;
        } else if name == c"VK_KHR_relaxed_block_layout" {
            self.khr_relaxed_block_layout = true;
        } else if name == c"VK_KHR_get_memory_requirements2" {
            self.khr_get_memory_requirements2 = true;
        } else if name == c"VK_KHR_image_format_list" {
            self.khr_image_format_list = true;
        } else if name == c"VK_EXT_blend_operation_advanced" {
            self.ext_blend_operation_advanced = true;
        } else if name == c"VK_NV_fragment_coverage_to_color" {
            self.nv_fragment_coverage_to_color = true;
        } else if name == c"VK_KHR_acceleration_structure" {
            self.khr_acceleration_structure = true;
        } else if name == c"VK_KHR_ray_tracing_pipeline" {
            self.khr_ray_tracing_pipeline = true;
        } else if name == c"VK_KHR_ray_query" {
            self.khr_ray_query = true;
        } else if name == c"VK_NV_framebuffer_mixed_samples" {
            self.nv_framebuffer_mixed_samples = true;
        } else if name == c"VK_NV_fill_rectangle" {
            self.nv_fill_rectangle = true;
        } else if name == c"VK_NV_shader_sm_builtins" {
            self.nv_shader_sm_builtins = true;
        } else if name == c"VK_EXT_post_depth_coverage" {
            self.ext_post_depth_coverage = true;
        } else if name == c"VK_KHR_sampler_ycbcr_conversion" {
            self.khr_sampler_ycbcr_conversion = true;
        } else if name == c"VK_KHR_bind_memory2" {
            self.khr_bind_memory2 = true;
        } else if name == c"VK_EXT_image_drm_format_modifier" {
            self.ext_image_drm_format_modifier = true;
        } else if name == c"VK_EXT_validation_cache" {
            self.ext_validation_cache = true;
        } else if name == c"VK_EXT_descriptor_indexing" {
            self.ext_descriptor_indexing = true;
        } else if name == c"VK_EXT_shader_viewport_index_layer" {
            self.ext_shader_viewport_index_layer = true;
        } else if name == c"VK_KHR_portability_subset" {
            self.khr_portability_subset = true;
        } else if name == c"VK_NV_shading_rate_image" {
            self.nv_shading_rate_image = true;
        } else if name == c"VK_NV_ray_tracing" {
            self.nv_ray_tracing = true;
        } else if name == c"VK_NV_representative_fragment_test" {
            self.nv_representative_fragment_test = true;
        } else if name == c"VK_KHR_maintenance3" {
            self.khr_maintenance3 = true;
        } else if name == c"VK_KHR_draw_indirect_count" {
            self.khr_draw_indirect_count = true;
        } else if name == c"VK_EXT_filter_cubic" {
            self.ext_filter_cubic = true;
        } else if name == c"VK_QCOM_render_pass_shader_resolve" {
            self.qcom_render_pass_shader_resolve = true;
        } else if name == c"VK_QCOM_cooperative_matrix_conversion" {
            self.qcom_cooperative_matrix_conversion = true;
        } else if name == c"VK_EXT_global_priority" {
            self.ext_global_priority = true;
        } else if name == c"VK_KHR_shader_subgroup_extended_types" {
            self.khr_shader_subgroup_extended_types = true;
        } else if name == c"VK_KHR_8bit_storage" {
            self.khr_8bit_storage = true;
        } else if name == c"VK_EXT_external_memory_host" {
            self.ext_external_memory_host = true;
        } else if name == c"VK_AMD_buffer_marker" {
            self.amd_buffer_marker = true;
        } else if name == c"VK_KHR_shader_atomic_int64" {
            self.khr_shader_atomic_int64 = true;
        } else if name == c"VK_KHR_shader_clock" {
            self.khr_shader_clock = true;
        } else if name == c"VK_AMD_pipeline_compiler_control" {
            self.amd_pipeline_compiler_control = true;
        } else if name == c"VK_EXT_calibrated_timestamps" {
            self.ext_calibrated_timestamps = true;
        } else if name == c"VK_AMD_shader_core_properties" {
            self.amd_shader_core_properties = true;
        } else if name == c"VK_KHR_global_priority" {
            self.khr_global_priority = true;
        } else if name == c"VK_AMD_memory_overallocation_behavior" {
            self.amd_memory_overallocation_behavior = true;
        } else if name == c"VK_EXT_vertex_attribute_divisor" {
            self.ext_vertex_attribute_divisor = true;
        } else if name == c"VK_EXT_pipeline_creation_feedback" {
            self.ext_pipeline_creation_feedback = true;
        } else if name == c"VK_KHR_driver_properties" {
            self.khr_driver_properties = true;
        } else if name == c"VK_KHR_shader_float_controls" {
            self.khr_shader_float_controls = true;
        } else if name == c"VK_NV_shader_subgroup_partitioned" {
            self.nv_shader_subgroup_partitioned = true;
        } else if name == c"VK_KHR_depth_stencil_resolve" {
            self.khr_depth_stencil_resolve = true;
        } else if name == c"VK_KHR_swapchain_mutable_format" {
            self.khr_swapchain_mutable_format = true;
        } else if name == c"VK_NV_compute_shader_derivatives" {
            self.nv_compute_shader_derivatives = true;
        } else if name == c"VK_NV_mesh_shader" {
            self.nv_mesh_shader = true;
        } else if name == c"VK_NV_fragment_shader_barycentric" {
            self.nv_fragment_shader_barycentric = true;
        } else if name == c"VK_NV_shader_image_footprint" {
            self.nv_shader_image_footprint = true;
        } else if name == c"VK_NV_scissor_exclusive" {
            self.nv_scissor_exclusive = true;
        } else if name == c"VK_NV_device_diagnostic_checkpoints" {
            self.nv_device_diagnostic_checkpoints = true;
        } else if name == c"VK_KHR_timeline_semaphore" {
            self.khr_timeline_semaphore = true;
        } else if name == c"VK_EXT_present_timing" {
            self.ext_present_timing = true;
        } else if name == c"VK_INTEL_shader_integer_functions2" {
            self.intel_shader_integer_functions2 = true;
        } else if name == c"VK_INTEL_performance_query" {
            self.intel_performance_query = true;
        } else if name == c"VK_KHR_vulkan_memory_model" {
            self.khr_vulkan_memory_model = true;
        } else if name == c"VK_EXT_pci_bus_info" {
            self.ext_pci_bus_info = true;
        } else if name == c"VK_AMD_display_native_hdr" {
            self.amd_display_native_hdr = true;
        } else if name == c"VK_KHR_shader_terminate_invocation" {
            self.khr_shader_terminate_invocation = true;
        } else if name == c"VK_EXT_fragment_density_map" {
            self.ext_fragment_density_map = true;
        } else if name == c"VK_EXT_scalar_block_layout" {
            self.ext_scalar_block_layout = true;
        } else if name == c"VK_GOOGLE_hlsl_functionality1" {
            self.google_hlsl_functionality1 = true;
        } else if name == c"VK_GOOGLE_decorate_string" {
            self.google_decorate_string = true;
        } else if name == c"VK_EXT_subgroup_size_control" {
            self.ext_subgroup_size_control = true;
        } else if name == c"VK_KHR_fragment_shading_rate" {
            self.khr_fragment_shading_rate = true;
        } else if name == c"VK_AMD_shader_core_properties2" {
            self.amd_shader_core_properties2 = true;
        } else if name == c"VK_AMD_device_coherent_memory" {
            self.amd_device_coherent_memory = true;
        } else if name == c"VK_KHR_dynamic_rendering_local_read" {
            self.khr_dynamic_rendering_local_read = true;
        } else if name == c"VK_EXT_shader_image_atomic_int64" {
            self.ext_shader_image_atomic_int64 = true;
        } else if name == c"VK_KHR_shader_quad_control" {
            self.khr_shader_quad_control = true;
        } else if name == c"VK_KHR_spirv_1_4" {
            self.khr_spirv_1_4 = true;
        } else if name == c"VK_EXT_memory_budget" {
            self.ext_memory_budget = true;
        } else if name == c"VK_EXT_memory_priority" {
            self.ext_memory_priority = true;
        } else if name == c"VK_NV_dedicated_allocation_image_aliasing" {
            self.nv_dedicated_allocation_image_aliasing = true;
        } else if name == c"VK_KHR_separate_depth_stencil_layouts" {
            self.khr_separate_depth_stencil_layouts = true;
        } else if name == c"VK_EXT_buffer_device_address" {
            self.ext_buffer_device_address = true;
        } else if name == c"VK_EXT_tooling_info" {
            self.ext_tooling_info = true;
        } else if name == c"VK_EXT_separate_stencil_usage" {
            self.ext_separate_stencil_usage = true;
        } else if name == c"VK_KHR_present_wait" {
            self.khr_present_wait = true;
        } else if name == c"VK_NV_cooperative_matrix" {
            self.nv_cooperative_matrix = true;
        } else if name == c"VK_NV_coverage_reduction_mode" {
            self.nv_coverage_reduction_mode = true;
        } else if name == c"VK_EXT_fragment_shader_interlock" {
            self.ext_fragment_shader_interlock = true;
        } else if name == c"VK_EXT_ycbcr_image_arrays" {
            self.ext_ycbcr_image_arrays = true;
        } else if name == c"VK_KHR_uniform_buffer_standard_layout" {
            self.khr_uniform_buffer_standard_layout = true;
        } else if name == c"VK_EXT_provoking_vertex" {
            self.ext_provoking_vertex = true;
        } else if name == c"VK_EXT_full_screen_exclusive" {
            self.ext_full_screen_exclusive = true;
        } else if name == c"VK_KHR_buffer_device_address" {
            self.khr_buffer_device_address = true;
        } else if name == c"VK_EXT_line_rasterization" {
            self.ext_line_rasterization = true;
        } else if name == c"VK_EXT_shader_atomic_float" {
            self.ext_shader_atomic_float = true;
        } else if name == c"VK_EXT_host_query_reset" {
            self.ext_host_query_reset = true;
        } else if name == c"VK_EXT_index_type_uint8" {
            self.ext_index_type_uint8 = true;
        } else if name == c"VK_EXT_extended_dynamic_state" {
            self.ext_extended_dynamic_state = true;
        } else if name == c"VK_KHR_deferred_host_operations" {
            self.khr_deferred_host_operations = true;
        } else if name == c"VK_KHR_pipeline_executable_properties" {
            self.khr_pipeline_executable_properties = true;
        } else if name == c"VK_EXT_host_image_copy" {
            self.ext_host_image_copy = true;
        } else if name == c"VK_KHR_map_memory2" {
            self.khr_map_memory2 = true;
        } else if name == c"VK_EXT_map_memory_placed" {
            self.ext_map_memory_placed = true;
        } else if name == c"VK_EXT_shader_atomic_float2" {
            self.ext_shader_atomic_float2 = true;
        } else if name == c"VK_EXT_swapchain_maintenance1" {
            self.ext_swapchain_maintenance1 = true;
        } else if name == c"VK_EXT_shader_demote_to_helper_invocation" {
            self.ext_shader_demote_to_helper_invocation = true;
        } else if name == c"VK_NV_device_generated_commands" {
            self.nv_device_generated_commands = true;
        } else if name == c"VK_NV_inherited_viewport_scissor" {
            self.nv_inherited_viewport_scissor = true;
        } else if name == c"VK_KHR_shader_integer_dot_product" {
            self.khr_shader_integer_dot_product = true;
        } else if name == c"VK_EXT_texel_buffer_alignment" {
            self.ext_texel_buffer_alignment = true;
        } else if name == c"VK_QCOM_render_pass_transform" {
            self.qcom_render_pass_transform = true;
        } else if name == c"VK_EXT_depth_bias_control" {
            self.ext_depth_bias_control = true;
        } else if name == c"VK_EXT_device_memory_report" {
            self.ext_device_memory_report = true;
        } else if name == c"VK_EXT_robustness2" {
            self.ext_robustness2 = true;
        } else if name == c"VK_EXT_custom_border_color" {
            self.ext_custom_border_color = true;
        } else if name == c"VK_EXT_texture_compression_astc_3d" {
            self.ext_texture_compression_astc_3d = true;
        } else if name == c"VK_GOOGLE_user_type" {
            self.google_user_type = true;
        } else if name == c"VK_KHR_pipeline_library" {
            self.khr_pipeline_library = true;
        } else if name == c"VK_NV_present_barrier" {
            self.nv_present_barrier = true;
        } else if name == c"VK_KHR_shader_non_semantic_info" {
            self.khr_shader_non_semantic_info = true;
        } else if name == c"VK_KHR_present_id" {
            self.khr_present_id = true;
        } else if name == c"VK_EXT_private_data" {
            self.ext_private_data = true;
        } else if name == c"VK_EXT_pipeline_creation_cache_control" {
            self.ext_pipeline_creation_cache_control = true;
        } else if name == c"VK_NV_device_diagnostics_config" {
            self.nv_device_diagnostics_config = true;
        } else if name == c"VK_QCOM_render_pass_store_ops" {
            self.qcom_render_pass_store_ops = true;
        } else if name == c"VK_NV_cuda_kernel_launch" {
            self.nv_cuda_kernel_launch = true;
        } else if name == c"VK_QCOM_tile_shading" {
            self.qcom_tile_shading = true;
        } else if name == c"VK_NV_low_latency" {
            self.nv_low_latency = true;
        } else if name == c"VK_EXT_metal_objects" {
            self.ext_metal_objects = true;
        } else if name == c"VK_KHR_synchronization2" {
            self.khr_synchronization2 = true;
        } else if name == c"VK_EXT_descriptor_buffer" {
            self.ext_descriptor_buffer = true;
        } else if name == c"VK_EXT_graphics_pipeline_library" {
            self.ext_graphics_pipeline_library = true;
        } else if name == c"VK_AMD_shader_early_and_late_fragment_tests" {
            self.amd_shader_early_and_late_fragment_tests = true;
        } else if name == c"VK_KHR_fragment_shader_barycentric" {
            self.khr_fragment_shader_barycentric = true;
        } else if name == c"VK_KHR_shader_subgroup_uniform_control_flow" {
            self.khr_shader_subgroup_uniform_control_flow = true;
        } else if name == c"VK_KHR_zero_initialize_workgroup_memory" {
            self.khr_zero_initialize_workgroup_memory = true;
        } else if name == c"VK_NV_fragment_shading_rate_enums" {
            self.nv_fragment_shading_rate_enums = true;
        } else if name == c"VK_NV_ray_tracing_motion_blur" {
            self.nv_ray_tracing_motion_blur = true;
        } else if name == c"VK_EXT_mesh_shader" {
            self.ext_mesh_shader = true;
        } else if name == c"VK_EXT_ycbcr_2plane_444_formats" {
            self.ext_ycbcr_2plane_444_formats = true;
        } else if name == c"VK_EXT_fragment_density_map2" {
            self.ext_fragment_density_map2 = true;
        } else if name == c"VK_QCOM_rotated_copy_commands" {
            self.qcom_rotated_copy_commands = true;
        } else if name == c"VK_EXT_image_robustness" {
            self.ext_image_robustness = true;
        } else if name == c"VK_KHR_workgroup_memory_explicit_layout" {
            self.khr_workgroup_memory_explicit_layout = true;
        } else if name == c"VK_KHR_copy_commands2" {
            self.khr_copy_commands2 = true;
        } else if name == c"VK_EXT_image_compression_control" {
            self.ext_image_compression_control = true;
        } else if name == c"VK_EXT_attachment_feedback_loop_layout" {
            self.ext_attachment_feedback_loop_layout = true;
        } else if name == c"VK_EXT_4444_formats" {
            self.ext_4444_formats = true;
        } else if name == c"VK_EXT_device_fault" {
            self.ext_device_fault = true;
        } else if name == c"VK_ARM_rasterization_order_attachment_access" {
            self.arm_rasterization_order_attachment_access = true;
        } else if name == c"VK_EXT_rgba10x6_formats" {
            self.ext_rgba10x6_formats = true;
        } else if name == c"VK_NV_acquire_winrt_display" {
            self.nv_acquire_winrt_display = true;
        } else if name == c"VK_VALVE_mutable_descriptor_type" {
            self.valve_mutable_descriptor_type = true;
        } else if name == c"VK_EXT_vertex_input_dynamic_state" {
            self.ext_vertex_input_dynamic_state = true;
        } else if name == c"VK_EXT_physical_device_drm" {
            self.ext_physical_device_drm = true;
        } else if name == c"VK_EXT_device_address_binding_report" {
            self.ext_device_address_binding_report = true;
        } else if name == c"VK_EXT_depth_clip_control" {
            self.ext_depth_clip_control = true;
        } else if name == c"VK_EXT_primitive_topology_list_restart" {
            self.ext_primitive_topology_list_restart = true;
        } else if name == c"VK_KHR_format_feature_flags2" {
            self.khr_format_feature_flags2 = true;
        } else if name == c"VK_EXT_present_mode_fifo_latest_ready" {
            self.ext_present_mode_fifo_latest_ready = true;
        } else if name == c"VK_FUCHSIA_external_memory" {
            self.fuchsia_external_memory = true;
        } else if name == c"VK_FUCHSIA_external_semaphore" {
            self.fuchsia_external_semaphore = true;
        } else if name == c"VK_FUCHSIA_buffer_collection" {
            self.fuchsia_buffer_collection = true;
        } else if name == c"VK_HUAWEI_subpass_shading" {
            self.huawei_subpass_shading = true;
        } else if name == c"VK_HUAWEI_invocation_mask" {
            self.huawei_invocation_mask = true;
        } else if name == c"VK_NV_external_memory_rdma" {
            self.nv_external_memory_rdma = true;
        } else if name == c"VK_EXT_pipeline_properties" {
            self.ext_pipeline_properties = true;
        } else if name == c"VK_EXT_frame_boundary" {
            self.ext_frame_boundary = true;
        } else if name == c"VK_EXT_multisampled_render_to_single_sampled" {
            self.ext_multisampled_render_to_single_sampled = true;
        } else if name == c"VK_EXT_extended_dynamic_state2" {
            self.ext_extended_dynamic_state2 = true;
        } else if name == c"VK_EXT_color_write_enable" {
            self.ext_color_write_enable = true;
        } else if name == c"VK_EXT_primitives_generated_query" {
            self.ext_primitives_generated_query = true;
        } else if name == c"VK_KHR_ray_tracing_maintenance1" {
            self.khr_ray_tracing_maintenance1 = true;
        } else if name == c"VK_KHR_shader_untyped_pointers" {
            self.khr_shader_untyped_pointers = true;
        } else if name == c"VK_EXT_global_priority_query" {
            self.ext_global_priority_query = true;
        } else if name == c"VK_EXT_image_view_min_lod" {
            self.ext_image_view_min_lod = true;
        } else if name == c"VK_EXT_multi_draw" {
            self.ext_multi_draw = true;
        } else if name == c"VK_EXT_image_2d_view_of_3d" {
            self.ext_image_2d_view_of_3d = true;
        } else if name == c"VK_EXT_shader_tile_image" {
            self.ext_shader_tile_image = true;
        } else if name == c"VK_EXT_opacity_micromap" {
            self.ext_opacity_micromap = true;
        } else if name == c"VK_NV_displacement_micromap" {
            self.nv_displacement_micromap = true;
        } else if name == c"VK_EXT_load_store_op_none" {
            self.ext_load_store_op_none = true;
        } else if name == c"VK_HUAWEI_cluster_culling_shader" {
            self.huawei_cluster_culling_shader = true;
        } else if name == c"VK_EXT_border_color_swizzle" {
            self.ext_border_color_swizzle = true;
        } else if name == c"VK_EXT_pageable_device_local_memory" {
            self.ext_pageable_device_local_memory = true;
        } else if name == c"VK_KHR_maintenance4" {
            self.khr_maintenance4 = true;
        } else if name == c"VK_ARM_shader_core_properties" {
            self.arm_shader_core_properties = true;
        } else if name == c"VK_KHR_shader_subgroup_rotate" {
            self.khr_shader_subgroup_rotate = true;
        } else if name == c"VK_ARM_scheduling_controls" {
            self.arm_scheduling_controls = true;
        } else if name == c"VK_EXT_image_sliced_view_of_3d" {
            self.ext_image_sliced_view_of_3d = true;
        } else if name == c"VK_VALVE_descriptor_set_host_mapping" {
            self.valve_descriptor_set_host_mapping = true;
        } else if name == c"VK_EXT_depth_clamp_zero_one" {
            self.ext_depth_clamp_zero_one = true;
        } else if name == c"VK_EXT_non_seamless_cube_map" {
            self.ext_non_seamless_cube_map = true;
        } else if name == c"VK_ARM_render_pass_striped" {
            self.arm_render_pass_striped = true;
        } else if name == c"VK_QCOM_fragment_density_map_offset" {
            self.qcom_fragment_density_map_offset = true;
        } else if name == c"VK_NV_copy_memory_indirect" {
            self.nv_copy_memory_indirect = true;
        } else if name == c"VK_NV_memory_decompression" {
            self.nv_memory_decompression = true;
        } else if name == c"VK_NV_device_generated_commands_compute" {
            self.nv_device_generated_commands_compute = true;
        } else if name == c"VK_NV_ray_tracing_linear_swept_spheres" {
            self.nv_ray_tracing_linear_swept_spheres = true;
        } else if name == c"VK_NV_linear_color_attachment" {
            self.nv_linear_color_attachment = true;
        } else if name == c"VK_KHR_shader_maximal_reconvergence" {
            self.khr_shader_maximal_reconvergence = true;
        } else if name == c"VK_EXT_image_compression_control_swapchain" {
            self.ext_image_compression_control_swapchain = true;
        } else if name == c"VK_QCOM_image_processing" {
            self.qcom_image_processing = true;
        } else if name == c"VK_EXT_nested_command_buffer" {
            self.ext_nested_command_buffer = true;
        } else if name == c"VK_OHOS_external_memory" {
            self.ohos_external_memory = true;
        } else if name == c"VK_EXT_external_memory_acquire_unmodified" {
            self.ext_external_memory_acquire_unmodified = true;
        } else if name == c"VK_EXT_extended_dynamic_state3" {
            self.ext_extended_dynamic_state3 = true;
        } else if name == c"VK_EXT_subpass_merge_feedback" {
            self.ext_subpass_merge_feedback = true;
        } else if name == c"VK_ARM_tensors" {
            self.arm_tensors = true;
        } else if name == c"VK_EXT_shader_module_identifier" {
            self.ext_shader_module_identifier = true;
        } else if name == c"VK_EXT_rasterization_order_attachment_access" {
            self.ext_rasterization_order_attachment_access = true;
        } else if name == c"VK_NV_optical_flow" {
            self.nv_optical_flow = true;
        } else if name == c"VK_EXT_legacy_dithering" {
            self.ext_legacy_dithering = true;
        } else if name == c"VK_EXT_pipeline_protected_access" {
            self.ext_pipeline_protected_access = true;
        } else if name == c"VK_ANDROID_external_format_resolve" {
            self.android_external_format_resolve = true;
        } else if name == c"VK_KHR_maintenance5" {
            self.khr_maintenance5 = true;
        } else if name == c"VK_AMD_anti_lag" {
            self.amd_anti_lag = true;
        } else if name == c"VK_AMDX_dense_geometry_format" {
            self.amdx_dense_geometry_format = true;
        } else if name == c"VK_KHR_present_id2" {
            self.khr_present_id2 = true;
        } else if name == c"VK_KHR_present_wait2" {
            self.khr_present_wait2 = true;
        } else if name == c"VK_KHR_ray_tracing_position_fetch" {
            self.khr_ray_tracing_position_fetch = true;
        } else if name == c"VK_EXT_shader_object" {
            self.ext_shader_object = true;
        } else if name == c"VK_KHR_pipeline_binary" {
            self.khr_pipeline_binary = true;
        } else if name == c"VK_QCOM_tile_properties" {
            self.qcom_tile_properties = true;
        } else if name == c"VK_SEC_amigo_profiling" {
            self.sec_amigo_profiling = true;
        } else if name == c"VK_KHR_swapchain_maintenance1" {
            self.khr_swapchain_maintenance1 = true;
        } else if name == c"VK_QCOM_multiview_per_view_viewports" {
            self.qcom_multiview_per_view_viewports = true;
        } else if name == c"VK_NV_ray_tracing_invocation_reorder" {
            self.nv_ray_tracing_invocation_reorder = true;
        } else if name == c"VK_NV_cooperative_vector" {
            self.nv_cooperative_vector = true;
        } else if name == c"VK_NV_extended_sparse_address_space" {
            self.nv_extended_sparse_address_space = true;
        } else if name == c"VK_EXT_mutable_descriptor_type" {
            self.ext_mutable_descriptor_type = true;
        } else if name == c"VK_EXT_legacy_vertex_attributes" {
            self.ext_legacy_vertex_attributes = true;
        } else if name == c"VK_ARM_shader_core_builtins" {
            self.arm_shader_core_builtins = true;
        } else if name == c"VK_EXT_pipeline_library_group_handles" {
            self.ext_pipeline_library_group_handles = true;
        } else if name == c"VK_EXT_dynamic_rendering_unused_attachments" {
            self.ext_dynamic_rendering_unused_attachments = true;
        } else if name == c"VK_KHR_internally_synchronized_queues" {
            self.khr_internally_synchronized_queues = true;
        } else if name == c"VK_NV_low_latency2" {
            self.nv_low_latency2 = true;
        } else if name == c"VK_KHR_cooperative_matrix" {
            self.khr_cooperative_matrix = true;
        } else if name == c"VK_ARM_data_graph" {
            self.arm_data_graph = true;
        } else if name == c"VK_QCOM_multiview_per_view_render_areas" {
            self.qcom_multiview_per_view_render_areas = true;
        } else if name == c"VK_KHR_compute_shader_derivatives" {
            self.khr_compute_shader_derivatives = true;
        } else if name == c"VK_NV_per_stage_descriptor_set" {
            self.nv_per_stage_descriptor_set = true;
        } else if name == c"VK_QCOM_image_processing2" {
            self.qcom_image_processing2 = true;
        } else if name == c"VK_QCOM_filter_cubic_weights" {
            self.qcom_filter_cubic_weights = true;
        } else if name == c"VK_QCOM_ycbcr_degamma" {
            self.qcom_ycbcr_degamma = true;
        } else if name == c"VK_QCOM_filter_cubic_clamp" {
            self.qcom_filter_cubic_clamp = true;
        } else if name == c"VK_EXT_attachment_feedback_loop_dynamic_state" {
            self.ext_attachment_feedback_loop_dynamic_state = true;
        } else if name == c"VK_KHR_vertex_attribute_divisor" {
            self.khr_vertex_attribute_divisor = true;
        } else if name == c"VK_KHR_load_store_op_none" {
            self.khr_load_store_op_none = true;
        } else if name == c"VK_KHR_unified_image_layouts" {
            self.khr_unified_image_layouts = true;
        } else if name == c"VK_KHR_shader_float_controls2" {
            self.khr_shader_float_controls2 = true;
        } else if name == c"VK_MSFT_layered_driver" {
            self.msft_layered_driver = true;
        } else if name == c"VK_KHR_index_type_uint8" {
            self.khr_index_type_uint8 = true;
        } else if name == c"VK_KHR_line_rasterization" {
            self.khr_line_rasterization = true;
        } else if name == c"VK_KHR_calibrated_timestamps" {
            self.khr_calibrated_timestamps = true;
        } else if name == c"VK_KHR_shader_expect_assume" {
            self.khr_shader_expect_assume = true;
        } else if name == c"VK_KHR_maintenance6" {
            self.khr_maintenance6 = true;
        } else if name == c"VK_NV_descriptor_pool_overallocation" {
            self.nv_descriptor_pool_overallocation = true;
        } else if name == c"VK_QCOM_tile_memory_heap" {
            self.qcom_tile_memory_heap = true;
        } else if name == c"VK_KHR_copy_memory_indirect" {
            self.khr_copy_memory_indirect = true;
        } else if name == c"VK_EXT_memory_decompression" {
            self.ext_memory_decompression = true;
        } else if name == c"VK_NV_raw_access_chains" {
            self.nv_raw_access_chains = true;
        } else if name == c"VK_NV_external_compute_queue" {
            self.nv_external_compute_queue = true;
        } else if name == c"VK_KHR_shader_relaxed_extended_instruction" {
            self.khr_shader_relaxed_extended_instruction = true;
        } else if name == c"VK_NV_command_buffer_inheritance" {
            self.nv_command_buffer_inheritance = true;
        } else if name == c"VK_KHR_maintenance7" {
            self.khr_maintenance7 = true;
        } else if name == c"VK_NV_shader_atomic_float16_vector" {
            self.nv_shader_atomic_float16_vector = true;
        } else if name == c"VK_EXT_shader_replicated_composites" {
            self.ext_shader_replicated_composites = true;
        } else if name == c"VK_EXT_shader_float8" {
            self.ext_shader_float8 = true;
        } else if name == c"VK_NV_ray_tracing_validation" {
            self.nv_ray_tracing_validation = true;
        } else if name == c"VK_NV_cluster_acceleration_structure" {
            self.nv_cluster_acceleration_structure = true;
        } else if name == c"VK_NV_partitioned_acceleration_structure" {
            self.nv_partitioned_acceleration_structure = true;
        } else if name == c"VK_EXT_device_generated_commands" {
            self.ext_device_generated_commands = true;
        } else if name == c"VK_KHR_maintenance8" {
            self.khr_maintenance8 = true;
        } else if name == c"VK_MESA_image_alignment_control" {
            self.mesa_image_alignment_control = true;
        } else if name == c"VK_KHR_shader_fma" {
            self.khr_shader_fma = true;
        } else if name == c"VK_NV_push_constant_bank" {
            self.nv_push_constant_bank = true;
        } else if name == c"VK_EXT_ray_tracing_invocation_reorder" {
            self.ext_ray_tracing_invocation_reorder = true;
        } else if name == c"VK_EXT_depth_clamp_control" {
            self.ext_depth_clamp_control = true;
        } else if name == c"VK_KHR_maintenance9" {
            self.khr_maintenance9 = true;
        } else if name == c"VK_HUAWEI_hdr_vivid" {
            self.huawei_hdr_vivid = true;
        } else if name == c"VK_NV_cooperative_matrix2" {
            self.nv_cooperative_matrix2 = true;
        } else if name == c"VK_ARM_pipeline_opacity_micromap" {
            self.arm_pipeline_opacity_micromap = true;
        } else if name == c"VK_EXT_external_memory_metal" {
            self.ext_external_memory_metal = true;
        } else if name == c"VK_KHR_depth_clamp_zero_one" {
            self.khr_depth_clamp_zero_one = true;
        } else if name == c"VK_ARM_performance_counters_by_region" {
            self.arm_performance_counters_by_region = true;
        } else if name == c"VK_EXT_vertex_attribute_robustness" {
            self.ext_vertex_attribute_robustness = true;
        } else if name == c"VK_ARM_format_pack" {
            self.arm_format_pack = true;
        } else if name == c"VK_VALVE_fragment_density_map_layered" {
            self.valve_fragment_density_map_layered = true;
        } else if name == c"VK_KHR_robustness2" {
            self.khr_robustness2 = true;
        } else if name == c"VK_NV_present_metering" {
            self.nv_present_metering = true;
        } else if name == c"VK_EXT_fragment_density_map_offset" {
            self.ext_fragment_density_map_offset = true;
        } else if name == c"VK_EXT_zero_initialize_device_memory" {
            self.ext_zero_initialize_device_memory = true;
        } else if name == c"VK_KHR_present_mode_fifo_latest_ready" {
            self.khr_present_mode_fifo_latest_ready = true;
        } else if name == c"VK_EXT_shader_64bit_indexing" {
            self.ext_shader_64bit_indexing = true;
        } else if name == c"VK_EXT_custom_resolve" {
            self.ext_custom_resolve = true;
        } else if name == c"VK_QCOM_data_graph_model" {
            self.qcom_data_graph_model = true;
        } else if name == c"VK_KHR_maintenance10" {
            self.khr_maintenance10 = true;
        } else if name == c"VK_EXT_shader_long_vector" {
            self.ext_shader_long_vector = true;
        } else if name == c"VK_SEC_pipeline_cache_incremental_mode" {
            self.sec_pipeline_cache_incremental_mode = true;
        } else if name == c"VK_EXT_shader_uniform_buffer_unsized_array" {
            self.ext_shader_uniform_buffer_unsized_array = true;
        } else if name == c"VK_NV_compute_occupancy_priority" {
            self.nv_compute_occupancy_priority = true;
        } else if name == c"VK_EXT_shader_subgroup_partitioned" {
            self.ext_shader_subgroup_partitioned = true;
        } else if name == c"VK_VALVE_shader_mixed_float_dot_product" {
            self.valve_shader_mixed_float_dot_product = true;
        }
    }
    pub fn new(core_version: vk::Version) -> Self {
        Self {
            core_version,
            khr_swapchain: false,
            khr_display_swapchain: false,
            nv_glsl_shader: false,
            ext_depth_range_unrestricted: false,
            khr_sampler_mirror_clamp_to_edge: false,
            img_filter_cubic: false,
            amd_rasterization_order: false,
            amd_shader_trinary_minmax: false,
            amd_shader_explicit_vertex_parameter: false,
            ext_debug_marker: false,
            amd_gcn_shader: false,
            nv_dedicated_allocation: false,
            ext_transform_feedback: false,
            nvx_binary_import: false,
            nvx_image_view_handle: false,
            amd_draw_indirect_count: false,
            amd_negative_viewport_height: false,
            amd_gpu_shader_half_float: false,
            amd_shader_ballot: false,
            amd_texture_gather_bias_lod: false,
            amd_shader_info: false,
            khr_dynamic_rendering: false,
            amd_shader_image_load_store_lod: false,
            nv_corner_sampled_image: false,
            khr_multiview: false,
            img_format_pvrtc: false,
            nv_external_memory: false,
            nv_external_memory_win32: false,
            nv_win32_keyed_mutex: false,
            khr_device_group: false,
            khr_shader_draw_parameters: false,
            ext_shader_subgroup_ballot: false,
            ext_shader_subgroup_vote: false,
            ext_texture_compression_astc_hdr: false,
            ext_astc_decode_mode: false,
            ext_pipeline_robustness: false,
            khr_maintenance1: false,
            khr_external_memory: false,
            khr_external_memory_win32: false,
            khr_external_memory_fd: false,
            khr_win32_keyed_mutex: false,
            khr_external_semaphore: false,
            khr_external_semaphore_win32: false,
            khr_external_semaphore_fd: false,
            khr_push_descriptor: false,
            ext_conditional_rendering: false,
            khr_shader_float16_int8: false,
            khr_16bit_storage: false,
            khr_incremental_present: false,
            khr_descriptor_update_template: false,
            nv_clip_space_w_scaling: false,
            ext_display_control: false,
            google_display_timing: false,
            nv_sample_mask_override_coverage: false,
            nv_geometry_shader_passthrough: false,
            nv_viewport_array2: false,
            nvx_multiview_per_view_attributes: false,
            nv_viewport_swizzle: false,
            ext_discard_rectangles: false,
            ext_conservative_rasterization: false,
            ext_depth_clip_enable: false,
            ext_hdr_metadata: false,
            khr_imageless_framebuffer: false,
            khr_create_renderpass2: false,
            img_relaxed_line_rasterization: false,
            khr_shared_presentable_image: false,
            khr_external_fence: false,
            khr_external_fence_win32: false,
            khr_external_fence_fd: false,
            khr_performance_query: false,
            khr_maintenance2: false,
            khr_variable_pointers: false,
            ext_external_memory_dma_buf: false,
            ext_queue_family_foreign: false,
            khr_dedicated_allocation: false,
            android_external_memory_android_hardware_buffer: false,
            ext_sampler_filter_minmax: false,
            khr_storage_buffer_storage_class: false,
            amd_gpu_shader_int16: false,
            amdx_shader_enqueue: false,
            ext_descriptor_heap: false,
            amd_mixed_attachment_samples: false,
            amd_shader_fragment_mask: false,
            ext_inline_uniform_block: false,
            ext_shader_stencil_export: false,
            khr_shader_bfloat16: false,
            ext_sample_locations: false,
            khr_relaxed_block_layout: false,
            khr_get_memory_requirements2: false,
            khr_image_format_list: false,
            ext_blend_operation_advanced: false,
            nv_fragment_coverage_to_color: false,
            khr_acceleration_structure: false,
            khr_ray_tracing_pipeline: false,
            khr_ray_query: false,
            nv_framebuffer_mixed_samples: false,
            nv_fill_rectangle: false,
            nv_shader_sm_builtins: false,
            ext_post_depth_coverage: false,
            khr_sampler_ycbcr_conversion: false,
            khr_bind_memory2: false,
            ext_image_drm_format_modifier: false,
            ext_validation_cache: false,
            ext_descriptor_indexing: false,
            ext_shader_viewport_index_layer: false,
            khr_portability_subset: false,
            nv_shading_rate_image: false,
            nv_ray_tracing: false,
            nv_representative_fragment_test: false,
            khr_maintenance3: false,
            khr_draw_indirect_count: false,
            ext_filter_cubic: false,
            qcom_render_pass_shader_resolve: false,
            qcom_cooperative_matrix_conversion: false,
            ext_global_priority: false,
            khr_shader_subgroup_extended_types: false,
            khr_8bit_storage: false,
            ext_external_memory_host: false,
            amd_buffer_marker: false,
            khr_shader_atomic_int64: false,
            khr_shader_clock: false,
            amd_pipeline_compiler_control: false,
            ext_calibrated_timestamps: false,
            amd_shader_core_properties: false,
            khr_global_priority: false,
            amd_memory_overallocation_behavior: false,
            ext_vertex_attribute_divisor: false,
            ext_pipeline_creation_feedback: false,
            khr_driver_properties: false,
            khr_shader_float_controls: false,
            nv_shader_subgroup_partitioned: false,
            khr_depth_stencil_resolve: false,
            khr_swapchain_mutable_format: false,
            nv_compute_shader_derivatives: false,
            nv_mesh_shader: false,
            nv_fragment_shader_barycentric: false,
            nv_shader_image_footprint: false,
            nv_scissor_exclusive: false,
            nv_device_diagnostic_checkpoints: false,
            khr_timeline_semaphore: false,
            ext_present_timing: false,
            intel_shader_integer_functions2: false,
            intel_performance_query: false,
            khr_vulkan_memory_model: false,
            ext_pci_bus_info: false,
            amd_display_native_hdr: false,
            khr_shader_terminate_invocation: false,
            ext_fragment_density_map: false,
            ext_scalar_block_layout: false,
            google_hlsl_functionality1: false,
            google_decorate_string: false,
            ext_subgroup_size_control: false,
            khr_fragment_shading_rate: false,
            amd_shader_core_properties2: false,
            amd_device_coherent_memory: false,
            khr_dynamic_rendering_local_read: false,
            ext_shader_image_atomic_int64: false,
            khr_shader_quad_control: false,
            khr_spirv_1_4: false,
            ext_memory_budget: false,
            ext_memory_priority: false,
            nv_dedicated_allocation_image_aliasing: false,
            khr_separate_depth_stencil_layouts: false,
            ext_buffer_device_address: false,
            ext_tooling_info: false,
            ext_separate_stencil_usage: false,
            khr_present_wait: false,
            nv_cooperative_matrix: false,
            nv_coverage_reduction_mode: false,
            ext_fragment_shader_interlock: false,
            ext_ycbcr_image_arrays: false,
            khr_uniform_buffer_standard_layout: false,
            ext_provoking_vertex: false,
            ext_full_screen_exclusive: false,
            khr_buffer_device_address: false,
            ext_line_rasterization: false,
            ext_shader_atomic_float: false,
            ext_host_query_reset: false,
            ext_index_type_uint8: false,
            ext_extended_dynamic_state: false,
            khr_deferred_host_operations: false,
            khr_pipeline_executable_properties: false,
            ext_host_image_copy: false,
            khr_map_memory2: false,
            ext_map_memory_placed: false,
            ext_shader_atomic_float2: false,
            ext_swapchain_maintenance1: false,
            ext_shader_demote_to_helper_invocation: false,
            nv_device_generated_commands: false,
            nv_inherited_viewport_scissor: false,
            khr_shader_integer_dot_product: false,
            ext_texel_buffer_alignment: false,
            qcom_render_pass_transform: false,
            ext_depth_bias_control: false,
            ext_device_memory_report: false,
            ext_robustness2: false,
            ext_custom_border_color: false,
            ext_texture_compression_astc_3d: false,
            google_user_type: false,
            khr_pipeline_library: false,
            nv_present_barrier: false,
            khr_shader_non_semantic_info: false,
            khr_present_id: false,
            ext_private_data: false,
            ext_pipeline_creation_cache_control: false,
            nv_device_diagnostics_config: false,
            qcom_render_pass_store_ops: false,
            nv_cuda_kernel_launch: false,
            qcom_tile_shading: false,
            nv_low_latency: false,
            ext_metal_objects: false,
            khr_synchronization2: false,
            ext_descriptor_buffer: false,
            ext_graphics_pipeline_library: false,
            amd_shader_early_and_late_fragment_tests: false,
            khr_fragment_shader_barycentric: false,
            khr_shader_subgroup_uniform_control_flow: false,
            khr_zero_initialize_workgroup_memory: false,
            nv_fragment_shading_rate_enums: false,
            nv_ray_tracing_motion_blur: false,
            ext_mesh_shader: false,
            ext_ycbcr_2plane_444_formats: false,
            ext_fragment_density_map2: false,
            qcom_rotated_copy_commands: false,
            ext_image_robustness: false,
            khr_workgroup_memory_explicit_layout: false,
            khr_copy_commands2: false,
            ext_image_compression_control: false,
            ext_attachment_feedback_loop_layout: false,
            ext_4444_formats: false,
            ext_device_fault: false,
            arm_rasterization_order_attachment_access: false,
            ext_rgba10x6_formats: false,
            nv_acquire_winrt_display: false,
            valve_mutable_descriptor_type: false,
            ext_vertex_input_dynamic_state: false,
            ext_physical_device_drm: false,
            ext_device_address_binding_report: false,
            ext_depth_clip_control: false,
            ext_primitive_topology_list_restart: false,
            khr_format_feature_flags2: false,
            ext_present_mode_fifo_latest_ready: false,
            fuchsia_external_memory: false,
            fuchsia_external_semaphore: false,
            fuchsia_buffer_collection: false,
            huawei_subpass_shading: false,
            huawei_invocation_mask: false,
            nv_external_memory_rdma: false,
            ext_pipeline_properties: false,
            ext_frame_boundary: false,
            ext_multisampled_render_to_single_sampled: false,
            ext_extended_dynamic_state2: false,
            ext_color_write_enable: false,
            ext_primitives_generated_query: false,
            khr_ray_tracing_maintenance1: false,
            khr_shader_untyped_pointers: false,
            ext_global_priority_query: false,
            ext_image_view_min_lod: false,
            ext_multi_draw: false,
            ext_image_2d_view_of_3d: false,
            ext_shader_tile_image: false,
            ext_opacity_micromap: false,
            nv_displacement_micromap: false,
            ext_load_store_op_none: false,
            huawei_cluster_culling_shader: false,
            ext_border_color_swizzle: false,
            ext_pageable_device_local_memory: false,
            khr_maintenance4: false,
            arm_shader_core_properties: false,
            khr_shader_subgroup_rotate: false,
            arm_scheduling_controls: false,
            ext_image_sliced_view_of_3d: false,
            valve_descriptor_set_host_mapping: false,
            ext_depth_clamp_zero_one: false,
            ext_non_seamless_cube_map: false,
            arm_render_pass_striped: false,
            qcom_fragment_density_map_offset: false,
            nv_copy_memory_indirect: false,
            nv_memory_decompression: false,
            nv_device_generated_commands_compute: false,
            nv_ray_tracing_linear_swept_spheres: false,
            nv_linear_color_attachment: false,
            khr_shader_maximal_reconvergence: false,
            ext_image_compression_control_swapchain: false,
            qcom_image_processing: false,
            ext_nested_command_buffer: false,
            ohos_external_memory: false,
            ext_external_memory_acquire_unmodified: false,
            ext_extended_dynamic_state3: false,
            ext_subpass_merge_feedback: false,
            arm_tensors: false,
            ext_shader_module_identifier: false,
            ext_rasterization_order_attachment_access: false,
            nv_optical_flow: false,
            ext_legacy_dithering: false,
            ext_pipeline_protected_access: false,
            android_external_format_resolve: false,
            khr_maintenance5: false,
            amd_anti_lag: false,
            amdx_dense_geometry_format: false,
            khr_present_id2: false,
            khr_present_wait2: false,
            khr_ray_tracing_position_fetch: false,
            ext_shader_object: false,
            khr_pipeline_binary: false,
            qcom_tile_properties: false,
            sec_amigo_profiling: false,
            khr_swapchain_maintenance1: false,
            qcom_multiview_per_view_viewports: false,
            nv_ray_tracing_invocation_reorder: false,
            nv_cooperative_vector: false,
            nv_extended_sparse_address_space: false,
            ext_mutable_descriptor_type: false,
            ext_legacy_vertex_attributes: false,
            arm_shader_core_builtins: false,
            ext_pipeline_library_group_handles: false,
            ext_dynamic_rendering_unused_attachments: false,
            khr_internally_synchronized_queues: false,
            nv_low_latency2: false,
            khr_cooperative_matrix: false,
            arm_data_graph: false,
            qcom_multiview_per_view_render_areas: false,
            khr_compute_shader_derivatives: false,
            nv_per_stage_descriptor_set: false,
            qcom_image_processing2: false,
            qcom_filter_cubic_weights: false,
            qcom_ycbcr_degamma: false,
            qcom_filter_cubic_clamp: false,
            ext_attachment_feedback_loop_dynamic_state: false,
            khr_vertex_attribute_divisor: false,
            khr_load_store_op_none: false,
            khr_unified_image_layouts: false,
            khr_shader_float_controls2: false,
            msft_layered_driver: false,
            khr_index_type_uint8: false,
            khr_line_rasterization: false,
            khr_calibrated_timestamps: false,
            khr_shader_expect_assume: false,
            khr_maintenance6: false,
            nv_descriptor_pool_overallocation: false,
            qcom_tile_memory_heap: false,
            khr_copy_memory_indirect: false,
            ext_memory_decompression: false,
            nv_raw_access_chains: false,
            nv_external_compute_queue: false,
            khr_shader_relaxed_extended_instruction: false,
            nv_command_buffer_inheritance: false,
            khr_maintenance7: false,
            nv_shader_atomic_float16_vector: false,
            ext_shader_replicated_composites: false,
            ext_shader_float8: false,
            nv_ray_tracing_validation: false,
            nv_cluster_acceleration_structure: false,
            nv_partitioned_acceleration_structure: false,
            ext_device_generated_commands: false,
            khr_maintenance8: false,
            mesa_image_alignment_control: false,
            khr_shader_fma: false,
            nv_push_constant_bank: false,
            ext_ray_tracing_invocation_reorder: false,
            ext_depth_clamp_control: false,
            khr_maintenance9: false,
            huawei_hdr_vivid: false,
            nv_cooperative_matrix2: false,
            arm_pipeline_opacity_micromap: false,
            ext_external_memory_metal: false,
            khr_depth_clamp_zero_one: false,
            arm_performance_counters_by_region: false,
            ext_vertex_attribute_robustness: false,
            arm_format_pack: false,
            valve_fragment_density_map_layered: false,
            khr_robustness2: false,
            nv_present_metering: false,
            ext_fragment_density_map_offset: false,
            ext_zero_initialize_device_memory: false,
            khr_present_mode_fifo_latest_ready: false,
            ext_shader_64bit_indexing: false,
            ext_custom_resolve: false,
            qcom_data_graph_model: false,
            khr_maintenance10: false,
            ext_shader_long_vector: false,
            sec_pipeline_cache_incremental_mode: false,
            ext_shader_uniform_buffer_unsized_array: false,
            nv_compute_occupancy_priority: false,
            ext_shader_subgroup_partitioned: false,
            valve_shader_mixed_float_dot_product: false,
        }
    }
    pub fn from_properties(core_version: vk::Version, properties: &[vk::ExtensionProperties]) -> Self {
        let mut ext = Self::new(core_version);
        for ep in properties.iter() {
            if ep.extension_name.iter().any(|&c| c == 0) {
                let name = unsafe { CStr::from_ptr(ep.extension_name.as_ptr()) };
                ext.enable_by_name(name);
            }
        }
        ext
    }
    pub fn supports_khr_swapchain(&self) -> bool {
        self.khr_swapchain
    }
    pub fn enable_khr_swapchain(&mut self) {
        self.khr_swapchain = true;
    }
    pub fn supports_khr_display_swapchain(&self) -> bool {
        self.khr_display_swapchain && self.supports_khr_swapchain()
    }
    pub fn enable_khr_display_swapchain(&mut self) {
        self.khr_display_swapchain = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_nv_glsl_shader(&self) -> bool {
        self.nv_glsl_shader
    }
    pub fn enable_nv_glsl_shader(&mut self) {
        self.nv_glsl_shader = true;
    }
    pub fn supports_ext_depth_range_unrestricted(&self) -> bool {
        self.ext_depth_range_unrestricted
    }
    pub fn enable_ext_depth_range_unrestricted(&mut self) {
        self.ext_depth_range_unrestricted = true;
    }
    pub fn supports_khr_sampler_mirror_clamp_to_edge(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_sampler_mirror_clamp_to_edge
    }
    pub fn enable_khr_sampler_mirror_clamp_to_edge(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_sampler_mirror_clamp_to_edge = true;
        }
    }
    pub fn supports_img_filter_cubic(&self) -> bool {
        self.img_filter_cubic
    }
    pub fn enable_img_filter_cubic(&mut self) {
        self.img_filter_cubic = true;
    }
    pub fn supports_amd_rasterization_order(&self) -> bool {
        self.amd_rasterization_order
    }
    pub fn enable_amd_rasterization_order(&mut self) {
        self.amd_rasterization_order = true;
    }
    pub fn supports_amd_shader_trinary_minmax(&self) -> bool {
        self.amd_shader_trinary_minmax
    }
    pub fn enable_amd_shader_trinary_minmax(&mut self) {
        self.amd_shader_trinary_minmax = true;
    }
    pub fn supports_amd_shader_explicit_vertex_parameter(&self) -> bool {
        self.amd_shader_explicit_vertex_parameter
    }
    pub fn enable_amd_shader_explicit_vertex_parameter(&mut self) {
        self.amd_shader_explicit_vertex_parameter = true;
    }
    pub fn supports_ext_debug_marker(&self) -> bool {
        self.ext_debug_marker
    }
    pub fn enable_ext_debug_marker(&mut self) {
        self.ext_debug_marker = true;
    }
    pub fn supports_amd_gcn_shader(&self) -> bool {
        self.amd_gcn_shader
    }
    pub fn enable_amd_gcn_shader(&mut self) {
        self.amd_gcn_shader = true;
    }
    pub fn supports_nv_dedicated_allocation(&self) -> bool {
        self.nv_dedicated_allocation
    }
    pub fn enable_nv_dedicated_allocation(&mut self) {
        self.nv_dedicated_allocation = true;
    }
    pub fn supports_ext_transform_feedback(&self) -> bool {
        self.ext_transform_feedback
    }
    pub fn enable_ext_transform_feedback(&mut self) {
        self.ext_transform_feedback = true;
    }
    pub fn supports_nvx_binary_import(&self) -> bool {
        self.nvx_binary_import
    }
    pub fn enable_nvx_binary_import(&mut self) {
        self.nvx_binary_import = true;
    }
    pub fn supports_nvx_image_view_handle(&self) -> bool {
        self.nvx_image_view_handle
    }
    pub fn enable_nvx_image_view_handle(&mut self) {
        self.nvx_image_view_handle = true;
    }
    pub fn supports_amd_draw_indirect_count(&self) -> bool {
        self.amd_draw_indirect_count
    }
    pub fn enable_amd_draw_indirect_count(&mut self) {
        self.amd_draw_indirect_count = true;
    }
    pub fn supports_amd_negative_viewport_height(&self) -> bool {
        self.amd_negative_viewport_height
    }
    pub fn enable_amd_negative_viewport_height(&mut self) {
        self.amd_negative_viewport_height = true;
    }
    pub fn supports_amd_gpu_shader_half_float(&self) -> bool {
        self.amd_gpu_shader_half_float
    }
    pub fn enable_amd_gpu_shader_half_float(&mut self) {
        self.amd_gpu_shader_half_float = true;
    }
    pub fn supports_amd_shader_ballot(&self) -> bool {
        self.amd_shader_ballot
    }
    pub fn enable_amd_shader_ballot(&mut self) {
        self.amd_shader_ballot = true;
    }
    pub fn supports_amd_texture_gather_bias_lod(&self) -> bool {
        self.amd_texture_gather_bias_lod
    }
    pub fn enable_amd_texture_gather_bias_lod(&mut self) {
        self.amd_texture_gather_bias_lod = true;
    }
    pub fn supports_amd_shader_info(&self) -> bool {
        self.amd_shader_info
    }
    pub fn enable_amd_shader_info(&mut self) {
        self.amd_shader_info = true;
    }
    pub fn supports_khr_dynamic_rendering(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.khr_dynamic_rendering
                && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || self.supports_khr_depth_stencil_resolve()))
    }
    pub fn enable_khr_dynamic_rendering(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_dynamic_rendering = true;
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_depth_stencil_resolve();
            }
        }
    }
    pub fn supports_amd_shader_image_load_store_lod(&self) -> bool {
        self.amd_shader_image_load_store_lod
    }
    pub fn enable_amd_shader_image_load_store_lod(&mut self) {
        self.amd_shader_image_load_store_lod = true;
    }
    pub fn supports_nv_corner_sampled_image(&self) -> bool {
        self.nv_corner_sampled_image
    }
    pub fn enable_nv_corner_sampled_image(&mut self) {
        self.nv_corner_sampled_image = true;
    }
    pub fn supports_khr_multiview(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_multiview
    }
    pub fn enable_khr_multiview(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_multiview = true;
        }
    }
    pub fn supports_img_format_pvrtc(&self) -> bool {
        self.img_format_pvrtc
    }
    pub fn enable_img_format_pvrtc(&mut self) {
        self.img_format_pvrtc = true;
    }
    pub fn supports_nv_external_memory(&self) -> bool {
        self.nv_external_memory
    }
    pub fn enable_nv_external_memory(&mut self) {
        self.nv_external_memory = true;
    }
    pub fn supports_nv_external_memory_win32(&self) -> bool {
        self.nv_external_memory_win32 && self.supports_nv_external_memory()
    }
    pub fn enable_nv_external_memory_win32(&mut self) {
        self.nv_external_memory_win32 = true;
        self.enable_nv_external_memory();
    }
    pub fn supports_nv_win32_keyed_mutex(&self) -> bool {
        self.nv_win32_keyed_mutex && self.supports_nv_external_memory_win32()
    }
    pub fn enable_nv_win32_keyed_mutex(&mut self) {
        self.nv_win32_keyed_mutex = true;
        self.enable_nv_external_memory_win32();
    }
    pub fn supports_khr_device_group(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_device_group
    }
    pub fn enable_khr_device_group(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_device_group = true;
        }
    }
    pub fn supports_khr_shader_draw_parameters(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_shader_draw_parameters
    }
    pub fn enable_khr_shader_draw_parameters(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_shader_draw_parameters = true;
        }
    }
    pub fn supports_ext_shader_subgroup_ballot(&self) -> bool {
        self.ext_shader_subgroup_ballot
    }
    pub fn enable_ext_shader_subgroup_ballot(&mut self) {
        self.ext_shader_subgroup_ballot = true;
    }
    pub fn supports_ext_shader_subgroup_vote(&self) -> bool {
        self.ext_shader_subgroup_vote
    }
    pub fn enable_ext_shader_subgroup_vote(&mut self) {
        self.ext_shader_subgroup_vote = true;
    }
    pub fn supports_ext_texture_compression_astc_hdr(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_texture_compression_astc_hdr
    }
    pub fn enable_ext_texture_compression_astc_hdr(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_texture_compression_astc_hdr = true;
        }
    }
    pub fn supports_ext_astc_decode_mode(&self) -> bool {
        self.ext_astc_decode_mode
    }
    pub fn enable_ext_astc_decode_mode(&mut self) {
        self.ext_astc_decode_mode = true;
    }
    pub fn supports_ext_pipeline_robustness(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.ext_pipeline_robustness
    }
    pub fn enable_ext_pipeline_robustness(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.ext_pipeline_robustness = true;
        }
    }
    pub fn supports_khr_maintenance1(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_maintenance1
    }
    pub fn enable_khr_maintenance1(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_maintenance1 = true;
        }
    }
    pub fn supports_khr_external_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_external_memory
    }
    pub fn enable_khr_external_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_memory = true;
        }
    }
    pub fn supports_khr_external_memory_win32(&self) -> bool {
        self.khr_external_memory_win32
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_khr_external_memory_win32(&mut self) {
        self.khr_external_memory_win32 = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_external_memory_fd(&self) -> bool {
        self.khr_external_memory_fd
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_khr_external_memory_fd(&mut self) {
        self.khr_external_memory_fd = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_win32_keyed_mutex(&self) -> bool {
        self.khr_win32_keyed_mutex && self.supports_khr_external_memory_win32()
    }
    pub fn enable_khr_win32_keyed_mutex(&mut self) {
        self.khr_win32_keyed_mutex = true;
        self.enable_khr_external_memory_win32();
    }
    pub fn supports_khr_external_semaphore(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_external_semaphore
    }
    pub fn enable_khr_external_semaphore(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_semaphore = true;
        }
    }
    pub fn supports_khr_external_semaphore_win32(&self) -> bool {
        self.khr_external_semaphore_win32 && self.supports_khr_external_semaphore()
    }
    pub fn enable_khr_external_semaphore_win32(&mut self) {
        self.khr_external_semaphore_win32 = true;
        self.enable_khr_external_semaphore();
    }
    pub fn supports_khr_external_semaphore_fd(&self) -> bool {
        self.khr_external_semaphore_fd
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_semaphore())
    }
    pub fn enable_khr_external_semaphore_fd(&mut self) {
        self.khr_external_semaphore_fd = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_semaphore();
        }
    }
    pub fn supports_khr_push_descriptor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_push_descriptor
    }
    pub fn enable_khr_push_descriptor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_push_descriptor = true;
        }
    }
    pub fn supports_ext_conditional_rendering(&self) -> bool {
        self.ext_conditional_rendering
    }
    pub fn enable_ext_conditional_rendering(&mut self) {
        self.ext_conditional_rendering = true;
    }
    pub fn supports_khr_shader_float16_int8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_shader_float16_int8
    }
    pub fn enable_khr_shader_float16_int8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_shader_float16_int8 = true;
        }
    }
    pub fn supports_khr_16bit_storage(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_16bit_storage
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_storage_buffer_storage_class()))
    }
    pub fn enable_khr_16bit_storage(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_16bit_storage = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_storage_buffer_storage_class();
            }
        }
    }
    pub fn supports_khr_incremental_present(&self) -> bool {
        self.khr_incremental_present && self.supports_khr_swapchain()
    }
    pub fn enable_khr_incremental_present(&mut self) {
        self.khr_incremental_present = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_descriptor_update_template(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_descriptor_update_template
    }
    pub fn enable_khr_descriptor_update_template(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_descriptor_update_template = true;
        }
    }
    pub fn supports_nv_clip_space_w_scaling(&self) -> bool {
        self.nv_clip_space_w_scaling
    }
    pub fn enable_nv_clip_space_w_scaling(&mut self) {
        self.nv_clip_space_w_scaling = true;
    }
    pub fn supports_ext_display_control(&self) -> bool {
        self.ext_display_control && self.supports_khr_swapchain()
    }
    pub fn enable_ext_display_control(&mut self) {
        self.ext_display_control = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_google_display_timing(&self) -> bool {
        self.google_display_timing && self.supports_khr_swapchain()
    }
    pub fn enable_google_display_timing(&mut self) {
        self.google_display_timing = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_nv_sample_mask_override_coverage(&self) -> bool {
        self.nv_sample_mask_override_coverage
    }
    pub fn enable_nv_sample_mask_override_coverage(&mut self) {
        self.nv_sample_mask_override_coverage = true;
    }
    pub fn supports_nv_geometry_shader_passthrough(&self) -> bool {
        self.nv_geometry_shader_passthrough
    }
    pub fn enable_nv_geometry_shader_passthrough(&mut self) {
        self.nv_geometry_shader_passthrough = true;
    }
    pub fn supports_nv_viewport_array2(&self) -> bool {
        self.nv_viewport_array2
    }
    pub fn enable_nv_viewport_array2(&mut self) {
        self.nv_viewport_array2 = true;
    }
    pub fn supports_nvx_multiview_per_view_attributes(&self) -> bool {
        self.nvx_multiview_per_view_attributes
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_multiview())
    }
    pub fn enable_nvx_multiview_per_view_attributes(&mut self) {
        self.nvx_multiview_per_view_attributes = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_multiview();
        }
    }
    pub fn supports_nv_viewport_swizzle(&self) -> bool {
        self.nv_viewport_swizzle
    }
    pub fn enable_nv_viewport_swizzle(&mut self) {
        self.nv_viewport_swizzle = true;
    }
    pub fn supports_ext_discard_rectangles(&self) -> bool {
        self.ext_discard_rectangles
    }
    pub fn enable_ext_discard_rectangles(&mut self) {
        self.ext_discard_rectangles = true;
    }
    pub fn supports_ext_conservative_rasterization(&self) -> bool {
        self.ext_conservative_rasterization
    }
    pub fn enable_ext_conservative_rasterization(&mut self) {
        self.ext_conservative_rasterization = true;
    }
    pub fn supports_ext_depth_clip_enable(&self) -> bool {
        self.ext_depth_clip_enable
    }
    pub fn enable_ext_depth_clip_enable(&mut self) {
        self.ext_depth_clip_enable = true;
    }
    pub fn supports_ext_hdr_metadata(&self) -> bool {
        self.ext_hdr_metadata && self.supports_khr_swapchain()
    }
    pub fn enable_ext_hdr_metadata(&mut self) {
        self.ext_hdr_metadata = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_imageless_framebuffer(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_imageless_framebuffer
                && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                        || self.supports_khr_maintenance2())
                        && self.supports_khr_image_format_list())))
    }
    pub fn enable_khr_imageless_framebuffer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_imageless_framebuffer = true;
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                    self.enable_khr_maintenance2();
                }
                self.enable_khr_image_format_list();
            }
        }
    }
    pub fn supports_khr_create_renderpass2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_create_renderpass2
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || (self.supports_khr_multiview() && self.supports_khr_maintenance2())))
    }
    pub fn enable_khr_create_renderpass2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_create_renderpass2 = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_multiview();
                self.enable_khr_maintenance2();
            }
        }
    }
    pub fn supports_img_relaxed_line_rasterization(&self) -> bool {
        self.img_relaxed_line_rasterization
    }
    pub fn enable_img_relaxed_line_rasterization(&mut self) {
        self.img_relaxed_line_rasterization = true;
    }
    pub fn supports_khr_shared_presentable_image(&self) -> bool {
        self.khr_shared_presentable_image && self.supports_khr_swapchain()
    }
    pub fn enable_khr_shared_presentable_image(&mut self) {
        self.khr_shared_presentable_image = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_external_fence(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_external_fence
    }
    pub fn enable_khr_external_fence(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_external_fence = true;
        }
    }
    pub fn supports_khr_external_fence_win32(&self) -> bool {
        self.khr_external_fence_win32 && self.supports_khr_external_fence()
    }
    pub fn enable_khr_external_fence_win32(&mut self) {
        self.khr_external_fence_win32 = true;
        self.enable_khr_external_fence();
    }
    pub fn supports_khr_external_fence_fd(&self) -> bool {
        self.khr_external_fence_fd
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_fence())
    }
    pub fn enable_khr_external_fence_fd(&mut self) {
        self.khr_external_fence_fd = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_fence();
        }
    }
    pub fn supports_khr_performance_query(&self) -> bool {
        self.khr_performance_query
    }
    pub fn enable_khr_performance_query(&mut self) {
        self.khr_performance_query = true;
    }
    pub fn supports_khr_maintenance2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_maintenance2
    }
    pub fn enable_khr_maintenance2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_maintenance2 = true;
        }
    }
    pub fn supports_khr_variable_pointers(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_variable_pointers
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_storage_buffer_storage_class()))
    }
    pub fn enable_khr_variable_pointers(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_variable_pointers = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_storage_buffer_storage_class();
            }
        }
    }
    pub fn supports_ext_external_memory_dma_buf(&self) -> bool {
        self.ext_external_memory_dma_buf && self.supports_khr_external_memory_fd()
    }
    pub fn enable_ext_external_memory_dma_buf(&mut self) {
        self.ext_external_memory_dma_buf = true;
        self.enable_khr_external_memory_fd();
    }
    pub fn supports_ext_queue_family_foreign(&self) -> bool {
        self.ext_queue_family_foreign
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_ext_queue_family_foreign(&mut self) {
        self.ext_queue_family_foreign = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_dedicated_allocation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_dedicated_allocation
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_get_memory_requirements2()))
    }
    pub fn enable_khr_dedicated_allocation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_dedicated_allocation = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_get_memory_requirements2();
            }
        }
    }
    pub fn supports_android_external_memory_android_hardware_buffer(&self) -> bool {
        self.android_external_memory_android_hardware_buffer
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || (self.supports_khr_sampler_ycbcr_conversion()
                    && self.supports_khr_external_memory()
                    && self.supports_khr_dedicated_allocation()))
            && self.supports_ext_queue_family_foreign()
    }
    pub fn enable_android_external_memory_android_hardware_buffer(&mut self) {
        self.android_external_memory_android_hardware_buffer = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
            self.enable_khr_external_memory();
            self.enable_khr_dedicated_allocation();
        }
        self.enable_ext_queue_family_foreign();
    }
    pub fn supports_ext_sampler_filter_minmax(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.ext_sampler_filter_minmax
    }
    pub fn enable_ext_sampler_filter_minmax(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_sampler_filter_minmax = true;
        }
    }
    pub fn supports_khr_storage_buffer_storage_class(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_storage_buffer_storage_class
    }
    pub fn enable_khr_storage_buffer_storage_class(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_storage_buffer_storage_class = true;
        }
    }
    pub fn supports_amd_gpu_shader_int16(&self) -> bool {
        self.amd_gpu_shader_int16
    }
    pub fn enable_amd_gpu_shader_int16(&mut self) {
        self.amd_gpu_shader_int16 = true;
    }
    pub fn supports_amdx_shader_enqueue(&self) -> bool {
        self.amdx_shader_enqueue
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || (self.supports_khr_synchronization2()
                    && self.supports_khr_spirv_1_4()
                    && self.supports_ext_extended_dynamic_state()))
            && self.supports_khr_maintenance5()
            && self.supports_khr_pipeline_library()
    }
    pub fn enable_amdx_shader_enqueue(&mut self) {
        self.amdx_shader_enqueue = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
            self.enable_khr_spirv_1_4();
            self.enable_ext_extended_dynamic_state();
        }
        self.enable_khr_maintenance5();
        self.enable_khr_pipeline_library();
    }
    pub fn supports_ext_descriptor_heap(&self) -> bool {
        self.ext_descriptor_heap
            && self.supports_khr_maintenance5()
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_buffer_device_address())
    }
    pub fn enable_ext_descriptor_heap(&mut self) {
        self.ext_descriptor_heap = true;
        self.enable_khr_maintenance5();
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_amd_mixed_attachment_samples(&self) -> bool {
        self.amd_mixed_attachment_samples
    }
    pub fn enable_amd_mixed_attachment_samples(&mut self) {
        self.amd_mixed_attachment_samples = true;
    }
    pub fn supports_amd_shader_fragment_mask(&self) -> bool {
        self.amd_shader_fragment_mask
    }
    pub fn enable_amd_shader_fragment_mask(&mut self) {
        self.amd_shader_fragment_mask = true;
    }
    pub fn supports_ext_inline_uniform_block(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.ext_inline_uniform_block
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance1()))
    }
    pub fn enable_ext_inline_uniform_block(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_inline_uniform_block = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_maintenance1();
            }
        }
    }
    pub fn supports_ext_shader_stencil_export(&self) -> bool {
        self.ext_shader_stencil_export
    }
    pub fn enable_ext_shader_stencil_export(&mut self) {
        self.ext_shader_stencil_export = true;
    }
    pub fn supports_khr_shader_bfloat16(&self) -> bool {
        self.khr_shader_bfloat16
    }
    pub fn enable_khr_shader_bfloat16(&mut self) {
        self.khr_shader_bfloat16 = true;
    }
    pub fn supports_ext_sample_locations(&self) -> bool {
        self.ext_sample_locations
    }
    pub fn enable_ext_sample_locations(&mut self) {
        self.ext_sample_locations = true;
    }
    pub fn supports_khr_relaxed_block_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_relaxed_block_layout
    }
    pub fn enable_khr_relaxed_block_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_relaxed_block_layout = true;
        }
    }
    pub fn supports_khr_get_memory_requirements2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_get_memory_requirements2
    }
    pub fn enable_khr_get_memory_requirements2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_get_memory_requirements2 = true;
        }
    }
    pub fn supports_khr_image_format_list(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_image_format_list
    }
    pub fn enable_khr_image_format_list(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_image_format_list = true;
        }
    }
    pub fn supports_ext_blend_operation_advanced(&self) -> bool {
        self.ext_blend_operation_advanced
    }
    pub fn enable_ext_blend_operation_advanced(&mut self) {
        self.ext_blend_operation_advanced = true;
    }
    pub fn supports_nv_fragment_coverage_to_color(&self) -> bool {
        self.nv_fragment_coverage_to_color
    }
    pub fn enable_nv_fragment_coverage_to_color(&mut self) {
        self.nv_fragment_coverage_to_color = true;
    }
    pub fn supports_khr_acceleration_structure(&self) -> bool {
        self.khr_acceleration_structure
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    && self.supports_ext_descriptor_indexing()
                    && self.supports_khr_buffer_device_address()))
            && self.supports_khr_deferred_host_operations()
    }
    pub fn enable_khr_acceleration_structure(&mut self) {
        self.khr_acceleration_structure = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_ext_descriptor_indexing();
            self.enable_khr_buffer_device_address();
        }
        self.enable_khr_deferred_host_operations();
    }
    pub fn supports_khr_ray_tracing_pipeline(&self) -> bool {
        self.khr_ray_tracing_pipeline
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4())
            && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_pipeline(&mut self) {
        self.khr_ray_tracing_pipeline = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_khr_ray_query(&self) -> bool {
        self.khr_ray_query
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4())
            && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_query(&mut self) {
        self.khr_ray_query = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_nv_framebuffer_mixed_samples(&self) -> bool {
        self.nv_framebuffer_mixed_samples
    }
    pub fn enable_nv_framebuffer_mixed_samples(&mut self) {
        self.nv_framebuffer_mixed_samples = true;
    }
    pub fn supports_nv_fill_rectangle(&self) -> bool {
        self.nv_fill_rectangle
    }
    pub fn enable_nv_fill_rectangle(&mut self) {
        self.nv_fill_rectangle = true;
    }
    pub fn supports_nv_shader_sm_builtins(&self) -> bool {
        self.nv_shader_sm_builtins && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_nv_shader_sm_builtins(&mut self) {
        self.nv_shader_sm_builtins = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_ext_post_depth_coverage(&self) -> bool {
        self.ext_post_depth_coverage
    }
    pub fn enable_ext_post_depth_coverage(&mut self) {
        self.ext_post_depth_coverage = true;
    }
    pub fn supports_khr_sampler_ycbcr_conversion(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            || (self.khr_sampler_ycbcr_conversion
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || (self.supports_khr_maintenance1()
                        && self.supports_khr_bind_memory2()
                        && self.supports_khr_get_memory_requirements2())))
    }
    pub fn enable_khr_sampler_ycbcr_conversion(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_sampler_ycbcr_conversion = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_maintenance1();
                self.enable_khr_bind_memory2();
                self.enable_khr_get_memory_requirements2();
            }
        }
    }
    pub fn supports_khr_bind_memory2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_bind_memory2
    }
    pub fn enable_khr_bind_memory2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_bind_memory2 = true;
        }
    }
    pub fn supports_ext_image_drm_format_modifier(&self) -> bool {
        self.ext_image_drm_format_modifier
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || ((self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || (self.supports_khr_bind_memory2() && self.supports_khr_sampler_ycbcr_conversion()))
                    && self.supports_khr_image_format_list()))
    }
    pub fn enable_ext_image_drm_format_modifier(&mut self) {
        self.ext_image_drm_format_modifier = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_bind_memory2();
                self.enable_khr_sampler_ycbcr_conversion();
            }
            self.enable_khr_image_format_list();
        }
    }
    pub fn supports_ext_validation_cache(&self) -> bool {
        self.ext_validation_cache
    }
    pub fn enable_ext_validation_cache(&mut self) {
        self.ext_validation_cache = true;
    }
    pub fn supports_ext_descriptor_indexing(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.ext_descriptor_indexing
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance3()))
    }
    pub fn enable_ext_descriptor_indexing(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_descriptor_indexing = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_maintenance3();
            }
        }
    }
    pub fn supports_ext_shader_viewport_index_layer(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.ext_shader_viewport_index_layer
    }
    pub fn enable_ext_shader_viewport_index_layer(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_shader_viewport_index_layer = true;
        }
    }
    pub fn supports_khr_portability_subset(&self) -> bool {
        self.khr_portability_subset
    }
    pub fn enable_khr_portability_subset(&mut self) {
        self.khr_portability_subset = true;
    }
    pub fn supports_nv_shading_rate_image(&self) -> bool {
        self.nv_shading_rate_image
    }
    pub fn enable_nv_shading_rate_image(&mut self) {
        self.nv_shading_rate_image = true;
    }
    pub fn supports_nv_ray_tracing(&self) -> bool {
        self.nv_ray_tracing
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_memory_requirements2())
    }
    pub fn enable_nv_ray_tracing(&mut self) {
        self.nv_ray_tracing = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_memory_requirements2();
        }
    }
    pub fn supports_nv_representative_fragment_test(&self) -> bool {
        self.nv_representative_fragment_test
    }
    pub fn enable_nv_representative_fragment_test(&mut self) {
        self.nv_representative_fragment_test = true;
    }
    pub fn supports_khr_maintenance3(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.khr_maintenance3
    }
    pub fn enable_khr_maintenance3(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.khr_maintenance3 = true;
        }
    }
    pub fn supports_khr_draw_indirect_count(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_draw_indirect_count
    }
    pub fn enable_khr_draw_indirect_count(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_draw_indirect_count = true;
        }
    }
    pub fn supports_ext_filter_cubic(&self) -> bool {
        self.ext_filter_cubic
    }
    pub fn enable_ext_filter_cubic(&mut self) {
        self.ext_filter_cubic = true;
    }
    pub fn supports_qcom_render_pass_shader_resolve(&self) -> bool {
        self.qcom_render_pass_shader_resolve
    }
    pub fn enable_qcom_render_pass_shader_resolve(&mut self) {
        self.qcom_render_pass_shader_resolve = true;
    }
    pub fn supports_qcom_cooperative_matrix_conversion(&self) -> bool {
        self.qcom_cooperative_matrix_conversion && self.supports_khr_cooperative_matrix()
    }
    pub fn enable_qcom_cooperative_matrix_conversion(&mut self) {
        self.qcom_cooperative_matrix_conversion = true;
        self.enable_khr_cooperative_matrix();
    }
    pub fn supports_ext_global_priority(&self) -> bool {
        self.ext_global_priority
    }
    pub fn enable_ext_global_priority(&mut self) {
        self.ext_global_priority = true;
    }
    pub fn supports_khr_shader_subgroup_extended_types(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_shader_subgroup_extended_types && self.core_version >= vk::Version::from_raw_parts(1, 1, 0))
    }
    pub fn enable_khr_shader_subgroup_extended_types(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_shader_subgroup_extended_types = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
        }
    }
    pub fn supports_khr_8bit_storage(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_8bit_storage
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_storage_buffer_storage_class()))
    }
    pub fn enable_khr_8bit_storage(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_8bit_storage = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_storage_buffer_storage_class();
            }
        }
    }
    pub fn supports_ext_external_memory_host(&self) -> bool {
        self.ext_external_memory_host
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_ext_external_memory_host(&mut self) {
        self.ext_external_memory_host = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_amd_buffer_marker(&self) -> bool {
        self.amd_buffer_marker
    }
    pub fn enable_amd_buffer_marker(&mut self) {
        self.amd_buffer_marker = true;
    }
    pub fn supports_khr_shader_atomic_int64(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_shader_atomic_int64
    }
    pub fn enable_khr_shader_atomic_int64(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_shader_atomic_int64 = true;
        }
    }
    pub fn supports_khr_shader_clock(&self) -> bool {
        self.khr_shader_clock
    }
    pub fn enable_khr_shader_clock(&mut self) {
        self.khr_shader_clock = true;
    }
    pub fn supports_amd_pipeline_compiler_control(&self) -> bool {
        self.amd_pipeline_compiler_control
    }
    pub fn enable_amd_pipeline_compiler_control(&mut self) {
        self.amd_pipeline_compiler_control = true;
    }
    pub fn supports_ext_calibrated_timestamps(&self) -> bool {
        self.ext_calibrated_timestamps
    }
    pub fn enable_ext_calibrated_timestamps(&mut self) {
        self.ext_calibrated_timestamps = true;
    }
    pub fn supports_amd_shader_core_properties(&self) -> bool {
        self.amd_shader_core_properties
    }
    pub fn enable_amd_shader_core_properties(&mut self) {
        self.amd_shader_core_properties = true;
    }
    pub fn supports_khr_global_priority(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_global_priority
    }
    pub fn enable_khr_global_priority(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_global_priority = true;
        }
    }
    pub fn supports_amd_memory_overallocation_behavior(&self) -> bool {
        self.amd_memory_overallocation_behavior
    }
    pub fn enable_amd_memory_overallocation_behavior(&mut self) {
        self.amd_memory_overallocation_behavior = true;
    }
    pub fn supports_ext_vertex_attribute_divisor(&self) -> bool {
        self.ext_vertex_attribute_divisor
    }
    pub fn enable_ext_vertex_attribute_divisor(&mut self) {
        self.ext_vertex_attribute_divisor = true;
    }
    pub fn supports_ext_pipeline_creation_feedback(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_pipeline_creation_feedback
    }
    pub fn enable_ext_pipeline_creation_feedback(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_pipeline_creation_feedback = true;
        }
    }
    pub fn supports_khr_driver_properties(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_driver_properties
    }
    pub fn enable_khr_driver_properties(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_driver_properties = true;
        }
    }
    pub fn supports_khr_shader_float_controls(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_shader_float_controls
    }
    pub fn enable_khr_shader_float_controls(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_shader_float_controls = true;
        }
    }
    pub fn supports_nv_shader_subgroup_partitioned(&self) -> bool {
        self.nv_shader_subgroup_partitioned && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_nv_shader_subgroup_partitioned(&mut self) {
        self.nv_shader_subgroup_partitioned = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_khr_depth_stencil_resolve(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_depth_stencil_resolve
                && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || self.supports_khr_create_renderpass2()))
    }
    pub fn enable_khr_depth_stencil_resolve(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_depth_stencil_resolve = true;
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_create_renderpass2();
            }
        }
    }
    pub fn supports_khr_swapchain_mutable_format(&self) -> bool {
        self.khr_swapchain_mutable_format
            && self.supports_khr_swapchain()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance2())
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_image_format_list())
    }
    pub fn enable_khr_swapchain_mutable_format(&mut self) {
        self.khr_swapchain_mutable_format = true;
        self.enable_khr_swapchain();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_maintenance2();
        }
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_image_format_list();
        }
    }
    pub fn supports_nv_compute_shader_derivatives(&self) -> bool {
        self.nv_compute_shader_derivatives
    }
    pub fn enable_nv_compute_shader_derivatives(&mut self) {
        self.nv_compute_shader_derivatives = true;
    }
    pub fn supports_nv_mesh_shader(&self) -> bool {
        self.nv_mesh_shader
    }
    pub fn enable_nv_mesh_shader(&mut self) {
        self.nv_mesh_shader = true;
    }
    pub fn supports_nv_fragment_shader_barycentric(&self) -> bool {
        self.nv_fragment_shader_barycentric
    }
    pub fn enable_nv_fragment_shader_barycentric(&mut self) {
        self.nv_fragment_shader_barycentric = true;
    }
    pub fn supports_nv_shader_image_footprint(&self) -> bool {
        self.nv_shader_image_footprint
    }
    pub fn enable_nv_shader_image_footprint(&mut self) {
        self.nv_shader_image_footprint = true;
    }
    pub fn supports_nv_scissor_exclusive(&self) -> bool {
        self.nv_scissor_exclusive
    }
    pub fn enable_nv_scissor_exclusive(&mut self) {
        self.nv_scissor_exclusive = true;
    }
    pub fn supports_nv_device_diagnostic_checkpoints(&self) -> bool {
        self.nv_device_diagnostic_checkpoints
    }
    pub fn enable_nv_device_diagnostic_checkpoints(&mut self) {
        self.nv_device_diagnostic_checkpoints = true;
    }
    pub fn supports_khr_timeline_semaphore(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_timeline_semaphore
    }
    pub fn enable_khr_timeline_semaphore(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_timeline_semaphore = true;
        }
    }
    pub fn supports_ext_present_timing(&self) -> bool {
        self.ext_present_timing
            && self.supports_khr_swapchain()
            && self.supports_khr_present_id2()
            && self.supports_khr_calibrated_timestamps()
    }
    pub fn enable_ext_present_timing(&mut self) {
        self.ext_present_timing = true;
        self.enable_khr_swapchain();
        self.enable_khr_present_id2();
        self.enable_khr_calibrated_timestamps();
    }
    pub fn supports_intel_shader_integer_functions2(&self) -> bool {
        self.intel_shader_integer_functions2
    }
    pub fn enable_intel_shader_integer_functions2(&mut self) {
        self.intel_shader_integer_functions2 = true;
    }
    pub fn supports_intel_performance_query(&self) -> bool {
        self.intel_performance_query
    }
    pub fn enable_intel_performance_query(&mut self) {
        self.intel_performance_query = true;
    }
    pub fn supports_khr_vulkan_memory_model(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_vulkan_memory_model
    }
    pub fn enable_khr_vulkan_memory_model(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_vulkan_memory_model = true;
        }
    }
    pub fn supports_ext_pci_bus_info(&self) -> bool {
        self.ext_pci_bus_info
    }
    pub fn enable_ext_pci_bus_info(&mut self) {
        self.ext_pci_bus_info = true;
    }
    pub fn supports_amd_display_native_hdr(&self) -> bool {
        self.amd_display_native_hdr && self.supports_khr_swapchain()
    }
    pub fn enable_amd_display_native_hdr(&mut self) {
        self.amd_display_native_hdr = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_shader_terminate_invocation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_shader_terminate_invocation
    }
    pub fn enable_khr_shader_terminate_invocation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_shader_terminate_invocation = true;
        }
    }
    pub fn supports_ext_fragment_density_map(&self) -> bool {
        self.ext_fragment_density_map
    }
    pub fn enable_ext_fragment_density_map(&mut self) {
        self.ext_fragment_density_map = true;
    }
    pub fn supports_ext_scalar_block_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.ext_scalar_block_layout
    }
    pub fn enable_ext_scalar_block_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_scalar_block_layout = true;
        }
    }
    pub fn supports_google_hlsl_functionality1(&self) -> bool {
        self.google_hlsl_functionality1
    }
    pub fn enable_google_hlsl_functionality1(&mut self) {
        self.google_hlsl_functionality1 = true;
    }
    pub fn supports_google_decorate_string(&self) -> bool {
        self.google_decorate_string
    }
    pub fn enable_google_decorate_string(&mut self) {
        self.google_decorate_string = true;
    }
    pub fn supports_ext_subgroup_size_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.ext_subgroup_size_control && self.core_version >= vk::Version::from_raw_parts(1, 1, 0))
    }
    pub fn enable_ext_subgroup_size_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_subgroup_size_control = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
        }
    }
    pub fn supports_khr_fragment_shading_rate(&self) -> bool {
        self.khr_fragment_shading_rate
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_create_renderpass2())
    }
    pub fn enable_khr_fragment_shading_rate(&mut self) {
        self.khr_fragment_shading_rate = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
        }
    }
    pub fn supports_amd_shader_core_properties2(&self) -> bool {
        self.amd_shader_core_properties2 && self.supports_amd_shader_core_properties()
    }
    pub fn enable_amd_shader_core_properties2(&mut self) {
        self.amd_shader_core_properties2 = true;
        self.enable_amd_shader_core_properties();
    }
    pub fn supports_amd_device_coherent_memory(&self) -> bool {
        self.amd_device_coherent_memory
    }
    pub fn enable_amd_device_coherent_memory(&mut self) {
        self.amd_device_coherent_memory = true;
    }
    pub fn supports_khr_dynamic_rendering_local_read(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.khr_dynamic_rendering_local_read
                && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering()))
    }
    pub fn enable_khr_dynamic_rendering_local_read(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_dynamic_rendering_local_read = true;
            if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
                self.enable_khr_dynamic_rendering();
            }
        }
    }
    pub fn supports_ext_shader_image_atomic_int64(&self) -> bool {
        self.ext_shader_image_atomic_int64
    }
    pub fn enable_ext_shader_image_atomic_int64(&mut self) {
        self.ext_shader_image_atomic_int64 = true;
    }
    pub fn supports_khr_shader_quad_control(&self) -> bool {
        self.khr_shader_quad_control
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    && self.supports_khr_vulkan_memory_model()))
            && self.supports_khr_shader_maximal_reconvergence()
    }
    pub fn enable_khr_shader_quad_control(&mut self) {
        self.khr_shader_quad_control = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_vulkan_memory_model();
        }
        self.enable_khr_shader_maximal_reconvergence();
    }
    pub fn supports_khr_spirv_1_4(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_spirv_1_4
                && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                && self.supports_khr_shader_float_controls())
    }
    pub fn enable_khr_spirv_1_4(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_spirv_1_4 = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_shader_float_controls();
        }
    }
    pub fn supports_ext_memory_budget(&self) -> bool {
        self.ext_memory_budget
    }
    pub fn enable_ext_memory_budget(&mut self) {
        self.ext_memory_budget = true;
    }
    pub fn supports_ext_memory_priority(&self) -> bool {
        self.ext_memory_priority
    }
    pub fn enable_ext_memory_priority(&mut self) {
        self.ext_memory_priority = true;
    }
    pub fn supports_nv_dedicated_allocation_image_aliasing(&self) -> bool {
        self.nv_dedicated_allocation_image_aliasing
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_dedicated_allocation())
    }
    pub fn enable_nv_dedicated_allocation_image_aliasing(&mut self) {
        self.nv_dedicated_allocation_image_aliasing = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_dedicated_allocation();
        }
    }
    pub fn supports_khr_separate_depth_stencil_layouts(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_separate_depth_stencil_layouts
                && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || self.supports_khr_create_renderpass2()))
    }
    pub fn enable_khr_separate_depth_stencil_layouts(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_separate_depth_stencil_layouts = true;
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_create_renderpass2();
            }
        }
    }
    pub fn supports_ext_buffer_device_address(&self) -> bool {
        self.ext_buffer_device_address
    }
    pub fn enable_ext_buffer_device_address(&mut self) {
        self.ext_buffer_device_address = true;
    }
    pub fn supports_ext_tooling_info(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_tooling_info
    }
    pub fn enable_ext_tooling_info(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_tooling_info = true;
        }
    }
    pub fn supports_ext_separate_stencil_usage(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.ext_separate_stencil_usage
    }
    pub fn enable_ext_separate_stencil_usage(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_separate_stencil_usage = true;
        }
    }
    pub fn supports_khr_present_wait(&self) -> bool {
        self.khr_present_wait && self.supports_khr_swapchain() && self.supports_khr_present_id()
    }
    pub fn enable_khr_present_wait(&mut self) {
        self.khr_present_wait = true;
        self.enable_khr_swapchain();
        self.enable_khr_present_id();
    }
    pub fn supports_nv_cooperative_matrix(&self) -> bool {
        self.nv_cooperative_matrix
    }
    pub fn enable_nv_cooperative_matrix(&mut self) {
        self.nv_cooperative_matrix = true;
    }
    pub fn supports_nv_coverage_reduction_mode(&self) -> bool {
        self.nv_coverage_reduction_mode && self.supports_nv_framebuffer_mixed_samples()
    }
    pub fn enable_nv_coverage_reduction_mode(&mut self) {
        self.nv_coverage_reduction_mode = true;
        self.enable_nv_framebuffer_mixed_samples();
    }
    pub fn supports_ext_fragment_shader_interlock(&self) -> bool {
        self.ext_fragment_shader_interlock
    }
    pub fn enable_ext_fragment_shader_interlock(&mut self) {
        self.ext_fragment_shader_interlock = true;
    }
    pub fn supports_ext_ycbcr_image_arrays(&self) -> bool {
        self.ext_ycbcr_image_arrays
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_sampler_ycbcr_conversion())
    }
    pub fn enable_ext_ycbcr_image_arrays(&mut self) {
        self.ext_ycbcr_image_arrays = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_khr_uniform_buffer_standard_layout(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.khr_uniform_buffer_standard_layout
    }
    pub fn enable_khr_uniform_buffer_standard_layout(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_uniform_buffer_standard_layout = true;
        }
    }
    pub fn supports_ext_provoking_vertex(&self) -> bool {
        self.ext_provoking_vertex
    }
    pub fn enable_ext_provoking_vertex(&mut self) {
        self.ext_provoking_vertex = true;
    }
    pub fn supports_ext_full_screen_exclusive(&self) -> bool {
        self.ext_full_screen_exclusive && self.supports_khr_swapchain()
    }
    pub fn enable_ext_full_screen_exclusive(&mut self) {
        self.ext_full_screen_exclusive = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_buffer_device_address(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
            || (self.khr_buffer_device_address
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_device_group()))
    }
    pub fn enable_khr_buffer_device_address(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.khr_buffer_device_address = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_device_group();
            }
        }
    }
    pub fn supports_ext_line_rasterization(&self) -> bool {
        self.ext_line_rasterization
    }
    pub fn enable_ext_line_rasterization(&mut self) {
        self.ext_line_rasterization = true;
    }
    pub fn supports_ext_shader_atomic_float(&self) -> bool {
        self.ext_shader_atomic_float
    }
    pub fn enable_ext_shader_atomic_float(&mut self) {
        self.ext_shader_atomic_float = true;
    }
    pub fn supports_ext_host_query_reset(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.ext_host_query_reset
    }
    pub fn enable_ext_host_query_reset(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.ext_host_query_reset = true;
        }
    }
    pub fn supports_ext_index_type_uint8(&self) -> bool {
        self.ext_index_type_uint8
    }
    pub fn enable_ext_index_type_uint8(&mut self) {
        self.ext_index_type_uint8 = true;
    }
    pub fn supports_ext_extended_dynamic_state(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_extended_dynamic_state
    }
    pub fn enable_ext_extended_dynamic_state(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_extended_dynamic_state = true;
        }
    }
    pub fn supports_khr_deferred_host_operations(&self) -> bool {
        self.khr_deferred_host_operations
    }
    pub fn enable_khr_deferred_host_operations(&mut self) {
        self.khr_deferred_host_operations = true;
    }
    pub fn supports_khr_pipeline_executable_properties(&self) -> bool {
        self.khr_pipeline_executable_properties
    }
    pub fn enable_khr_pipeline_executable_properties(&mut self) {
        self.khr_pipeline_executable_properties = true;
    }
    pub fn supports_ext_host_image_copy(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.ext_host_image_copy
                && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                    || (self.supports_khr_copy_commands2() && self.supports_khr_format_feature_flags2())))
    }
    pub fn enable_ext_host_image_copy(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.ext_host_image_copy = true;
            if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
                self.enable_khr_copy_commands2();
                self.enable_khr_format_feature_flags2();
            }
        }
    }
    pub fn supports_khr_map_memory2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_map_memory2
    }
    pub fn enable_khr_map_memory2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_map_memory2 = true;
        }
    }
    pub fn supports_ext_map_memory_placed(&self) -> bool {
        self.ext_map_memory_placed
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_map_memory2())
    }
    pub fn enable_ext_map_memory_placed(&mut self) {
        self.ext_map_memory_placed = true;
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_map_memory2();
        }
    }
    pub fn supports_ext_shader_atomic_float2(&self) -> bool {
        self.ext_shader_atomic_float2 && self.supports_ext_shader_atomic_float()
    }
    pub fn enable_ext_shader_atomic_float2(&mut self) {
        self.ext_shader_atomic_float2 = true;
        self.enable_ext_shader_atomic_float();
    }
    pub fn supports_ext_swapchain_maintenance1(&self) -> bool {
        self.ext_swapchain_maintenance1 && self.supports_khr_swapchain()
    }
    pub fn enable_ext_swapchain_maintenance1(&mut self) {
        self.ext_swapchain_maintenance1 = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_shader_demote_to_helper_invocation(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_shader_demote_to_helper_invocation
    }
    pub fn enable_ext_shader_demote_to_helper_invocation(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_shader_demote_to_helper_invocation = true;
        }
    }
    pub fn supports_nv_device_generated_commands(&self) -> bool {
        self.nv_device_generated_commands
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    && self.supports_khr_buffer_device_address()))
    }
    pub fn enable_nv_device_generated_commands(&mut self) {
        self.nv_device_generated_commands = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_inherited_viewport_scissor(&self) -> bool {
        self.nv_inherited_viewport_scissor
    }
    pub fn enable_nv_inherited_viewport_scissor(&mut self) {
        self.nv_inherited_viewport_scissor = true;
    }
    pub fn supports_khr_shader_integer_dot_product(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_shader_integer_dot_product
    }
    pub fn enable_khr_shader_integer_dot_product(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_shader_integer_dot_product = true;
        }
    }
    pub fn supports_ext_texel_buffer_alignment(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_texel_buffer_alignment
    }
    pub fn enable_ext_texel_buffer_alignment(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_texel_buffer_alignment = true;
        }
    }
    pub fn supports_qcom_render_pass_transform(&self) -> bool {
        self.qcom_render_pass_transform
    }
    pub fn enable_qcom_render_pass_transform(&mut self) {
        self.qcom_render_pass_transform = true;
    }
    pub fn supports_ext_depth_bias_control(&self) -> bool {
        self.ext_depth_bias_control
    }
    pub fn enable_ext_depth_bias_control(&mut self) {
        self.ext_depth_bias_control = true;
    }
    pub fn supports_ext_device_memory_report(&self) -> bool {
        self.ext_device_memory_report
    }
    pub fn enable_ext_device_memory_report(&mut self) {
        self.ext_device_memory_report = true;
    }
    pub fn supports_ext_robustness2(&self) -> bool {
        self.ext_robustness2
    }
    pub fn enable_ext_robustness2(&mut self) {
        self.ext_robustness2 = true;
    }
    pub fn supports_ext_custom_border_color(&self) -> bool {
        self.ext_custom_border_color
    }
    pub fn enable_ext_custom_border_color(&mut self) {
        self.ext_custom_border_color = true;
    }
    pub fn supports_ext_texture_compression_astc_3d(&self) -> bool {
        self.ext_texture_compression_astc_3d
    }
    pub fn enable_ext_texture_compression_astc_3d(&mut self) {
        self.ext_texture_compression_astc_3d = true;
    }
    pub fn supports_google_user_type(&self) -> bool {
        self.google_user_type
    }
    pub fn enable_google_user_type(&mut self) {
        self.google_user_type = true;
    }
    pub fn supports_khr_pipeline_library(&self) -> bool {
        self.khr_pipeline_library
    }
    pub fn enable_khr_pipeline_library(&mut self) {
        self.khr_pipeline_library = true;
    }
    pub fn supports_nv_present_barrier(&self) -> bool {
        self.nv_present_barrier && self.supports_khr_swapchain()
    }
    pub fn enable_nv_present_barrier(&mut self) {
        self.nv_present_barrier = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_shader_non_semantic_info(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_shader_non_semantic_info
    }
    pub fn enable_khr_shader_non_semantic_info(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_shader_non_semantic_info = true;
        }
    }
    pub fn supports_khr_present_id(&self) -> bool {
        self.khr_present_id && self.supports_khr_swapchain()
    }
    pub fn enable_khr_present_id(&mut self) {
        self.khr_present_id = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_private_data(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_private_data
    }
    pub fn enable_ext_private_data(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_private_data = true;
        }
    }
    pub fn supports_ext_pipeline_creation_cache_control(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_pipeline_creation_cache_control
    }
    pub fn enable_ext_pipeline_creation_cache_control(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_pipeline_creation_cache_control = true;
        }
    }
    pub fn supports_nv_device_diagnostics_config(&self) -> bool {
        self.nv_device_diagnostics_config
    }
    pub fn enable_nv_device_diagnostics_config(&mut self) {
        self.nv_device_diagnostics_config = true;
    }
    pub fn supports_qcom_render_pass_store_ops(&self) -> bool {
        self.qcom_render_pass_store_ops
    }
    pub fn enable_qcom_render_pass_store_ops(&mut self) {
        self.qcom_render_pass_store_ops = true;
    }
    pub fn supports_nv_cuda_kernel_launch(&self) -> bool {
        self.nv_cuda_kernel_launch
    }
    pub fn enable_nv_cuda_kernel_launch(&mut self) {
        self.nv_cuda_kernel_launch = true;
    }
    pub fn supports_qcom_tile_shading(&self) -> bool {
        self.qcom_tile_shading && self.supports_qcom_tile_properties()
    }
    pub fn enable_qcom_tile_shading(&mut self) {
        self.qcom_tile_shading = true;
        self.enable_qcom_tile_properties();
    }
    pub fn supports_nv_low_latency(&self) -> bool {
        self.nv_low_latency
    }
    pub fn enable_nv_low_latency(&mut self) {
        self.nv_low_latency = true;
    }
    pub fn supports_ext_metal_objects(&self) -> bool {
        self.ext_metal_objects
    }
    pub fn enable_ext_metal_objects(&mut self) {
        self.ext_metal_objects = true;
    }
    pub fn supports_khr_synchronization2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_synchronization2
    }
    pub fn enable_khr_synchronization2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_synchronization2 = true;
        }
    }
    pub fn supports_ext_descriptor_buffer(&self) -> bool {
        self.ext_descriptor_buffer
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || (self.supports_khr_buffer_device_address() && self.supports_ext_descriptor_indexing()))
                    && self.supports_khr_synchronization2()))
    }
    pub fn enable_ext_descriptor_buffer(&mut self) {
        self.ext_descriptor_buffer = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_buffer_device_address();
                self.enable_ext_descriptor_indexing();
            }
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_ext_graphics_pipeline_library(&self) -> bool {
        self.ext_graphics_pipeline_library && self.supports_khr_pipeline_library()
    }
    pub fn enable_ext_graphics_pipeline_library(&mut self) {
        self.ext_graphics_pipeline_library = true;
        self.enable_khr_pipeline_library();
    }
    pub fn supports_amd_shader_early_and_late_fragment_tests(&self) -> bool {
        self.amd_shader_early_and_late_fragment_tests
    }
    pub fn enable_amd_shader_early_and_late_fragment_tests(&mut self) {
        self.amd_shader_early_and_late_fragment_tests = true;
    }
    pub fn supports_khr_fragment_shader_barycentric(&self) -> bool {
        self.khr_fragment_shader_barycentric
    }
    pub fn enable_khr_fragment_shader_barycentric(&mut self) {
        self.khr_fragment_shader_barycentric = true;
    }
    pub fn supports_khr_shader_subgroup_uniform_control_flow(&self) -> bool {
        self.khr_shader_subgroup_uniform_control_flow && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_khr_shader_subgroup_uniform_control_flow(&mut self) {
        self.khr_shader_subgroup_uniform_control_flow = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_khr_zero_initialize_workgroup_memory(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_zero_initialize_workgroup_memory
    }
    pub fn enable_khr_zero_initialize_workgroup_memory(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_zero_initialize_workgroup_memory = true;
        }
    }
    pub fn supports_nv_fragment_shading_rate_enums(&self) -> bool {
        self.nv_fragment_shading_rate_enums && self.supports_khr_fragment_shading_rate()
    }
    pub fn enable_nv_fragment_shading_rate_enums(&mut self) {
        self.nv_fragment_shading_rate_enums = true;
        self.enable_khr_fragment_shading_rate();
    }
    pub fn supports_nv_ray_tracing_motion_blur(&self) -> bool {
        self.nv_ray_tracing_motion_blur && self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_motion_blur(&mut self) {
        self.nv_ray_tracing_motion_blur = true;
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_ext_mesh_shader(&self) -> bool {
        self.ext_mesh_shader
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_spirv_1_4())
    }
    pub fn enable_ext_mesh_shader(&mut self) {
        self.ext_mesh_shader = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_spirv_1_4();
        }
    }
    pub fn supports_ext_ycbcr_2plane_444_formats(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.ext_ycbcr_2plane_444_formats
                && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || self.supports_khr_sampler_ycbcr_conversion()))
    }
    pub fn enable_ext_ycbcr_2plane_444_formats(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_ycbcr_2plane_444_formats = true;
            if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
                self.enable_khr_sampler_ycbcr_conversion();
            }
        }
    }
    pub fn supports_ext_fragment_density_map2(&self) -> bool {
        self.ext_fragment_density_map2 && self.supports_ext_fragment_density_map()
    }
    pub fn enable_ext_fragment_density_map2(&mut self) {
        self.ext_fragment_density_map2 = true;
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_qcom_rotated_copy_commands(&self) -> bool {
        self.qcom_rotated_copy_commands
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_copy_commands2())
    }
    pub fn enable_qcom_rotated_copy_commands(&mut self) {
        self.qcom_rotated_copy_commands = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_copy_commands2();
        }
    }
    pub fn supports_ext_image_robustness(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_image_robustness
    }
    pub fn enable_ext_image_robustness(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_image_robustness = true;
        }
    }
    pub fn supports_khr_workgroup_memory_explicit_layout(&self) -> bool {
        self.khr_workgroup_memory_explicit_layout
    }
    pub fn enable_khr_workgroup_memory_explicit_layout(&mut self) {
        self.khr_workgroup_memory_explicit_layout = true;
    }
    pub fn supports_khr_copy_commands2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_copy_commands2
    }
    pub fn enable_khr_copy_commands2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_copy_commands2 = true;
        }
    }
    pub fn supports_ext_image_compression_control(&self) -> bool {
        self.ext_image_compression_control
    }
    pub fn enable_ext_image_compression_control(&mut self) {
        self.ext_image_compression_control = true;
    }
    pub fn supports_ext_attachment_feedback_loop_layout(&self) -> bool {
        self.ext_attachment_feedback_loop_layout
    }
    pub fn enable_ext_attachment_feedback_loop_layout(&mut self) {
        self.ext_attachment_feedback_loop_layout = true;
    }
    pub fn supports_ext_4444_formats(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_4444_formats
    }
    pub fn enable_ext_4444_formats(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_4444_formats = true;
        }
    }
    pub fn supports_ext_device_fault(&self) -> bool {
        self.ext_device_fault
    }
    pub fn enable_ext_device_fault(&mut self) {
        self.ext_device_fault = true;
    }
    pub fn supports_arm_rasterization_order_attachment_access(&self) -> bool {
        self.arm_rasterization_order_attachment_access
    }
    pub fn enable_arm_rasterization_order_attachment_access(&mut self) {
        self.arm_rasterization_order_attachment_access = true;
    }
    pub fn supports_ext_rgba10x6_formats(&self) -> bool {
        self.ext_rgba10x6_formats
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_sampler_ycbcr_conversion())
    }
    pub fn enable_ext_rgba10x6_formats(&mut self) {
        self.ext_rgba10x6_formats = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_nv_acquire_winrt_display(&self) -> bool {
        self.nv_acquire_winrt_display
    }
    pub fn enable_nv_acquire_winrt_display(&mut self) {
        self.nv_acquire_winrt_display = true;
    }
    pub fn supports_valve_mutable_descriptor_type(&self) -> bool {
        self.valve_mutable_descriptor_type && self.supports_khr_maintenance3()
    }
    pub fn enable_valve_mutable_descriptor_type(&mut self) {
        self.valve_mutable_descriptor_type = true;
        self.enable_khr_maintenance3();
    }
    pub fn supports_ext_vertex_input_dynamic_state(&self) -> bool {
        self.ext_vertex_input_dynamic_state
    }
    pub fn enable_ext_vertex_input_dynamic_state(&mut self) {
        self.ext_vertex_input_dynamic_state = true;
    }
    pub fn supports_ext_physical_device_drm(&self) -> bool {
        self.ext_physical_device_drm
    }
    pub fn enable_ext_physical_device_drm(&mut self) {
        self.ext_physical_device_drm = true;
    }
    pub fn supports_ext_device_address_binding_report(&self) -> bool {
        self.ext_device_address_binding_report
    }
    pub fn enable_ext_device_address_binding_report(&mut self) {
        self.ext_device_address_binding_report = true;
    }
    pub fn supports_ext_depth_clip_control(&self) -> bool {
        self.ext_depth_clip_control
    }
    pub fn enable_ext_depth_clip_control(&mut self) {
        self.ext_depth_clip_control = true;
    }
    pub fn supports_ext_primitive_topology_list_restart(&self) -> bool {
        self.ext_primitive_topology_list_restart
    }
    pub fn enable_ext_primitive_topology_list_restart(&mut self) {
        self.ext_primitive_topology_list_restart = true;
    }
    pub fn supports_khr_format_feature_flags2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.khr_format_feature_flags2
    }
    pub fn enable_khr_format_feature_flags2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_format_feature_flags2 = true;
        }
    }
    pub fn supports_ext_present_mode_fifo_latest_ready(&self) -> bool {
        self.ext_present_mode_fifo_latest_ready && self.supports_khr_swapchain()
    }
    pub fn enable_ext_present_mode_fifo_latest_ready(&mut self) {
        self.ext_present_mode_fifo_latest_ready = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_fuchsia_external_memory(&self) -> bool {
        self.fuchsia_external_memory
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_fuchsia_external_memory(&mut self) {
        self.fuchsia_external_memory = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_fuchsia_external_semaphore(&self) -> bool {
        self.fuchsia_external_semaphore && self.supports_khr_external_semaphore()
    }
    pub fn enable_fuchsia_external_semaphore(&mut self) {
        self.fuchsia_external_semaphore = true;
        self.enable_khr_external_semaphore();
    }
    pub fn supports_fuchsia_buffer_collection(&self) -> bool {
        self.fuchsia_buffer_collection
            && self.supports_fuchsia_external_memory()
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_sampler_ycbcr_conversion())
    }
    pub fn enable_fuchsia_buffer_collection(&mut self) {
        self.fuchsia_buffer_collection = true;
        self.enable_fuchsia_external_memory();
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
        }
    }
    pub fn supports_huawei_subpass_shading(&self) -> bool {
        self.huawei_subpass_shading
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || self.supports_khr_create_renderpass2())
                    && self.supports_khr_synchronization2()))
    }
    pub fn enable_huawei_subpass_shading(&mut self) {
        self.huawei_subpass_shading = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_create_renderpass2();
            }
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_huawei_invocation_mask(&self) -> bool {
        self.huawei_invocation_mask
            && self.supports_khr_ray_tracing_pipeline()
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_synchronization2())
    }
    pub fn enable_huawei_invocation_mask(&mut self) {
        self.huawei_invocation_mask = true;
        self.enable_khr_ray_tracing_pipeline();
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_nv_external_memory_rdma(&self) -> bool {
        self.nv_external_memory_rdma
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_nv_external_memory_rdma(&mut self) {
        self.nv_external_memory_rdma = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_ext_pipeline_properties(&self) -> bool {
        self.ext_pipeline_properties
    }
    pub fn enable_ext_pipeline_properties(&mut self) {
        self.ext_pipeline_properties = true;
    }
    pub fn supports_ext_frame_boundary(&self) -> bool {
        self.ext_frame_boundary
    }
    pub fn enable_ext_frame_boundary(&mut self) {
        self.ext_frame_boundary = true;
    }
    pub fn supports_ext_multisampled_render_to_single_sampled(&self) -> bool {
        self.ext_multisampled_render_to_single_sampled
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                || (self.supports_khr_create_renderpass2() && self.supports_khr_depth_stencil_resolve()))
    }
    pub fn enable_ext_multisampled_render_to_single_sampled(&mut self) {
        self.ext_multisampled_render_to_single_sampled = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
            self.enable_khr_depth_stencil_resolve();
        }
    }
    pub fn supports_ext_extended_dynamic_state2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.ext_extended_dynamic_state2
    }
    pub fn enable_ext_extended_dynamic_state2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.ext_extended_dynamic_state2 = true;
        }
    }
    pub fn supports_ext_color_write_enable(&self) -> bool {
        self.ext_color_write_enable
    }
    pub fn enable_ext_color_write_enable(&mut self) {
        self.ext_color_write_enable = true;
    }
    pub fn supports_ext_primitives_generated_query(&self) -> bool {
        self.ext_primitives_generated_query && self.supports_ext_transform_feedback()
    }
    pub fn enable_ext_primitives_generated_query(&mut self) {
        self.ext_primitives_generated_query = true;
        self.enable_ext_transform_feedback();
    }
    pub fn supports_khr_ray_tracing_maintenance1(&self) -> bool {
        self.khr_ray_tracing_maintenance1 && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_maintenance1(&mut self) {
        self.khr_ray_tracing_maintenance1 = true;
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_khr_shader_untyped_pointers(&self) -> bool {
        self.khr_shader_untyped_pointers
    }
    pub fn enable_khr_shader_untyped_pointers(&mut self) {
        self.khr_shader_untyped_pointers = true;
    }
    pub fn supports_ext_global_priority_query(&self) -> bool {
        self.ext_global_priority_query && self.supports_ext_global_priority()
    }
    pub fn enable_ext_global_priority_query(&mut self) {
        self.ext_global_priority_query = true;
        self.enable_ext_global_priority();
    }
    pub fn supports_ext_image_view_min_lod(&self) -> bool {
        self.ext_image_view_min_lod
    }
    pub fn enable_ext_image_view_min_lod(&mut self) {
        self.ext_image_view_min_lod = true;
    }
    pub fn supports_ext_multi_draw(&self) -> bool {
        self.ext_multi_draw
    }
    pub fn enable_ext_multi_draw(&mut self) {
        self.ext_multi_draw = true;
    }
    pub fn supports_ext_image_2d_view_of_3d(&self) -> bool {
        self.ext_image_2d_view_of_3d
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance1())
    }
    pub fn enable_ext_image_2d_view_of_3d(&mut self) {
        self.ext_image_2d_view_of_3d = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_maintenance1();
        }
    }
    pub fn supports_ext_shader_tile_image(&self) -> bool {
        self.ext_shader_tile_image && self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
    }
    pub fn enable_ext_shader_tile_image(&mut self) {
        self.ext_shader_tile_image = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 3, 0));
    }
    pub fn supports_ext_opacity_micromap(&self) -> bool {
        self.ext_opacity_micromap
            && self.supports_khr_acceleration_structure()
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_synchronization2())
    }
    pub fn enable_ext_opacity_micromap(&mut self) {
        self.ext_opacity_micromap = true;
        self.enable_khr_acceleration_structure();
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_nv_displacement_micromap(&self) -> bool {
        self.nv_displacement_micromap && self.supports_ext_opacity_micromap()
    }
    pub fn enable_nv_displacement_micromap(&mut self) {
        self.nv_displacement_micromap = true;
        self.enable_ext_opacity_micromap();
    }
    pub fn supports_ext_load_store_op_none(&self) -> bool {
        self.ext_load_store_op_none
    }
    pub fn enable_ext_load_store_op_none(&mut self) {
        self.ext_load_store_op_none = true;
    }
    pub fn supports_huawei_cluster_culling_shader(&self) -> bool {
        self.huawei_cluster_culling_shader
    }
    pub fn enable_huawei_cluster_culling_shader(&mut self) {
        self.huawei_cluster_culling_shader = true;
    }
    pub fn supports_ext_border_color_swizzle(&self) -> bool {
        self.ext_border_color_swizzle && self.supports_ext_custom_border_color()
    }
    pub fn enable_ext_border_color_swizzle(&mut self) {
        self.ext_border_color_swizzle = true;
        self.enable_ext_custom_border_color();
    }
    pub fn supports_ext_pageable_device_local_memory(&self) -> bool {
        self.ext_pageable_device_local_memory && self.supports_ext_memory_priority()
    }
    pub fn enable_ext_pageable_device_local_memory(&mut self) {
        self.ext_pageable_device_local_memory = true;
        self.enable_ext_memory_priority();
    }
    pub fn supports_khr_maintenance4(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            || (self.khr_maintenance4 && self.core_version >= vk::Version::from_raw_parts(1, 1, 0))
    }
    pub fn enable_khr_maintenance4(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.khr_maintenance4 = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
        }
    }
    pub fn supports_arm_shader_core_properties(&self) -> bool {
        self.arm_shader_core_properties && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_arm_shader_core_properties(&mut self) {
        self.arm_shader_core_properties = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_khr_shader_subgroup_rotate(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_shader_subgroup_rotate
    }
    pub fn enable_khr_shader_subgroup_rotate(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_shader_subgroup_rotate = true;
        }
    }
    pub fn supports_arm_scheduling_controls(&self) -> bool {
        self.arm_scheduling_controls && self.supports_arm_shader_core_builtins()
    }
    pub fn enable_arm_scheduling_controls(&mut self) {
        self.arm_scheduling_controls = true;
        self.enable_arm_shader_core_builtins();
    }
    pub fn supports_ext_image_sliced_view_of_3d(&self) -> bool {
        self.ext_image_sliced_view_of_3d
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance1())
    }
    pub fn enable_ext_image_sliced_view_of_3d(&mut self) {
        self.ext_image_sliced_view_of_3d = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_maintenance1();
        }
    }
    pub fn supports_valve_descriptor_set_host_mapping(&self) -> bool {
        self.valve_descriptor_set_host_mapping
    }
    pub fn enable_valve_descriptor_set_host_mapping(&mut self) {
        self.valve_descriptor_set_host_mapping = true;
    }
    pub fn supports_ext_depth_clamp_zero_one(&self) -> bool {
        self.ext_depth_clamp_zero_one
    }
    pub fn enable_ext_depth_clamp_zero_one(&mut self) {
        self.ext_depth_clamp_zero_one = true;
    }
    pub fn supports_ext_non_seamless_cube_map(&self) -> bool {
        self.ext_non_seamless_cube_map
    }
    pub fn enable_ext_non_seamless_cube_map(&mut self) {
        self.ext_non_seamless_cube_map = true;
    }
    pub fn supports_arm_render_pass_striped(&self) -> bool {
        self.arm_render_pass_striped
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_synchronization2())
    }
    pub fn enable_arm_render_pass_striped(&mut self) {
        self.arm_render_pass_striped = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_qcom_fragment_density_map_offset(&self) -> bool {
        self.qcom_fragment_density_map_offset && self.supports_ext_fragment_density_map()
    }
    pub fn enable_qcom_fragment_density_map_offset(&mut self) {
        self.qcom_fragment_density_map_offset = true;
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_nv_copy_memory_indirect(&self) -> bool {
        self.nv_copy_memory_indirect
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_buffer_device_address())
    }
    pub fn enable_nv_copy_memory_indirect(&mut self) {
        self.nv_copy_memory_indirect = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_memory_decompression(&self) -> bool {
        self.nv_memory_decompression
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_buffer_device_address())
    }
    pub fn enable_nv_memory_decompression(&mut self) {
        self.nv_memory_decompression = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_nv_device_generated_commands_compute(&self) -> bool {
        self.nv_device_generated_commands_compute && self.supports_nv_device_generated_commands()
    }
    pub fn enable_nv_device_generated_commands_compute(&mut self) {
        self.nv_device_generated_commands_compute = true;
        self.enable_nv_device_generated_commands();
    }
    pub fn supports_nv_ray_tracing_linear_swept_spheres(&self) -> bool {
        self.nv_ray_tracing_linear_swept_spheres && self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_linear_swept_spheres(&mut self) {
        self.nv_ray_tracing_linear_swept_spheres = true;
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_nv_linear_color_attachment(&self) -> bool {
        self.nv_linear_color_attachment
    }
    pub fn enable_nv_linear_color_attachment(&mut self) {
        self.nv_linear_color_attachment = true;
    }
    pub fn supports_khr_shader_maximal_reconvergence(&self) -> bool {
        self.khr_shader_maximal_reconvergence && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_khr_shader_maximal_reconvergence(&mut self) {
        self.khr_shader_maximal_reconvergence = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_ext_image_compression_control_swapchain(&self) -> bool {
        self.ext_image_compression_control_swapchain && self.supports_ext_image_compression_control()
    }
    pub fn enable_ext_image_compression_control_swapchain(&mut self) {
        self.ext_image_compression_control_swapchain = true;
        self.enable_ext_image_compression_control();
    }
    pub fn supports_qcom_image_processing(&self) -> bool {
        self.qcom_image_processing
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_format_feature_flags2())
    }
    pub fn enable_qcom_image_processing(&mut self) {
        self.qcom_image_processing = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_format_feature_flags2();
        }
    }
    pub fn supports_ext_nested_command_buffer(&self) -> bool {
        self.ext_nested_command_buffer
    }
    pub fn enable_ext_nested_command_buffer(&mut self) {
        self.ext_nested_command_buffer = true;
    }
    pub fn supports_ohos_external_memory(&self) -> bool {
        self.ohos_external_memory
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || (self.supports_khr_sampler_ycbcr_conversion()
                    && self.supports_khr_external_memory()
                    && self.supports_khr_dedicated_allocation()))
            && self.supports_ext_queue_family_foreign()
    }
    pub fn enable_ohos_external_memory(&mut self) {
        self.ohos_external_memory = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_sampler_ycbcr_conversion();
            self.enable_khr_external_memory();
            self.enable_khr_dedicated_allocation();
        }
        self.enable_ext_queue_family_foreign();
    }
    pub fn supports_ext_external_memory_acquire_unmodified(&self) -> bool {
        self.ext_external_memory_acquire_unmodified
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_ext_external_memory_acquire_unmodified(&mut self) {
        self.ext_external_memory_acquire_unmodified = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_ext_extended_dynamic_state3(&self) -> bool {
        self.ext_extended_dynamic_state3
    }
    pub fn enable_ext_extended_dynamic_state3(&mut self) {
        self.ext_extended_dynamic_state3 = true;
    }
    pub fn supports_ext_subpass_merge_feedback(&self) -> bool {
        self.ext_subpass_merge_feedback
    }
    pub fn enable_ext_subpass_merge_feedback(&mut self) {
        self.ext_subpass_merge_feedback = true;
    }
    pub fn supports_arm_tensors(&self) -> bool {
        self.arm_tensors && self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
    }
    pub fn enable_arm_tensors(&mut self) {
        self.arm_tensors = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 3, 0));
    }
    pub fn supports_ext_shader_module_identifier(&self) -> bool {
        self.ext_shader_module_identifier
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || self.supports_ext_pipeline_creation_cache_control())
    }
    pub fn enable_ext_shader_module_identifier(&mut self) {
        self.ext_shader_module_identifier = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_ext_pipeline_creation_cache_control();
        }
    }
    pub fn supports_ext_rasterization_order_attachment_access(&self) -> bool {
        self.ext_rasterization_order_attachment_access
    }
    pub fn enable_ext_rasterization_order_attachment_access(&mut self) {
        self.ext_rasterization_order_attachment_access = true;
    }
    pub fn supports_nv_optical_flow(&self) -> bool {
        self.nv_optical_flow
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || (self.supports_khr_format_feature_flags2() && self.supports_khr_synchronization2()))
    }
    pub fn enable_nv_optical_flow(&mut self) {
        self.nv_optical_flow = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_format_feature_flags2();
            self.enable_khr_synchronization2();
        }
    }
    pub fn supports_ext_legacy_dithering(&self) -> bool {
        self.ext_legacy_dithering
    }
    pub fn enable_ext_legacy_dithering(&mut self) {
        self.ext_legacy_dithering = true;
    }
    pub fn supports_ext_pipeline_protected_access(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.ext_pipeline_protected_access
    }
    pub fn enable_ext_pipeline_protected_access(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.ext_pipeline_protected_access = true;
        }
    }
    pub fn supports_android_external_format_resolve(&self) -> bool {
        self.android_external_format_resolve && self.supports_android_external_memory_android_hardware_buffer()
    }
    pub fn enable_android_external_format_resolve(&mut self) {
        self.android_external_format_resolve = true;
        self.enable_android_external_memory_android_hardware_buffer();
    }
    pub fn supports_khr_maintenance5(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.khr_maintenance5
                && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                    || (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                        && self.supports_khr_dynamic_rendering())))
    }
    pub fn enable_khr_maintenance5(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_maintenance5 = true;
            if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
                // depends on minimum core version, caller must specify
                debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
                self.enable_khr_dynamic_rendering();
            }
        }
    }
    pub fn supports_amd_anti_lag(&self) -> bool {
        self.amd_anti_lag
    }
    pub fn enable_amd_anti_lag(&mut self) {
        self.amd_anti_lag = true;
    }
    pub fn supports_amdx_dense_geometry_format(&self) -> bool {
        self.amdx_dense_geometry_format
            && self.supports_khr_acceleration_structure()
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5())
    }
    pub fn enable_amdx_dense_geometry_format(&mut self) {
        self.amdx_dense_geometry_format = true;
        self.enable_khr_acceleration_structure();
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_khr_present_id2(&self) -> bool {
        self.khr_present_id2 && self.supports_khr_swapchain()
    }
    pub fn enable_khr_present_id2(&mut self) {
        self.khr_present_id2 = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_khr_present_wait2(&self) -> bool {
        self.khr_present_wait2 && self.supports_khr_swapchain() && self.supports_khr_present_id2()
    }
    pub fn enable_khr_present_wait2(&mut self) {
        self.khr_present_wait2 = true;
        self.enable_khr_swapchain();
        self.enable_khr_present_id2();
    }
    pub fn supports_khr_ray_tracing_position_fetch(&self) -> bool {
        self.khr_ray_tracing_position_fetch && self.supports_khr_acceleration_structure()
    }
    pub fn enable_khr_ray_tracing_position_fetch(&mut self) {
        self.khr_ray_tracing_position_fetch = true;
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_ext_shader_object(&self) -> bool {
        self.ext_shader_object
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_shader_object(&mut self) {
        self.ext_shader_object = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_khr_pipeline_binary(&self) -> bool {
        self.khr_pipeline_binary
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5())
    }
    pub fn enable_khr_pipeline_binary(&mut self) {
        self.khr_pipeline_binary = true;
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_qcom_tile_properties(&self) -> bool {
        self.qcom_tile_properties
    }
    pub fn enable_qcom_tile_properties(&mut self) {
        self.qcom_tile_properties = true;
    }
    pub fn supports_sec_amigo_profiling(&self) -> bool {
        self.sec_amigo_profiling
    }
    pub fn enable_sec_amigo_profiling(&mut self) {
        self.sec_amigo_profiling = true;
    }
    pub fn supports_khr_swapchain_maintenance1(&self) -> bool {
        self.khr_swapchain_maintenance1 && self.supports_khr_swapchain()
    }
    pub fn enable_khr_swapchain_maintenance1(&mut self) {
        self.khr_swapchain_maintenance1 = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_qcom_multiview_per_view_viewports(&self) -> bool {
        self.qcom_multiview_per_view_viewports
    }
    pub fn enable_qcom_multiview_per_view_viewports(&mut self) {
        self.qcom_multiview_per_view_viewports = true;
    }
    pub fn supports_nv_ray_tracing_invocation_reorder(&self) -> bool {
        self.nv_ray_tracing_invocation_reorder && self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_nv_ray_tracing_invocation_reorder(&mut self) {
        self.nv_ray_tracing_invocation_reorder = true;
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_nv_cooperative_vector(&self) -> bool {
        self.nv_cooperative_vector
    }
    pub fn enable_nv_cooperative_vector(&mut self) {
        self.nv_cooperative_vector = true;
    }
    pub fn supports_nv_extended_sparse_address_space(&self) -> bool {
        self.nv_extended_sparse_address_space
    }
    pub fn enable_nv_extended_sparse_address_space(&mut self) {
        self.nv_extended_sparse_address_space = true;
    }
    pub fn supports_ext_mutable_descriptor_type(&self) -> bool {
        self.ext_mutable_descriptor_type
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_maintenance3())
    }
    pub fn enable_ext_mutable_descriptor_type(&mut self) {
        self.ext_mutable_descriptor_type = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_maintenance3();
        }
    }
    pub fn supports_ext_legacy_vertex_attributes(&self) -> bool {
        self.ext_legacy_vertex_attributes && self.supports_ext_vertex_input_dynamic_state()
    }
    pub fn enable_ext_legacy_vertex_attributes(&mut self) {
        self.ext_legacy_vertex_attributes = true;
        self.enable_ext_vertex_input_dynamic_state();
    }
    pub fn supports_arm_shader_core_builtins(&self) -> bool {
        self.arm_shader_core_builtins
    }
    pub fn enable_arm_shader_core_builtins(&mut self) {
        self.arm_shader_core_builtins = true;
    }
    pub fn supports_ext_pipeline_library_group_handles(&self) -> bool {
        self.ext_pipeline_library_group_handles
            && self.supports_khr_ray_tracing_pipeline()
            && self.supports_khr_pipeline_library()
    }
    pub fn enable_ext_pipeline_library_group_handles(&mut self) {
        self.ext_pipeline_library_group_handles = true;
        self.enable_khr_ray_tracing_pipeline();
        self.enable_khr_pipeline_library();
    }
    pub fn supports_ext_dynamic_rendering_unused_attachments(&self) -> bool {
        self.ext_dynamic_rendering_unused_attachments
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_dynamic_rendering_unused_attachments(&mut self) {
        self.ext_dynamic_rendering_unused_attachments = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_khr_internally_synchronized_queues(&self) -> bool {
        self.khr_internally_synchronized_queues && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_khr_internally_synchronized_queues(&mut self) {
        self.khr_internally_synchronized_queues = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_nv_low_latency2(&self) -> bool {
        self.nv_low_latency2
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_timeline_semaphore())
            && (self.supports_khr_present_id() || self.supports_khr_present_id2())
    }
    pub fn enable_nv_low_latency2(&mut self) {
        self.nv_low_latency2 = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_timeline_semaphore();
        }
        // ambiguous dependency, caller must enable one explicitly
        debug_assert!(self.supports_khr_present_id() || self.supports_khr_present_id2());
    }
    pub fn supports_khr_cooperative_matrix(&self) -> bool {
        self.khr_cooperative_matrix
    }
    pub fn enable_khr_cooperative_matrix(&mut self) {
        self.khr_cooperative_matrix = true;
    }
    pub fn supports_arm_data_graph(&self) -> bool {
        self.arm_data_graph
            && self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            && self.supports_khr_maintenance5()
            && self.supports_khr_deferred_host_operations()
    }
    pub fn enable_arm_data_graph(&mut self) {
        self.arm_data_graph = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 3, 0));
        self.enable_khr_maintenance5();
        self.enable_khr_deferred_host_operations();
    }
    pub fn supports_qcom_multiview_per_view_render_areas(&self) -> bool {
        self.qcom_multiview_per_view_render_areas
    }
    pub fn enable_qcom_multiview_per_view_render_areas(&mut self) {
        self.qcom_multiview_per_view_render_areas = true;
    }
    pub fn supports_khr_compute_shader_derivatives(&self) -> bool {
        self.khr_compute_shader_derivatives
    }
    pub fn enable_khr_compute_shader_derivatives(&mut self) {
        self.khr_compute_shader_derivatives = true;
    }
    pub fn supports_nv_per_stage_descriptor_set(&self) -> bool {
        self.nv_per_stage_descriptor_set
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance6())
    }
    pub fn enable_nv_per_stage_descriptor_set(&mut self) {
        self.nv_per_stage_descriptor_set = true;
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance6();
        }
    }
    pub fn supports_qcom_image_processing2(&self) -> bool {
        self.qcom_image_processing2 && self.supports_qcom_image_processing()
    }
    pub fn enable_qcom_image_processing2(&mut self) {
        self.qcom_image_processing2 = true;
        self.enable_qcom_image_processing();
    }
    pub fn supports_qcom_filter_cubic_weights(&self) -> bool {
        self.qcom_filter_cubic_weights && self.supports_ext_filter_cubic()
    }
    pub fn enable_qcom_filter_cubic_weights(&mut self) {
        self.qcom_filter_cubic_weights = true;
        self.enable_ext_filter_cubic();
    }
    pub fn supports_qcom_ycbcr_degamma(&self) -> bool {
        self.qcom_ycbcr_degamma
    }
    pub fn enable_qcom_ycbcr_degamma(&mut self) {
        self.qcom_ycbcr_degamma = true;
    }
    pub fn supports_qcom_filter_cubic_clamp(&self) -> bool {
        self.qcom_filter_cubic_clamp
            && self.supports_ext_filter_cubic()
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_ext_sampler_filter_minmax())
    }
    pub fn enable_qcom_filter_cubic_clamp(&mut self) {
        self.qcom_filter_cubic_clamp = true;
        self.enable_ext_filter_cubic();
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_ext_sampler_filter_minmax();
        }
    }
    pub fn supports_ext_attachment_feedback_loop_dynamic_state(&self) -> bool {
        self.ext_attachment_feedback_loop_dynamic_state && self.supports_ext_attachment_feedback_loop_layout()
    }
    pub fn enable_ext_attachment_feedback_loop_dynamic_state(&mut self) {
        self.ext_attachment_feedback_loop_dynamic_state = true;
        self.enable_ext_attachment_feedback_loop_layout();
    }
    pub fn supports_khr_vertex_attribute_divisor(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_vertex_attribute_divisor
    }
    pub fn enable_khr_vertex_attribute_divisor(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_vertex_attribute_divisor = true;
        }
    }
    pub fn supports_khr_load_store_op_none(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_load_store_op_none
    }
    pub fn enable_khr_load_store_op_none(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_load_store_op_none = true;
        }
    }
    pub fn supports_khr_unified_image_layouts(&self) -> bool {
        self.khr_unified_image_layouts
    }
    pub fn enable_khr_unified_image_layouts(&mut self) {
        self.khr_unified_image_layouts = true;
    }
    pub fn supports_khr_shader_float_controls2(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.khr_shader_float_controls2
                && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                && self.supports_khr_shader_float_controls())
    }
    pub fn enable_khr_shader_float_controls2(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_shader_float_controls2 = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
            self.enable_khr_shader_float_controls();
        }
    }
    pub fn supports_msft_layered_driver(&self) -> bool {
        self.msft_layered_driver
    }
    pub fn enable_msft_layered_driver(&mut self) {
        self.msft_layered_driver = true;
    }
    pub fn supports_khr_index_type_uint8(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_index_type_uint8
    }
    pub fn enable_khr_index_type_uint8(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_index_type_uint8 = true;
        }
    }
    pub fn supports_khr_line_rasterization(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_line_rasterization
    }
    pub fn enable_khr_line_rasterization(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_line_rasterization = true;
        }
    }
    pub fn supports_khr_calibrated_timestamps(&self) -> bool {
        self.khr_calibrated_timestamps
    }
    pub fn enable_khr_calibrated_timestamps(&mut self) {
        self.khr_calibrated_timestamps = true;
    }
    pub fn supports_khr_shader_expect_assume(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.khr_shader_expect_assume
    }
    pub fn enable_khr_shader_expect_assume(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_shader_expect_assume = true;
        }
    }
    pub fn supports_khr_maintenance6(&self) -> bool {
        self.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            || (self.khr_maintenance6 && self.core_version >= vk::Version::from_raw_parts(1, 1, 0))
    }
    pub fn enable_khr_maintenance6(&mut self) {
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.khr_maintenance6 = true;
            // depends on minimum core version, caller must specify
            debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
        }
    }
    pub fn supports_nv_descriptor_pool_overallocation(&self) -> bool {
        self.nv_descriptor_pool_overallocation && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_nv_descriptor_pool_overallocation(&mut self) {
        self.nv_descriptor_pool_overallocation = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_qcom_tile_memory_heap(&self) -> bool {
        self.qcom_tile_memory_heap
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                || self.supports_khr_get_memory_requirements2())
    }
    pub fn enable_qcom_tile_memory_heap(&mut self) {
        self.qcom_tile_memory_heap = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_get_memory_requirements2();
        }
    }
    pub fn supports_khr_copy_memory_indirect(&self) -> bool {
        self.khr_copy_memory_indirect
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_buffer_device_address())
    }
    pub fn enable_khr_copy_memory_indirect(&mut self) {
        self.khr_copy_memory_indirect = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_buffer_device_address();
        }
    }
    pub fn supports_ext_memory_decompression(&self) -> bool {
        self.ext_memory_decompression && self.supports_khr_buffer_device_address()
    }
    pub fn enable_ext_memory_decompression(&mut self) {
        self.ext_memory_decompression = true;
        self.enable_khr_buffer_device_address();
    }
    pub fn supports_nv_raw_access_chains(&self) -> bool {
        self.nv_raw_access_chains
    }
    pub fn enable_nv_raw_access_chains(&mut self) {
        self.nv_raw_access_chains = true;
    }
    pub fn supports_nv_external_compute_queue(&self) -> bool {
        self.nv_external_compute_queue
    }
    pub fn enable_nv_external_compute_queue(&mut self) {
        self.nv_external_compute_queue = true;
    }
    pub fn supports_khr_shader_relaxed_extended_instruction(&self) -> bool {
        self.khr_shader_relaxed_extended_instruction
    }
    pub fn enable_khr_shader_relaxed_extended_instruction(&mut self) {
        self.khr_shader_relaxed_extended_instruction = true;
    }
    pub fn supports_nv_command_buffer_inheritance(&self) -> bool {
        self.nv_command_buffer_inheritance
    }
    pub fn enable_nv_command_buffer_inheritance(&mut self) {
        self.nv_command_buffer_inheritance = true;
    }
    pub fn supports_khr_maintenance7(&self) -> bool {
        self.khr_maintenance7 && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_khr_maintenance7(&mut self) {
        self.khr_maintenance7 = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_nv_shader_atomic_float16_vector(&self) -> bool {
        self.nv_shader_atomic_float16_vector
    }
    pub fn enable_nv_shader_atomic_float16_vector(&mut self) {
        self.nv_shader_atomic_float16_vector = true;
    }
    pub fn supports_ext_shader_replicated_composites(&self) -> bool {
        self.ext_shader_replicated_composites
    }
    pub fn enable_ext_shader_replicated_composites(&mut self) {
        self.ext_shader_replicated_composites = true;
    }
    pub fn supports_ext_shader_float8(&self) -> bool {
        self.ext_shader_float8
    }
    pub fn enable_ext_shader_float8(&mut self) {
        self.ext_shader_float8 = true;
    }
    pub fn supports_nv_ray_tracing_validation(&self) -> bool {
        self.nv_ray_tracing_validation
    }
    pub fn enable_nv_ray_tracing_validation(&mut self) {
        self.nv_ray_tracing_validation = true;
    }
    pub fn supports_nv_cluster_acceleration_structure(&self) -> bool {
        self.nv_cluster_acceleration_structure && self.supports_khr_acceleration_structure()
    }
    pub fn enable_nv_cluster_acceleration_structure(&mut self) {
        self.nv_cluster_acceleration_structure = true;
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_nv_partitioned_acceleration_structure(&self) -> bool {
        self.nv_partitioned_acceleration_structure && self.supports_khr_acceleration_structure()
    }
    pub fn enable_nv_partitioned_acceleration_structure(&mut self) {
        self.nv_partitioned_acceleration_structure = true;
        self.enable_khr_acceleration_structure();
    }
    pub fn supports_ext_device_generated_commands(&self) -> bool {
        self.ext_device_generated_commands
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0)
                || ((self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || self.supports_khr_buffer_device_address())
                    && self.supports_khr_maintenance5()))
    }
    pub fn enable_ext_device_generated_commands(&mut self) {
        self.ext_device_generated_commands = true;
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
                self.enable_khr_buffer_device_address();
            }
            self.enable_khr_maintenance5();
        }
    }
    pub fn supports_khr_maintenance8(&self) -> bool {
        self.khr_maintenance8 && self.core_version >= vk::Version::from_raw_parts(1, 1, 0)
    }
    pub fn enable_khr_maintenance8(&mut self) {
        self.khr_maintenance8 = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 1, 0));
    }
    pub fn supports_mesa_image_alignment_control(&self) -> bool {
        self.mesa_image_alignment_control
    }
    pub fn enable_mesa_image_alignment_control(&mut self) {
        self.mesa_image_alignment_control = true;
    }
    pub fn supports_khr_shader_fma(&self) -> bool {
        self.khr_shader_fma
    }
    pub fn enable_khr_shader_fma(&mut self) {
        self.khr_shader_fma = true;
    }
    pub fn supports_nv_push_constant_bank(&self) -> bool {
        self.nv_push_constant_bank
    }
    pub fn enable_nv_push_constant_bank(&mut self) {
        self.nv_push_constant_bank = true;
    }
    pub fn supports_ext_ray_tracing_invocation_reorder(&self) -> bool {
        self.ext_ray_tracing_invocation_reorder && self.supports_khr_ray_tracing_pipeline()
    }
    pub fn enable_ext_ray_tracing_invocation_reorder(&mut self) {
        self.ext_ray_tracing_invocation_reorder = true;
        self.enable_khr_ray_tracing_pipeline();
    }
    pub fn supports_ext_depth_clamp_control(&self) -> bool {
        self.ext_depth_clamp_control
    }
    pub fn enable_ext_depth_clamp_control(&mut self) {
        self.ext_depth_clamp_control = true;
    }
    pub fn supports_khr_maintenance9(&self) -> bool {
        self.khr_maintenance9
    }
    pub fn enable_khr_maintenance9(&mut self) {
        self.khr_maintenance9 = true;
    }
    pub fn supports_huawei_hdr_vivid(&self) -> bool {
        self.huawei_hdr_vivid && self.supports_khr_swapchain() && self.supports_ext_hdr_metadata()
    }
    pub fn enable_huawei_hdr_vivid(&mut self) {
        self.huawei_hdr_vivid = true;
        self.enable_khr_swapchain();
        self.enable_ext_hdr_metadata();
    }
    pub fn supports_nv_cooperative_matrix2(&self) -> bool {
        self.nv_cooperative_matrix2 && self.supports_khr_cooperative_matrix()
    }
    pub fn enable_nv_cooperative_matrix2(&mut self) {
        self.nv_cooperative_matrix2 = true;
        self.enable_khr_cooperative_matrix();
    }
    pub fn supports_arm_pipeline_opacity_micromap(&self) -> bool {
        self.arm_pipeline_opacity_micromap && self.supports_ext_opacity_micromap()
    }
    pub fn enable_arm_pipeline_opacity_micromap(&mut self) {
        self.arm_pipeline_opacity_micromap = true;
        self.enable_ext_opacity_micromap();
    }
    pub fn supports_ext_external_memory_metal(&self) -> bool {
        self.ext_external_memory_metal
            && (self.core_version >= vk::Version::from_raw_parts(1, 1, 0) || self.supports_khr_external_memory())
    }
    pub fn enable_ext_external_memory_metal(&mut self) {
        self.ext_external_memory_metal = true;
        if self.core_version < vk::Version::from_raw_parts(1, 1, 0) {
            self.enable_khr_external_memory();
        }
    }
    pub fn supports_khr_depth_clamp_zero_one(&self) -> bool {
        self.khr_depth_clamp_zero_one
    }
    pub fn enable_khr_depth_clamp_zero_one(&mut self) {
        self.khr_depth_clamp_zero_one = true;
    }
    pub fn supports_arm_performance_counters_by_region(&self) -> bool {
        self.arm_performance_counters_by_region
    }
    pub fn enable_arm_performance_counters_by_region(&mut self) {
        self.arm_performance_counters_by_region = true;
    }
    pub fn supports_ext_vertex_attribute_robustness(&self) -> bool {
        self.ext_vertex_attribute_robustness
    }
    pub fn enable_ext_vertex_attribute_robustness(&mut self) {
        self.ext_vertex_attribute_robustness = true;
    }
    pub fn supports_arm_format_pack(&self) -> bool {
        self.arm_format_pack
    }
    pub fn enable_arm_format_pack(&mut self) {
        self.arm_format_pack = true;
    }
    pub fn supports_valve_fragment_density_map_layered(&self) -> bool {
        self.valve_fragment_density_map_layered
            && (self.core_version >= vk::Version::from_raw_parts(1, 4, 0) || self.supports_khr_maintenance5())
            && self.supports_ext_fragment_density_map()
    }
    pub fn enable_valve_fragment_density_map_layered(&mut self) {
        self.valve_fragment_density_map_layered = true;
        if self.core_version < vk::Version::from_raw_parts(1, 4, 0) {
            self.enable_khr_maintenance5();
        }
        self.enable_ext_fragment_density_map();
    }
    pub fn supports_khr_robustness2(&self) -> bool {
        self.khr_robustness2
    }
    pub fn enable_khr_robustness2(&mut self) {
        self.khr_robustness2 = true;
    }
    pub fn supports_nv_present_metering(&self) -> bool {
        self.nv_present_metering
    }
    pub fn enable_nv_present_metering(&mut self) {
        self.nv_present_metering = true;
    }
    pub fn supports_ext_fragment_density_map_offset(&self) -> bool {
        self.ext_fragment_density_map_offset
            && self.supports_ext_fragment_density_map()
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_create_renderpass2())
            && (self.core_version >= vk::Version::from_raw_parts(1, 3, 0) || self.supports_khr_dynamic_rendering())
    }
    pub fn enable_ext_fragment_density_map_offset(&mut self) {
        self.ext_fragment_density_map_offset = true;
        self.enable_ext_fragment_density_map();
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_create_renderpass2();
        }
        if self.core_version < vk::Version::from_raw_parts(1, 3, 0) {
            self.enable_khr_dynamic_rendering();
        }
    }
    pub fn supports_ext_zero_initialize_device_memory(&self) -> bool {
        self.ext_zero_initialize_device_memory
    }
    pub fn enable_ext_zero_initialize_device_memory(&mut self) {
        self.ext_zero_initialize_device_memory = true;
    }
    pub fn supports_khr_present_mode_fifo_latest_ready(&self) -> bool {
        self.khr_present_mode_fifo_latest_ready && self.supports_khr_swapchain()
    }
    pub fn enable_khr_present_mode_fifo_latest_ready(&mut self) {
        self.khr_present_mode_fifo_latest_ready = true;
        self.enable_khr_swapchain();
    }
    pub fn supports_ext_shader_64bit_indexing(&self) -> bool {
        self.ext_shader_64bit_indexing
    }
    pub fn enable_ext_shader_64bit_indexing(&mut self) {
        self.ext_shader_64bit_indexing = true;
    }
    pub fn supports_ext_custom_resolve(&self) -> bool {
        self.ext_custom_resolve
    }
    pub fn enable_ext_custom_resolve(&mut self) {
        self.ext_custom_resolve = true;
    }
    pub fn supports_qcom_data_graph_model(&self) -> bool {
        self.qcom_data_graph_model && self.supports_arm_data_graph()
    }
    pub fn enable_qcom_data_graph_model(&mut self) {
        self.qcom_data_graph_model = true;
        self.enable_arm_data_graph();
    }
    pub fn supports_khr_maintenance10(&self) -> bool {
        self.khr_maintenance10
    }
    pub fn enable_khr_maintenance10(&mut self) {
        self.khr_maintenance10 = true;
    }
    pub fn supports_ext_shader_long_vector(&self) -> bool {
        self.ext_shader_long_vector && self.core_version >= vk::Version::from_raw_parts(1, 2, 0)
    }
    pub fn enable_ext_shader_long_vector(&mut self) {
        self.ext_shader_long_vector = true;
        // depends on minimum core version, caller must specify
        debug_assert!(self.core_version >= vk::Version::from_raw_parts(1, 2, 0));
    }
    pub fn supports_sec_pipeline_cache_incremental_mode(&self) -> bool {
        self.sec_pipeline_cache_incremental_mode
    }
    pub fn enable_sec_pipeline_cache_incremental_mode(&mut self) {
        self.sec_pipeline_cache_incremental_mode = true;
    }
    pub fn supports_ext_shader_uniform_buffer_unsized_array(&self) -> bool {
        self.ext_shader_uniform_buffer_unsized_array
    }
    pub fn enable_ext_shader_uniform_buffer_unsized_array(&mut self) {
        self.ext_shader_uniform_buffer_unsized_array = true;
    }
    pub fn supports_nv_compute_occupancy_priority(&self) -> bool {
        self.nv_compute_occupancy_priority
    }
    pub fn enable_nv_compute_occupancy_priority(&mut self) {
        self.nv_compute_occupancy_priority = true;
    }
    pub fn supports_ext_shader_subgroup_partitioned(&self) -> bool {
        self.ext_shader_subgroup_partitioned
    }
    pub fn enable_ext_shader_subgroup_partitioned(&mut self) {
        self.ext_shader_subgroup_partitioned = true;
    }
    pub fn supports_valve_shader_mixed_float_dot_product(&self) -> bool {
        self.valve_shader_mixed_float_dot_product
            && (self.core_version >= vk::Version::from_raw_parts(1, 2, 0) || self.supports_khr_shader_float16_int8())
    }
    pub fn enable_valve_shader_mixed_float_dot_product(&mut self) {
        self.valve_shader_mixed_float_dot_product = true;
        if self.core_version < vk::Version::from_raw_parts(1, 2, 0) {
            self.enable_khr_shader_float16_int8();
        }
    }
    pub fn to_name_vec(&self) -> Vec<&'static CStr> {
        let mut v = Vec::new();
        if self.khr_swapchain {
            v.push(c"VK_KHR_swapchain");
        }
        if self.khr_display_swapchain {
            v.push(c"VK_KHR_display_swapchain");
        }
        if self.nv_glsl_shader {
            v.push(c"VK_NV_glsl_shader");
        }
        if self.ext_depth_range_unrestricted {
            v.push(c"VK_EXT_depth_range_unrestricted");
        }
        if self.khr_sampler_mirror_clamp_to_edge {
            v.push(c"VK_KHR_sampler_mirror_clamp_to_edge");
        }
        if self.img_filter_cubic {
            v.push(c"VK_IMG_filter_cubic");
        }
        if self.amd_rasterization_order {
            v.push(c"VK_AMD_rasterization_order");
        }
        if self.amd_shader_trinary_minmax {
            v.push(c"VK_AMD_shader_trinary_minmax");
        }
        if self.amd_shader_explicit_vertex_parameter {
            v.push(c"VK_AMD_shader_explicit_vertex_parameter");
        }
        if self.ext_debug_marker {
            v.push(c"VK_EXT_debug_marker");
        }
        if self.amd_gcn_shader {
            v.push(c"VK_AMD_gcn_shader");
        }
        if self.nv_dedicated_allocation {
            v.push(c"VK_NV_dedicated_allocation");
        }
        if self.ext_transform_feedback {
            v.push(c"VK_EXT_transform_feedback");
        }
        if self.nvx_binary_import {
            v.push(c"VK_NVX_binary_import");
        }
        if self.nvx_image_view_handle {
            v.push(c"VK_NVX_image_view_handle");
        }
        if self.amd_draw_indirect_count {
            v.push(c"VK_AMD_draw_indirect_count");
        }
        if self.amd_negative_viewport_height {
            v.push(c"VK_AMD_negative_viewport_height");
        }
        if self.amd_gpu_shader_half_float {
            v.push(c"VK_AMD_gpu_shader_half_float");
        }
        if self.amd_shader_ballot {
            v.push(c"VK_AMD_shader_ballot");
        }
        if self.amd_texture_gather_bias_lod {
            v.push(c"VK_AMD_texture_gather_bias_lod");
        }
        if self.amd_shader_info {
            v.push(c"VK_AMD_shader_info");
        }
        if self.khr_dynamic_rendering {
            v.push(c"VK_KHR_dynamic_rendering");
        }
        if self.amd_shader_image_load_store_lod {
            v.push(c"VK_AMD_shader_image_load_store_lod");
        }
        if self.nv_corner_sampled_image {
            v.push(c"VK_NV_corner_sampled_image");
        }
        if self.khr_multiview {
            v.push(c"VK_KHR_multiview");
        }
        if self.img_format_pvrtc {
            v.push(c"VK_IMG_format_pvrtc");
        }
        if self.nv_external_memory {
            v.push(c"VK_NV_external_memory");
        }
        if self.nv_external_memory_win32 {
            v.push(c"VK_NV_external_memory_win32");
        }
        if self.nv_win32_keyed_mutex {
            v.push(c"VK_NV_win32_keyed_mutex");
        }
        if self.khr_device_group {
            v.push(c"VK_KHR_device_group");
        }
        if self.khr_shader_draw_parameters {
            v.push(c"VK_KHR_shader_draw_parameters");
        }
        if self.ext_shader_subgroup_ballot {
            v.push(c"VK_EXT_shader_subgroup_ballot");
        }
        if self.ext_shader_subgroup_vote {
            v.push(c"VK_EXT_shader_subgroup_vote");
        }
        if self.ext_texture_compression_astc_hdr {
            v.push(c"VK_EXT_texture_compression_astc_hdr");
        }
        if self.ext_astc_decode_mode {
            v.push(c"VK_EXT_astc_decode_mode");
        }
        if self.ext_pipeline_robustness {
            v.push(c"VK_EXT_pipeline_robustness");
        }
        if self.khr_maintenance1 {
            v.push(c"VK_KHR_maintenance1");
        }
        if self.khr_external_memory {
            v.push(c"VK_KHR_external_memory");
        }
        if self.khr_external_memory_win32 {
            v.push(c"VK_KHR_external_memory_win32");
        }
        if self.khr_external_memory_fd {
            v.push(c"VK_KHR_external_memory_fd");
        }
        if self.khr_win32_keyed_mutex {
            v.push(c"VK_KHR_win32_keyed_mutex");
        }
        if self.khr_external_semaphore {
            v.push(c"VK_KHR_external_semaphore");
        }
        if self.khr_external_semaphore_win32 {
            v.push(c"VK_KHR_external_semaphore_win32");
        }
        if self.khr_external_semaphore_fd {
            v.push(c"VK_KHR_external_semaphore_fd");
        }
        if self.khr_push_descriptor {
            v.push(c"VK_KHR_push_descriptor");
        }
        if self.ext_conditional_rendering {
            v.push(c"VK_EXT_conditional_rendering");
        }
        if self.khr_shader_float16_int8 {
            v.push(c"VK_KHR_shader_float16_int8");
        }
        if self.khr_16bit_storage {
            v.push(c"VK_KHR_16bit_storage");
        }
        if self.khr_incremental_present {
            v.push(c"VK_KHR_incremental_present");
        }
        if self.khr_descriptor_update_template {
            v.push(c"VK_KHR_descriptor_update_template");
        }
        if self.nv_clip_space_w_scaling {
            v.push(c"VK_NV_clip_space_w_scaling");
        }
        if self.ext_display_control {
            v.push(c"VK_EXT_display_control");
        }
        if self.google_display_timing {
            v.push(c"VK_GOOGLE_display_timing");
        }
        if self.nv_sample_mask_override_coverage {
            v.push(c"VK_NV_sample_mask_override_coverage");
        }
        if self.nv_geometry_shader_passthrough {
            v.push(c"VK_NV_geometry_shader_passthrough");
        }
        if self.nv_viewport_array2 {
            v.push(c"VK_NV_viewport_array2");
        }
        if self.nvx_multiview_per_view_attributes {
            v.push(c"VK_NVX_multiview_per_view_attributes");
        }
        if self.nv_viewport_swizzle {
            v.push(c"VK_NV_viewport_swizzle");
        }
        if self.ext_discard_rectangles {
            v.push(c"VK_EXT_discard_rectangles");
        }
        if self.ext_conservative_rasterization {
            v.push(c"VK_EXT_conservative_rasterization");
        }
        if self.ext_depth_clip_enable {
            v.push(c"VK_EXT_depth_clip_enable");
        }
        if self.ext_hdr_metadata {
            v.push(c"VK_EXT_hdr_metadata");
        }
        if self.khr_imageless_framebuffer {
            v.push(c"VK_KHR_imageless_framebuffer");
        }
        if self.khr_create_renderpass2 {
            v.push(c"VK_KHR_create_renderpass2");
        }
        if self.img_relaxed_line_rasterization {
            v.push(c"VK_IMG_relaxed_line_rasterization");
        }
        if self.khr_shared_presentable_image {
            v.push(c"VK_KHR_shared_presentable_image");
        }
        if self.khr_external_fence {
            v.push(c"VK_KHR_external_fence");
        }
        if self.khr_external_fence_win32 {
            v.push(c"VK_KHR_external_fence_win32");
        }
        if self.khr_external_fence_fd {
            v.push(c"VK_KHR_external_fence_fd");
        }
        if self.khr_performance_query {
            v.push(c"VK_KHR_performance_query");
        }
        if self.khr_maintenance2 {
            v.push(c"VK_KHR_maintenance2");
        }
        if self.khr_variable_pointers {
            v.push(c"VK_KHR_variable_pointers");
        }
        if self.ext_external_memory_dma_buf {
            v.push(c"VK_EXT_external_memory_dma_buf");
        }
        if self.ext_queue_family_foreign {
            v.push(c"VK_EXT_queue_family_foreign");
        }
        if self.khr_dedicated_allocation {
            v.push(c"VK_KHR_dedicated_allocation");
        }
        if self.android_external_memory_android_hardware_buffer {
            v.push(c"VK_ANDROID_external_memory_android_hardware_buffer");
        }
        if self.ext_sampler_filter_minmax {
            v.push(c"VK_EXT_sampler_filter_minmax");
        }
        if self.khr_storage_buffer_storage_class {
            v.push(c"VK_KHR_storage_buffer_storage_class");
        }
        if self.amd_gpu_shader_int16 {
            v.push(c"VK_AMD_gpu_shader_int16");
        }
        if self.amdx_shader_enqueue {
            v.push(c"VK_AMDX_shader_enqueue");
        }
        if self.ext_descriptor_heap {
            v.push(c"VK_EXT_descriptor_heap");
        }
        if self.amd_mixed_attachment_samples {
            v.push(c"VK_AMD_mixed_attachment_samples");
        }
        if self.amd_shader_fragment_mask {
            v.push(c"VK_AMD_shader_fragment_mask");
        }
        if self.ext_inline_uniform_block {
            v.push(c"VK_EXT_inline_uniform_block");
        }
        if self.ext_shader_stencil_export {
            v.push(c"VK_EXT_shader_stencil_export");
        }
        if self.khr_shader_bfloat16 {
            v.push(c"VK_KHR_shader_bfloat16");
        }
        if self.ext_sample_locations {
            v.push(c"VK_EXT_sample_locations");
        }
        if self.khr_relaxed_block_layout {
            v.push(c"VK_KHR_relaxed_block_layout");
        }
        if self.khr_get_memory_requirements2 {
            v.push(c"VK_KHR_get_memory_requirements2");
        }
        if self.khr_image_format_list {
            v.push(c"VK_KHR_image_format_list");
        }
        if self.ext_blend_operation_advanced {
            v.push(c"VK_EXT_blend_operation_advanced");
        }
        if self.nv_fragment_coverage_to_color {
            v.push(c"VK_NV_fragment_coverage_to_color");
        }
        if self.khr_acceleration_structure {
            v.push(c"VK_KHR_acceleration_structure");
        }
        if self.khr_ray_tracing_pipeline {
            v.push(c"VK_KHR_ray_tracing_pipeline");
        }
        if self.khr_ray_query {
            v.push(c"VK_KHR_ray_query");
        }
        if self.nv_framebuffer_mixed_samples {
            v.push(c"VK_NV_framebuffer_mixed_samples");
        }
        if self.nv_fill_rectangle {
            v.push(c"VK_NV_fill_rectangle");
        }
        if self.nv_shader_sm_builtins {
            v.push(c"VK_NV_shader_sm_builtins");
        }
        if self.ext_post_depth_coverage {
            v.push(c"VK_EXT_post_depth_coverage");
        }
        if self.khr_sampler_ycbcr_conversion {
            v.push(c"VK_KHR_sampler_ycbcr_conversion");
        }
        if self.khr_bind_memory2 {
            v.push(c"VK_KHR_bind_memory2");
        }
        if self.ext_image_drm_format_modifier {
            v.push(c"VK_EXT_image_drm_format_modifier");
        }
        if self.ext_validation_cache {
            v.push(c"VK_EXT_validation_cache");
        }
        if self.ext_descriptor_indexing {
            v.push(c"VK_EXT_descriptor_indexing");
        }
        if self.ext_shader_viewport_index_layer {
            v.push(c"VK_EXT_shader_viewport_index_layer");
        }
        if self.khr_portability_subset {
            v.push(c"VK_KHR_portability_subset");
        }
        if self.nv_shading_rate_image {
            v.push(c"VK_NV_shading_rate_image");
        }
        if self.nv_ray_tracing {
            v.push(c"VK_NV_ray_tracing");
        }
        if self.nv_representative_fragment_test {
            v.push(c"VK_NV_representative_fragment_test");
        }
        if self.khr_maintenance3 {
            v.push(c"VK_KHR_maintenance3");
        }
        if self.khr_draw_indirect_count {
            v.push(c"VK_KHR_draw_indirect_count");
        }
        if self.ext_filter_cubic {
            v.push(c"VK_EXT_filter_cubic");
        }
        if self.qcom_render_pass_shader_resolve {
            v.push(c"VK_QCOM_render_pass_shader_resolve");
        }
        if self.qcom_cooperative_matrix_conversion {
            v.push(c"VK_QCOM_cooperative_matrix_conversion");
        }
        if self.ext_global_priority {
            v.push(c"VK_EXT_global_priority");
        }
        if self.khr_shader_subgroup_extended_types {
            v.push(c"VK_KHR_shader_subgroup_extended_types");
        }
        if self.khr_8bit_storage {
            v.push(c"VK_KHR_8bit_storage");
        }
        if self.ext_external_memory_host {
            v.push(c"VK_EXT_external_memory_host");
        }
        if self.amd_buffer_marker {
            v.push(c"VK_AMD_buffer_marker");
        }
        if self.khr_shader_atomic_int64 {
            v.push(c"VK_KHR_shader_atomic_int64");
        }
        if self.khr_shader_clock {
            v.push(c"VK_KHR_shader_clock");
        }
        if self.amd_pipeline_compiler_control {
            v.push(c"VK_AMD_pipeline_compiler_control");
        }
        if self.ext_calibrated_timestamps {
            v.push(c"VK_EXT_calibrated_timestamps");
        }
        if self.amd_shader_core_properties {
            v.push(c"VK_AMD_shader_core_properties");
        }
        if self.khr_global_priority {
            v.push(c"VK_KHR_global_priority");
        }
        if self.amd_memory_overallocation_behavior {
            v.push(c"VK_AMD_memory_overallocation_behavior");
        }
        if self.ext_vertex_attribute_divisor {
            v.push(c"VK_EXT_vertex_attribute_divisor");
        }
        if self.ext_pipeline_creation_feedback {
            v.push(c"VK_EXT_pipeline_creation_feedback");
        }
        if self.khr_driver_properties {
            v.push(c"VK_KHR_driver_properties");
        }
        if self.khr_shader_float_controls {
            v.push(c"VK_KHR_shader_float_controls");
        }
        if self.nv_shader_subgroup_partitioned {
            v.push(c"VK_NV_shader_subgroup_partitioned");
        }
        if self.khr_depth_stencil_resolve {
            v.push(c"VK_KHR_depth_stencil_resolve");
        }
        if self.khr_swapchain_mutable_format {
            v.push(c"VK_KHR_swapchain_mutable_format");
        }
        if self.nv_compute_shader_derivatives {
            v.push(c"VK_NV_compute_shader_derivatives");
        }
        if self.nv_mesh_shader {
            v.push(c"VK_NV_mesh_shader");
        }
        if self.nv_fragment_shader_barycentric {
            v.push(c"VK_NV_fragment_shader_barycentric");
        }
        if self.nv_shader_image_footprint {
            v.push(c"VK_NV_shader_image_footprint");
        }
        if self.nv_scissor_exclusive {
            v.push(c"VK_NV_scissor_exclusive");
        }
        if self.nv_device_diagnostic_checkpoints {
            v.push(c"VK_NV_device_diagnostic_checkpoints");
        }
        if self.khr_timeline_semaphore {
            v.push(c"VK_KHR_timeline_semaphore");
        }
        if self.ext_present_timing {
            v.push(c"VK_EXT_present_timing");
        }
        if self.intel_shader_integer_functions2 {
            v.push(c"VK_INTEL_shader_integer_functions2");
        }
        if self.intel_performance_query {
            v.push(c"VK_INTEL_performance_query");
        }
        if self.khr_vulkan_memory_model {
            v.push(c"VK_KHR_vulkan_memory_model");
        }
        if self.ext_pci_bus_info {
            v.push(c"VK_EXT_pci_bus_info");
        }
        if self.amd_display_native_hdr {
            v.push(c"VK_AMD_display_native_hdr");
        }
        if self.khr_shader_terminate_invocation {
            v.push(c"VK_KHR_shader_terminate_invocation");
        }
        if self.ext_fragment_density_map {
            v.push(c"VK_EXT_fragment_density_map");
        }
        if self.ext_scalar_block_layout {
            v.push(c"VK_EXT_scalar_block_layout");
        }
        if self.google_hlsl_functionality1 {
            v.push(c"VK_GOOGLE_hlsl_functionality1");
        }
        if self.google_decorate_string {
            v.push(c"VK_GOOGLE_decorate_string");
        }
        if self.ext_subgroup_size_control {
            v.push(c"VK_EXT_subgroup_size_control");
        }
        if self.khr_fragment_shading_rate {
            v.push(c"VK_KHR_fragment_shading_rate");
        }
        if self.amd_shader_core_properties2 {
            v.push(c"VK_AMD_shader_core_properties2");
        }
        if self.amd_device_coherent_memory {
            v.push(c"VK_AMD_device_coherent_memory");
        }
        if self.khr_dynamic_rendering_local_read {
            v.push(c"VK_KHR_dynamic_rendering_local_read");
        }
        if self.ext_shader_image_atomic_int64 {
            v.push(c"VK_EXT_shader_image_atomic_int64");
        }
        if self.khr_shader_quad_control {
            v.push(c"VK_KHR_shader_quad_control");
        }
        if self.khr_spirv_1_4 {
            v.push(c"VK_KHR_spirv_1_4");
        }
        if self.ext_memory_budget {
            v.push(c"VK_EXT_memory_budget");
        }
        if self.ext_memory_priority {
            v.push(c"VK_EXT_memory_priority");
        }
        if self.nv_dedicated_allocation_image_aliasing {
            v.push(c"VK_NV_dedicated_allocation_image_aliasing");
        }
        if self.khr_separate_depth_stencil_layouts {
            v.push(c"VK_KHR_separate_depth_stencil_layouts");
        }
        if self.ext_buffer_device_address {
            v.push(c"VK_EXT_buffer_device_address");
        }
        if self.ext_tooling_info {
            v.push(c"VK_EXT_tooling_info");
        }
        if self.ext_separate_stencil_usage {
            v.push(c"VK_EXT_separate_stencil_usage");
        }
        if self.khr_present_wait {
            v.push(c"VK_KHR_present_wait");
        }
        if self.nv_cooperative_matrix {
            v.push(c"VK_NV_cooperative_matrix");
        }
        if self.nv_coverage_reduction_mode {
            v.push(c"VK_NV_coverage_reduction_mode");
        }
        if self.ext_fragment_shader_interlock {
            v.push(c"VK_EXT_fragment_shader_interlock");
        }
        if self.ext_ycbcr_image_arrays {
            v.push(c"VK_EXT_ycbcr_image_arrays");
        }
        if self.khr_uniform_buffer_standard_layout {
            v.push(c"VK_KHR_uniform_buffer_standard_layout");
        }
        if self.ext_provoking_vertex {
            v.push(c"VK_EXT_provoking_vertex");
        }
        if self.ext_full_screen_exclusive {
            v.push(c"VK_EXT_full_screen_exclusive");
        }
        if self.khr_buffer_device_address {
            v.push(c"VK_KHR_buffer_device_address");
        }
        if self.ext_line_rasterization {
            v.push(c"VK_EXT_line_rasterization");
        }
        if self.ext_shader_atomic_float {
            v.push(c"VK_EXT_shader_atomic_float");
        }
        if self.ext_host_query_reset {
            v.push(c"VK_EXT_host_query_reset");
        }
        if self.ext_index_type_uint8 {
            v.push(c"VK_EXT_index_type_uint8");
        }
        if self.ext_extended_dynamic_state {
            v.push(c"VK_EXT_extended_dynamic_state");
        }
        if self.khr_deferred_host_operations {
            v.push(c"VK_KHR_deferred_host_operations");
        }
        if self.khr_pipeline_executable_properties {
            v.push(c"VK_KHR_pipeline_executable_properties");
        }
        if self.ext_host_image_copy {
            v.push(c"VK_EXT_host_image_copy");
        }
        if self.khr_map_memory2 {
            v.push(c"VK_KHR_map_memory2");
        }
        if self.ext_map_memory_placed {
            v.push(c"VK_EXT_map_memory_placed");
        }
        if self.ext_shader_atomic_float2 {
            v.push(c"VK_EXT_shader_atomic_float2");
        }
        if self.ext_swapchain_maintenance1 {
            v.push(c"VK_EXT_swapchain_maintenance1");
        }
        if self.ext_shader_demote_to_helper_invocation {
            v.push(c"VK_EXT_shader_demote_to_helper_invocation");
        }
        if self.nv_device_generated_commands {
            v.push(c"VK_NV_device_generated_commands");
        }
        if self.nv_inherited_viewport_scissor {
            v.push(c"VK_NV_inherited_viewport_scissor");
        }
        if self.khr_shader_integer_dot_product {
            v.push(c"VK_KHR_shader_integer_dot_product");
        }
        if self.ext_texel_buffer_alignment {
            v.push(c"VK_EXT_texel_buffer_alignment");
        }
        if self.qcom_render_pass_transform {
            v.push(c"VK_QCOM_render_pass_transform");
        }
        if self.ext_depth_bias_control {
            v.push(c"VK_EXT_depth_bias_control");
        }
        if self.ext_device_memory_report {
            v.push(c"VK_EXT_device_memory_report");
        }
        if self.ext_robustness2 {
            v.push(c"VK_EXT_robustness2");
        }
        if self.ext_custom_border_color {
            v.push(c"VK_EXT_custom_border_color");
        }
        if self.ext_texture_compression_astc_3d {
            v.push(c"VK_EXT_texture_compression_astc_3d");
        }
        if self.google_user_type {
            v.push(c"VK_GOOGLE_user_type");
        }
        if self.khr_pipeline_library {
            v.push(c"VK_KHR_pipeline_library");
        }
        if self.nv_present_barrier {
            v.push(c"VK_NV_present_barrier");
        }
        if self.khr_shader_non_semantic_info {
            v.push(c"VK_KHR_shader_non_semantic_info");
        }
        if self.khr_present_id {
            v.push(c"VK_KHR_present_id");
        }
        if self.ext_private_data {
            v.push(c"VK_EXT_private_data");
        }
        if self.ext_pipeline_creation_cache_control {
            v.push(c"VK_EXT_pipeline_creation_cache_control");
        }
        if self.nv_device_diagnostics_config {
            v.push(c"VK_NV_device_diagnostics_config");
        }
        if self.qcom_render_pass_store_ops {
            v.push(c"VK_QCOM_render_pass_store_ops");
        }
        if self.nv_cuda_kernel_launch {
            v.push(c"VK_NV_cuda_kernel_launch");
        }
        if self.qcom_tile_shading {
            v.push(c"VK_QCOM_tile_shading");
        }
        if self.nv_low_latency {
            v.push(c"VK_NV_low_latency");
        }
        if self.ext_metal_objects {
            v.push(c"VK_EXT_metal_objects");
        }
        if self.khr_synchronization2 {
            v.push(c"VK_KHR_synchronization2");
        }
        if self.ext_descriptor_buffer {
            v.push(c"VK_EXT_descriptor_buffer");
        }
        if self.ext_graphics_pipeline_library {
            v.push(c"VK_EXT_graphics_pipeline_library");
        }
        if self.amd_shader_early_and_late_fragment_tests {
            v.push(c"VK_AMD_shader_early_and_late_fragment_tests");
        }
        if self.khr_fragment_shader_barycentric {
            v.push(c"VK_KHR_fragment_shader_barycentric");
        }
        if self.khr_shader_subgroup_uniform_control_flow {
            v.push(c"VK_KHR_shader_subgroup_uniform_control_flow");
        }
        if self.khr_zero_initialize_workgroup_memory {
            v.push(c"VK_KHR_zero_initialize_workgroup_memory");
        }
        if self.nv_fragment_shading_rate_enums {
            v.push(c"VK_NV_fragment_shading_rate_enums");
        }
        if self.nv_ray_tracing_motion_blur {
            v.push(c"VK_NV_ray_tracing_motion_blur");
        }
        if self.ext_mesh_shader {
            v.push(c"VK_EXT_mesh_shader");
        }
        if self.ext_ycbcr_2plane_444_formats {
            v.push(c"VK_EXT_ycbcr_2plane_444_formats");
        }
        if self.ext_fragment_density_map2 {
            v.push(c"VK_EXT_fragment_density_map2");
        }
        if self.qcom_rotated_copy_commands {
            v.push(c"VK_QCOM_rotated_copy_commands");
        }
        if self.ext_image_robustness {
            v.push(c"VK_EXT_image_robustness");
        }
        if self.khr_workgroup_memory_explicit_layout {
            v.push(c"VK_KHR_workgroup_memory_explicit_layout");
        }
        if self.khr_copy_commands2 {
            v.push(c"VK_KHR_copy_commands2");
        }
        if self.ext_image_compression_control {
            v.push(c"VK_EXT_image_compression_control");
        }
        if self.ext_attachment_feedback_loop_layout {
            v.push(c"VK_EXT_attachment_feedback_loop_layout");
        }
        if self.ext_4444_formats {
            v.push(c"VK_EXT_4444_formats");
        }
        if self.ext_device_fault {
            v.push(c"VK_EXT_device_fault");
        }
        if self.arm_rasterization_order_attachment_access {
            v.push(c"VK_ARM_rasterization_order_attachment_access");
        }
        if self.ext_rgba10x6_formats {
            v.push(c"VK_EXT_rgba10x6_formats");
        }
        if self.nv_acquire_winrt_display {
            v.push(c"VK_NV_acquire_winrt_display");
        }
        if self.valve_mutable_descriptor_type {
            v.push(c"VK_VALVE_mutable_descriptor_type");
        }
        if self.ext_vertex_input_dynamic_state {
            v.push(c"VK_EXT_vertex_input_dynamic_state");
        }
        if self.ext_physical_device_drm {
            v.push(c"VK_EXT_physical_device_drm");
        }
        if self.ext_device_address_binding_report {
            v.push(c"VK_EXT_device_address_binding_report");
        }
        if self.ext_depth_clip_control {
            v.push(c"VK_EXT_depth_clip_control");
        }
        if self.ext_primitive_topology_list_restart {
            v.push(c"VK_EXT_primitive_topology_list_restart");
        }
        if self.khr_format_feature_flags2 {
            v.push(c"VK_KHR_format_feature_flags2");
        }
        if self.ext_present_mode_fifo_latest_ready {
            v.push(c"VK_EXT_present_mode_fifo_latest_ready");
        }
        if self.fuchsia_external_memory {
            v.push(c"VK_FUCHSIA_external_memory");
        }
        if self.fuchsia_external_semaphore {
            v.push(c"VK_FUCHSIA_external_semaphore");
        }
        if self.fuchsia_buffer_collection {
            v.push(c"VK_FUCHSIA_buffer_collection");
        }
        if self.huawei_subpass_shading {
            v.push(c"VK_HUAWEI_subpass_shading");
        }
        if self.huawei_invocation_mask {
            v.push(c"VK_HUAWEI_invocation_mask");
        }
        if self.nv_external_memory_rdma {
            v.push(c"VK_NV_external_memory_rdma");
        }
        if self.ext_pipeline_properties {
            v.push(c"VK_EXT_pipeline_properties");
        }
        if self.ext_frame_boundary {
            v.push(c"VK_EXT_frame_boundary");
        }
        if self.ext_multisampled_render_to_single_sampled {
            v.push(c"VK_EXT_multisampled_render_to_single_sampled");
        }
        if self.ext_extended_dynamic_state2 {
            v.push(c"VK_EXT_extended_dynamic_state2");
        }
        if self.ext_color_write_enable {
            v.push(c"VK_EXT_color_write_enable");
        }
        if self.ext_primitives_generated_query {
            v.push(c"VK_EXT_primitives_generated_query");
        }
        if self.khr_ray_tracing_maintenance1 {
            v.push(c"VK_KHR_ray_tracing_maintenance1");
        }
        if self.khr_shader_untyped_pointers {
            v.push(c"VK_KHR_shader_untyped_pointers");
        }
        if self.ext_global_priority_query {
            v.push(c"VK_EXT_global_priority_query");
        }
        if self.ext_image_view_min_lod {
            v.push(c"VK_EXT_image_view_min_lod");
        }
        if self.ext_multi_draw {
            v.push(c"VK_EXT_multi_draw");
        }
        if self.ext_image_2d_view_of_3d {
            v.push(c"VK_EXT_image_2d_view_of_3d");
        }
        if self.ext_shader_tile_image {
            v.push(c"VK_EXT_shader_tile_image");
        }
        if self.ext_opacity_micromap {
            v.push(c"VK_EXT_opacity_micromap");
        }
        if self.nv_displacement_micromap {
            v.push(c"VK_NV_displacement_micromap");
        }
        if self.ext_load_store_op_none {
            v.push(c"VK_EXT_load_store_op_none");
        }
        if self.huawei_cluster_culling_shader {
            v.push(c"VK_HUAWEI_cluster_culling_shader");
        }
        if self.ext_border_color_swizzle {
            v.push(c"VK_EXT_border_color_swizzle");
        }
        if self.ext_pageable_device_local_memory {
            v.push(c"VK_EXT_pageable_device_local_memory");
        }
        if self.khr_maintenance4 {
            v.push(c"VK_KHR_maintenance4");
        }
        if self.arm_shader_core_properties {
            v.push(c"VK_ARM_shader_core_properties");
        }
        if self.khr_shader_subgroup_rotate {
            v.push(c"VK_KHR_shader_subgroup_rotate");
        }
        if self.arm_scheduling_controls {
            v.push(c"VK_ARM_scheduling_controls");
        }
        if self.ext_image_sliced_view_of_3d {
            v.push(c"VK_EXT_image_sliced_view_of_3d");
        }
        if self.valve_descriptor_set_host_mapping {
            v.push(c"VK_VALVE_descriptor_set_host_mapping");
        }
        if self.ext_depth_clamp_zero_one {
            v.push(c"VK_EXT_depth_clamp_zero_one");
        }
        if self.ext_non_seamless_cube_map {
            v.push(c"VK_EXT_non_seamless_cube_map");
        }
        if self.arm_render_pass_striped {
            v.push(c"VK_ARM_render_pass_striped");
        }
        if self.qcom_fragment_density_map_offset {
            v.push(c"VK_QCOM_fragment_density_map_offset");
        }
        if self.nv_copy_memory_indirect {
            v.push(c"VK_NV_copy_memory_indirect");
        }
        if self.nv_memory_decompression {
            v.push(c"VK_NV_memory_decompression");
        }
        if self.nv_device_generated_commands_compute {
            v.push(c"VK_NV_device_generated_commands_compute");
        }
        if self.nv_ray_tracing_linear_swept_spheres {
            v.push(c"VK_NV_ray_tracing_linear_swept_spheres");
        }
        if self.nv_linear_color_attachment {
            v.push(c"VK_NV_linear_color_attachment");
        }
        if self.khr_shader_maximal_reconvergence {
            v.push(c"VK_KHR_shader_maximal_reconvergence");
        }
        if self.ext_image_compression_control_swapchain {
            v.push(c"VK_EXT_image_compression_control_swapchain");
        }
        if self.qcom_image_processing {
            v.push(c"VK_QCOM_image_processing");
        }
        if self.ext_nested_command_buffer {
            v.push(c"VK_EXT_nested_command_buffer");
        }
        if self.ohos_external_memory {
            v.push(c"VK_OHOS_external_memory");
        }
        if self.ext_external_memory_acquire_unmodified {
            v.push(c"VK_EXT_external_memory_acquire_unmodified");
        }
        if self.ext_extended_dynamic_state3 {
            v.push(c"VK_EXT_extended_dynamic_state3");
        }
        if self.ext_subpass_merge_feedback {
            v.push(c"VK_EXT_subpass_merge_feedback");
        }
        if self.arm_tensors {
            v.push(c"VK_ARM_tensors");
        }
        if self.ext_shader_module_identifier {
            v.push(c"VK_EXT_shader_module_identifier");
        }
        if self.ext_rasterization_order_attachment_access {
            v.push(c"VK_EXT_rasterization_order_attachment_access");
        }
        if self.nv_optical_flow {
            v.push(c"VK_NV_optical_flow");
        }
        if self.ext_legacy_dithering {
            v.push(c"VK_EXT_legacy_dithering");
        }
        if self.ext_pipeline_protected_access {
            v.push(c"VK_EXT_pipeline_protected_access");
        }
        if self.android_external_format_resolve {
            v.push(c"VK_ANDROID_external_format_resolve");
        }
        if self.khr_maintenance5 {
            v.push(c"VK_KHR_maintenance5");
        }
        if self.amd_anti_lag {
            v.push(c"VK_AMD_anti_lag");
        }
        if self.amdx_dense_geometry_format {
            v.push(c"VK_AMDX_dense_geometry_format");
        }
        if self.khr_present_id2 {
            v.push(c"VK_KHR_present_id2");
        }
        if self.khr_present_wait2 {
            v.push(c"VK_KHR_present_wait2");
        }
        if self.khr_ray_tracing_position_fetch {
            v.push(c"VK_KHR_ray_tracing_position_fetch");
        }
        if self.ext_shader_object {
            v.push(c"VK_EXT_shader_object");
        }
        if self.khr_pipeline_binary {
            v.push(c"VK_KHR_pipeline_binary");
        }
        if self.qcom_tile_properties {
            v.push(c"VK_QCOM_tile_properties");
        }
        if self.sec_amigo_profiling {
            v.push(c"VK_SEC_amigo_profiling");
        }
        if self.khr_swapchain_maintenance1 {
            v.push(c"VK_KHR_swapchain_maintenance1");
        }
        if self.qcom_multiview_per_view_viewports {
            v.push(c"VK_QCOM_multiview_per_view_viewports");
        }
        if self.nv_ray_tracing_invocation_reorder {
            v.push(c"VK_NV_ray_tracing_invocation_reorder");
        }
        if self.nv_cooperative_vector {
            v.push(c"VK_NV_cooperative_vector");
        }
        if self.nv_extended_sparse_address_space {
            v.push(c"VK_NV_extended_sparse_address_space");
        }
        if self.ext_mutable_descriptor_type {
            v.push(c"VK_EXT_mutable_descriptor_type");
        }
        if self.ext_legacy_vertex_attributes {
            v.push(c"VK_EXT_legacy_vertex_attributes");
        }
        if self.arm_shader_core_builtins {
            v.push(c"VK_ARM_shader_core_builtins");
        }
        if self.ext_pipeline_library_group_handles {
            v.push(c"VK_EXT_pipeline_library_group_handles");
        }
        if self.ext_dynamic_rendering_unused_attachments {
            v.push(c"VK_EXT_dynamic_rendering_unused_attachments");
        }
        if self.khr_internally_synchronized_queues {
            v.push(c"VK_KHR_internally_synchronized_queues");
        }
        if self.nv_low_latency2 {
            v.push(c"VK_NV_low_latency2");
        }
        if self.khr_cooperative_matrix {
            v.push(c"VK_KHR_cooperative_matrix");
        }
        if self.arm_data_graph {
            v.push(c"VK_ARM_data_graph");
        }
        if self.qcom_multiview_per_view_render_areas {
            v.push(c"VK_QCOM_multiview_per_view_render_areas");
        }
        if self.khr_compute_shader_derivatives {
            v.push(c"VK_KHR_compute_shader_derivatives");
        }
        if self.nv_per_stage_descriptor_set {
            v.push(c"VK_NV_per_stage_descriptor_set");
        }
        if self.qcom_image_processing2 {
            v.push(c"VK_QCOM_image_processing2");
        }
        if self.qcom_filter_cubic_weights {
            v.push(c"VK_QCOM_filter_cubic_weights");
        }
        if self.qcom_ycbcr_degamma {
            v.push(c"VK_QCOM_ycbcr_degamma");
        }
        if self.qcom_filter_cubic_clamp {
            v.push(c"VK_QCOM_filter_cubic_clamp");
        }
        if self.ext_attachment_feedback_loop_dynamic_state {
            v.push(c"VK_EXT_attachment_feedback_loop_dynamic_state");
        }
        if self.khr_vertex_attribute_divisor {
            v.push(c"VK_KHR_vertex_attribute_divisor");
        }
        if self.khr_load_store_op_none {
            v.push(c"VK_KHR_load_store_op_none");
        }
        if self.khr_unified_image_layouts {
            v.push(c"VK_KHR_unified_image_layouts");
        }
        if self.khr_shader_float_controls2 {
            v.push(c"VK_KHR_shader_float_controls2");
        }
        if self.msft_layered_driver {
            v.push(c"VK_MSFT_layered_driver");
        }
        if self.khr_index_type_uint8 {
            v.push(c"VK_KHR_index_type_uint8");
        }
        if self.khr_line_rasterization {
            v.push(c"VK_KHR_line_rasterization");
        }
        if self.khr_calibrated_timestamps {
            v.push(c"VK_KHR_calibrated_timestamps");
        }
        if self.khr_shader_expect_assume {
            v.push(c"VK_KHR_shader_expect_assume");
        }
        if self.khr_maintenance6 {
            v.push(c"VK_KHR_maintenance6");
        }
        if self.nv_descriptor_pool_overallocation {
            v.push(c"VK_NV_descriptor_pool_overallocation");
        }
        if self.qcom_tile_memory_heap {
            v.push(c"VK_QCOM_tile_memory_heap");
        }
        if self.khr_copy_memory_indirect {
            v.push(c"VK_KHR_copy_memory_indirect");
        }
        if self.ext_memory_decompression {
            v.push(c"VK_EXT_memory_decompression");
        }
        if self.nv_raw_access_chains {
            v.push(c"VK_NV_raw_access_chains");
        }
        if self.nv_external_compute_queue {
            v.push(c"VK_NV_external_compute_queue");
        }
        if self.khr_shader_relaxed_extended_instruction {
            v.push(c"VK_KHR_shader_relaxed_extended_instruction");
        }
        if self.nv_command_buffer_inheritance {
            v.push(c"VK_NV_command_buffer_inheritance");
        }
        if self.khr_maintenance7 {
            v.push(c"VK_KHR_maintenance7");
        }
        if self.nv_shader_atomic_float16_vector {
            v.push(c"VK_NV_shader_atomic_float16_vector");
        }
        if self.ext_shader_replicated_composites {
            v.push(c"VK_EXT_shader_replicated_composites");
        }
        if self.ext_shader_float8 {
            v.push(c"VK_EXT_shader_float8");
        }
        if self.nv_ray_tracing_validation {
            v.push(c"VK_NV_ray_tracing_validation");
        }
        if self.nv_cluster_acceleration_structure {
            v.push(c"VK_NV_cluster_acceleration_structure");
        }
        if self.nv_partitioned_acceleration_structure {
            v.push(c"VK_NV_partitioned_acceleration_structure");
        }
        if self.ext_device_generated_commands {
            v.push(c"VK_EXT_device_generated_commands");
        }
        if self.khr_maintenance8 {
            v.push(c"VK_KHR_maintenance8");
        }
        if self.mesa_image_alignment_control {
            v.push(c"VK_MESA_image_alignment_control");
        }
        if self.khr_shader_fma {
            v.push(c"VK_KHR_shader_fma");
        }
        if self.nv_push_constant_bank {
            v.push(c"VK_NV_push_constant_bank");
        }
        if self.ext_ray_tracing_invocation_reorder {
            v.push(c"VK_EXT_ray_tracing_invocation_reorder");
        }
        if self.ext_depth_clamp_control {
            v.push(c"VK_EXT_depth_clamp_control");
        }
        if self.khr_maintenance9 {
            v.push(c"VK_KHR_maintenance9");
        }
        if self.huawei_hdr_vivid {
            v.push(c"VK_HUAWEI_hdr_vivid");
        }
        if self.nv_cooperative_matrix2 {
            v.push(c"VK_NV_cooperative_matrix2");
        }
        if self.arm_pipeline_opacity_micromap {
            v.push(c"VK_ARM_pipeline_opacity_micromap");
        }
        if self.ext_external_memory_metal {
            v.push(c"VK_EXT_external_memory_metal");
        }
        if self.khr_depth_clamp_zero_one {
            v.push(c"VK_KHR_depth_clamp_zero_one");
        }
        if self.arm_performance_counters_by_region {
            v.push(c"VK_ARM_performance_counters_by_region");
        }
        if self.ext_vertex_attribute_robustness {
            v.push(c"VK_EXT_vertex_attribute_robustness");
        }
        if self.arm_format_pack {
            v.push(c"VK_ARM_format_pack");
        }
        if self.valve_fragment_density_map_layered {
            v.push(c"VK_VALVE_fragment_density_map_layered");
        }
        if self.khr_robustness2 {
            v.push(c"VK_KHR_robustness2");
        }
        if self.nv_present_metering {
            v.push(c"VK_NV_present_metering");
        }
        if self.ext_fragment_density_map_offset {
            v.push(c"VK_EXT_fragment_density_map_offset");
        }
        if self.ext_zero_initialize_device_memory {
            v.push(c"VK_EXT_zero_initialize_device_memory");
        }
        if self.khr_present_mode_fifo_latest_ready {
            v.push(c"VK_KHR_present_mode_fifo_latest_ready");
        }
        if self.ext_shader_64bit_indexing {
            v.push(c"VK_EXT_shader_64bit_indexing");
        }
        if self.ext_custom_resolve {
            v.push(c"VK_EXT_custom_resolve");
        }
        if self.qcom_data_graph_model {
            v.push(c"VK_QCOM_data_graph_model");
        }
        if self.khr_maintenance10 {
            v.push(c"VK_KHR_maintenance10");
        }
        if self.ext_shader_long_vector {
            v.push(c"VK_EXT_shader_long_vector");
        }
        if self.sec_pipeline_cache_incremental_mode {
            v.push(c"VK_SEC_pipeline_cache_incremental_mode");
        }
        if self.ext_shader_uniform_buffer_unsized_array {
            v.push(c"VK_EXT_shader_uniform_buffer_unsized_array");
        }
        if self.nv_compute_occupancy_priority {
            v.push(c"VK_NV_compute_occupancy_priority");
        }
        if self.ext_shader_subgroup_partitioned {
            v.push(c"VK_EXT_shader_subgroup_partitioned");
        }
        if self.valve_shader_mixed_float_dot_product {
            v.push(c"VK_VALVE_shader_mixed_float_dot_product");
        }
        v
    }
}

#[derive(Copy, Clone)]
pub struct Device {
    pub handle: vk::Device,
    pub extensions: DeviceExtensions,
    pub fp_destroy_device: vk::FnDestroyDevice,
    pub fp_get_device_queue: vk::FnGetDeviceQueue,
    pub fp_queue_submit: vk::FnQueueSubmit,
    pub fp_queue_wait_idle: vk::FnQueueWaitIdle,
    pub fp_device_wait_idle: vk::FnDeviceWaitIdle,
    pub fp_allocate_memory: vk::FnAllocateMemory,
    pub fp_free_memory: vk::FnFreeMemory,
    pub fp_map_memory: vk::FnMapMemory,
    pub fp_unmap_memory: vk::FnUnmapMemory,
    pub fp_flush_mapped_memory_ranges: vk::FnFlushMappedMemoryRanges,
    pub fp_invalidate_mapped_memory_ranges: vk::FnInvalidateMappedMemoryRanges,
    pub fp_get_device_memory_commitment: vk::FnGetDeviceMemoryCommitment,
    pub fp_get_buffer_memory_requirements: vk::FnGetBufferMemoryRequirements,
    pub fp_bind_buffer_memory: vk::FnBindBufferMemory,
    pub fp_get_image_memory_requirements: vk::FnGetImageMemoryRequirements,
    pub fp_bind_image_memory: vk::FnBindImageMemory,
    pub fp_get_image_sparse_memory_requirements: vk::FnGetImageSparseMemoryRequirements,
    pub fp_queue_bind_sparse: vk::FnQueueBindSparse,
    pub fp_create_fence: vk::FnCreateFence,
    pub fp_destroy_fence: vk::FnDestroyFence,
    pub fp_reset_fences: vk::FnResetFences,
    pub fp_get_fence_status: vk::FnGetFenceStatus,
    pub fp_wait_for_fences: vk::FnWaitForFences,
    pub fp_create_semaphore: vk::FnCreateSemaphore,
    pub fp_destroy_semaphore: vk::FnDestroySemaphore,
    pub fp_create_event: vk::FnCreateEvent,
    pub fp_destroy_event: vk::FnDestroyEvent,
    pub fp_get_event_status: vk::FnGetEventStatus,
    pub fp_set_event: vk::FnSetEvent,
    pub fp_reset_event: vk::FnResetEvent,
    pub fp_create_query_pool: vk::FnCreateQueryPool,
    pub fp_destroy_query_pool: vk::FnDestroyQueryPool,
    pub fp_get_query_pool_results: vk::FnGetQueryPoolResults,
    pub fp_reset_query_pool: Option<vk::FnResetQueryPool>,
    pub fp_create_buffer: vk::FnCreateBuffer,
    pub fp_destroy_buffer: vk::FnDestroyBuffer,
    pub fp_create_buffer_view: vk::FnCreateBufferView,
    pub fp_destroy_buffer_view: vk::FnDestroyBufferView,
    pub fp_create_image: vk::FnCreateImage,
    pub fp_destroy_image: vk::FnDestroyImage,
    pub fp_get_image_subresource_layout: vk::FnGetImageSubresourceLayout,
    pub fp_create_image_view: vk::FnCreateImageView,
    pub fp_destroy_image_view: vk::FnDestroyImageView,
    pub fp_create_shader_module: vk::FnCreateShaderModule,
    pub fp_destroy_shader_module: vk::FnDestroyShaderModule,
    pub fp_create_pipeline_cache: vk::FnCreatePipelineCache,
    pub fp_destroy_pipeline_cache: vk::FnDestroyPipelineCache,
    pub fp_get_pipeline_cache_data: vk::FnGetPipelineCacheData,
    pub fp_merge_pipeline_caches: vk::FnMergePipelineCaches,
    pub fp_create_pipeline_binaries_khr: Option<vk::FnCreatePipelineBinariesKHR>,
    pub fp_destroy_pipeline_binary_khr: Option<vk::FnDestroyPipelineBinaryKHR>,
    pub fp_get_pipeline_key_khr: Option<vk::FnGetPipelineKeyKHR>,
    pub fp_get_pipeline_binary_data_khr: Option<vk::FnGetPipelineBinaryDataKHR>,
    pub fp_release_captured_pipeline_data_khr: Option<vk::FnReleaseCapturedPipelineDataKHR>,
    pub fp_create_graphics_pipelines: vk::FnCreateGraphicsPipelines,
    pub fp_create_compute_pipelines: vk::FnCreateComputePipelines,
    pub fp_get_device_subpass_shading_max_workgroup_size_huawei:
        Option<vk::FnGetDeviceSubpassShadingMaxWorkgroupSizeHUAWEI>,
    pub fp_destroy_pipeline: vk::FnDestroyPipeline,
    pub fp_create_pipeline_layout: vk::FnCreatePipelineLayout,
    pub fp_destroy_pipeline_layout: vk::FnDestroyPipelineLayout,
    pub fp_create_sampler: vk::FnCreateSampler,
    pub fp_destroy_sampler: vk::FnDestroySampler,
    pub fp_create_descriptor_set_layout: vk::FnCreateDescriptorSetLayout,
    pub fp_destroy_descriptor_set_layout: vk::FnDestroyDescriptorSetLayout,
    pub fp_create_descriptor_pool: vk::FnCreateDescriptorPool,
    pub fp_destroy_descriptor_pool: vk::FnDestroyDescriptorPool,
    pub fp_reset_descriptor_pool: vk::FnResetDescriptorPool,
    pub fp_allocate_descriptor_sets: vk::FnAllocateDescriptorSets,
    pub fp_free_descriptor_sets: vk::FnFreeDescriptorSets,
    pub fp_update_descriptor_sets: vk::FnUpdateDescriptorSets,
    pub fp_create_framebuffer: vk::FnCreateFramebuffer,
    pub fp_destroy_framebuffer: vk::FnDestroyFramebuffer,
    pub fp_create_render_pass: vk::FnCreateRenderPass,
    pub fp_destroy_render_pass: vk::FnDestroyRenderPass,
    pub fp_get_render_area_granularity: vk::FnGetRenderAreaGranularity,
    pub fp_get_rendering_area_granularity: Option<vk::FnGetRenderingAreaGranularity>,
    pub fp_create_command_pool: vk::FnCreateCommandPool,
    pub fp_destroy_command_pool: vk::FnDestroyCommandPool,
    pub fp_reset_command_pool: vk::FnResetCommandPool,
    pub fp_allocate_command_buffers: vk::FnAllocateCommandBuffers,
    pub fp_free_command_buffers: vk::FnFreeCommandBuffers,
    pub fp_begin_command_buffer: vk::FnBeginCommandBuffer,
    pub fp_end_command_buffer: vk::FnEndCommandBuffer,
    pub fp_reset_command_buffer: vk::FnResetCommandBuffer,
    pub fp_cmd_bind_pipeline: vk::FnCmdBindPipeline,
    pub fp_cmd_set_attachment_feedback_loop_enable_ext: Option<vk::FnCmdSetAttachmentFeedbackLoopEnableEXT>,
    pub fp_cmd_set_viewport: vk::FnCmdSetViewport,
    pub fp_cmd_set_scissor: vk::FnCmdSetScissor,
    pub fp_cmd_set_line_width: vk::FnCmdSetLineWidth,
    pub fp_cmd_set_depth_bias: vk::FnCmdSetDepthBias,
    pub fp_cmd_set_blend_constants: vk::FnCmdSetBlendConstants,
    pub fp_cmd_set_depth_bounds: vk::FnCmdSetDepthBounds,
    pub fp_cmd_set_stencil_compare_mask: vk::FnCmdSetStencilCompareMask,
    pub fp_cmd_set_stencil_write_mask: vk::FnCmdSetStencilWriteMask,
    pub fp_cmd_set_stencil_reference: vk::FnCmdSetStencilReference,
    pub fp_cmd_bind_descriptor_sets: vk::FnCmdBindDescriptorSets,
    pub fp_cmd_bind_index_buffer: vk::FnCmdBindIndexBuffer,
    pub fp_cmd_bind_vertex_buffers: vk::FnCmdBindVertexBuffers,
    pub fp_cmd_draw: vk::FnCmdDraw,
    pub fp_cmd_draw_indexed: vk::FnCmdDrawIndexed,
    pub fp_cmd_draw_multi_ext: Option<vk::FnCmdDrawMultiEXT>,
    pub fp_cmd_draw_multi_indexed_ext: Option<vk::FnCmdDrawMultiIndexedEXT>,
    pub fp_cmd_draw_indirect: vk::FnCmdDrawIndirect,
    pub fp_cmd_draw_indexed_indirect: vk::FnCmdDrawIndexedIndirect,
    pub fp_cmd_dispatch: vk::FnCmdDispatch,
    pub fp_cmd_dispatch_indirect: vk::FnCmdDispatchIndirect,
    pub fp_cmd_subpass_shading_huawei: Option<vk::FnCmdSubpassShadingHUAWEI>,
    pub fp_cmd_draw_cluster_huawei: Option<vk::FnCmdDrawClusterHUAWEI>,
    pub fp_cmd_draw_cluster_indirect_huawei: Option<vk::FnCmdDrawClusterIndirectHUAWEI>,
    pub fp_cmd_update_pipeline_indirect_buffer_nv: Option<vk::FnCmdUpdatePipelineIndirectBufferNV>,
    pub fp_cmd_copy_buffer: vk::FnCmdCopyBuffer,
    pub fp_cmd_copy_image: vk::FnCmdCopyImage,
    pub fp_cmd_blit_image: vk::FnCmdBlitImage,
    pub fp_cmd_copy_buffer_to_image: vk::FnCmdCopyBufferToImage,
    pub fp_cmd_copy_image_to_buffer: vk::FnCmdCopyImageToBuffer,
    pub fp_cmd_copy_memory_indirect_nv: Option<vk::FnCmdCopyMemoryIndirectNV>,
    pub fp_cmd_copy_memory_indirect_khr: Option<vk::FnCmdCopyMemoryIndirectKHR>,
    pub fp_cmd_copy_memory_to_image_indirect_nv: Option<vk::FnCmdCopyMemoryToImageIndirectNV>,
    pub fp_cmd_copy_memory_to_image_indirect_khr: Option<vk::FnCmdCopyMemoryToImageIndirectKHR>,
    pub fp_cmd_update_buffer: vk::FnCmdUpdateBuffer,
    pub fp_cmd_fill_buffer: vk::FnCmdFillBuffer,
    pub fp_cmd_clear_color_image: vk::FnCmdClearColorImage,
    pub fp_cmd_clear_depth_stencil_image: vk::FnCmdClearDepthStencilImage,
    pub fp_cmd_clear_attachments: vk::FnCmdClearAttachments,
    pub fp_cmd_resolve_image: vk::FnCmdResolveImage,
    pub fp_cmd_set_event: vk::FnCmdSetEvent,
    pub fp_cmd_reset_event: vk::FnCmdResetEvent,
    pub fp_cmd_wait_events: vk::FnCmdWaitEvents,
    pub fp_cmd_pipeline_barrier: vk::FnCmdPipelineBarrier,
    pub fp_cmd_begin_query: vk::FnCmdBeginQuery,
    pub fp_cmd_end_query: vk::FnCmdEndQuery,
    pub fp_cmd_begin_conditional_rendering_ext: Option<vk::FnCmdBeginConditionalRenderingEXT>,
    pub fp_cmd_end_conditional_rendering_ext: Option<vk::FnCmdEndConditionalRenderingEXT>,
    pub fp_cmd_begin_custom_resolve_ext: Option<vk::FnCmdBeginCustomResolveEXT>,
    pub fp_cmd_reset_query_pool: vk::FnCmdResetQueryPool,
    pub fp_cmd_write_timestamp: vk::FnCmdWriteTimestamp,
    pub fp_cmd_copy_query_pool_results: vk::FnCmdCopyQueryPoolResults,
    pub fp_cmd_push_constants: vk::FnCmdPushConstants,
    pub fp_cmd_begin_render_pass: vk::FnCmdBeginRenderPass,
    pub fp_cmd_next_subpass: vk::FnCmdNextSubpass,
    pub fp_cmd_end_render_pass: vk::FnCmdEndRenderPass,
    pub fp_cmd_execute_commands: vk::FnCmdExecuteCommands,
    pub fp_create_shared_swapchains_khr: Option<vk::FnCreateSharedSwapchainsKHR>,
    pub fp_create_swapchain_khr: Option<vk::FnCreateSwapchainKHR>,
    pub fp_destroy_swapchain_khr: Option<vk::FnDestroySwapchainKHR>,
    pub fp_get_swapchain_images_khr: Option<vk::FnGetSwapchainImagesKHR>,
    pub fp_acquire_next_image_khr: Option<vk::FnAcquireNextImageKHR>,
    pub fp_queue_present_khr: Option<vk::FnQueuePresentKHR>,
    pub fp_debug_marker_set_object_name_ext: Option<vk::FnDebugMarkerSetObjectNameEXT>,
    pub fp_debug_marker_set_object_tag_ext: Option<vk::FnDebugMarkerSetObjectTagEXT>,
    pub fp_cmd_debug_marker_begin_ext: Option<vk::FnCmdDebugMarkerBeginEXT>,
    pub fp_cmd_debug_marker_end_ext: Option<vk::FnCmdDebugMarkerEndEXT>,
    pub fp_cmd_debug_marker_insert_ext: Option<vk::FnCmdDebugMarkerInsertEXT>,
    pub fp_get_memory_win32_handle_nv: Option<vk::FnGetMemoryWin32HandleNV>,
    pub fp_cmd_execute_generated_commands_nv: Option<vk::FnCmdExecuteGeneratedCommandsNV>,
    pub fp_cmd_preprocess_generated_commands_nv: Option<vk::FnCmdPreprocessGeneratedCommandsNV>,
    pub fp_cmd_bind_pipeline_shader_group_nv: Option<vk::FnCmdBindPipelineShaderGroupNV>,
    pub fp_get_generated_commands_memory_requirements_nv: Option<vk::FnGetGeneratedCommandsMemoryRequirementsNV>,
    pub fp_create_indirect_commands_layout_nv: Option<vk::FnCreateIndirectCommandsLayoutNV>,
    pub fp_destroy_indirect_commands_layout_nv: Option<vk::FnDestroyIndirectCommandsLayoutNV>,
    pub fp_cmd_execute_generated_commands_ext: Option<vk::FnCmdExecuteGeneratedCommandsEXT>,
    pub fp_cmd_preprocess_generated_commands_ext: Option<vk::FnCmdPreprocessGeneratedCommandsEXT>,
    pub fp_get_generated_commands_memory_requirements_ext: Option<vk::FnGetGeneratedCommandsMemoryRequirementsEXT>,
    pub fp_create_indirect_commands_layout_ext: Option<vk::FnCreateIndirectCommandsLayoutEXT>,
    pub fp_destroy_indirect_commands_layout_ext: Option<vk::FnDestroyIndirectCommandsLayoutEXT>,
    pub fp_create_indirect_execution_set_ext: Option<vk::FnCreateIndirectExecutionSetEXT>,
    pub fp_destroy_indirect_execution_set_ext: Option<vk::FnDestroyIndirectExecutionSetEXT>,
    pub fp_update_indirect_execution_set_pipeline_ext: Option<vk::FnUpdateIndirectExecutionSetPipelineEXT>,
    pub fp_update_indirect_execution_set_shader_ext: Option<vk::FnUpdateIndirectExecutionSetShaderEXT>,
    pub fp_cmd_push_descriptor_set: Option<vk::FnCmdPushDescriptorSet>,
    pub fp_trim_command_pool: Option<vk::FnTrimCommandPool>,
    pub fp_get_memory_win32_handle_khr: Option<vk::FnGetMemoryWin32HandleKHR>,
    pub fp_get_memory_win32_handle_properties_khr: Option<vk::FnGetMemoryWin32HandlePropertiesKHR>,
    pub fp_get_memory_fd_khr: Option<vk::FnGetMemoryFdKHR>,
    pub fp_get_memory_fd_properties_khr: Option<vk::FnGetMemoryFdPropertiesKHR>,
    pub fp_get_memory_zircon_handle_fuchsia: Option<vk::FnGetMemoryZirconHandleFUCHSIA>,
    pub fp_get_memory_zircon_handle_properties_fuchsia: Option<vk::FnGetMemoryZirconHandlePropertiesFUCHSIA>,
    pub fp_get_memory_remote_address_nv: Option<vk::FnGetMemoryRemoteAddressNV>,
    pub fp_get_semaphore_win32_handle_khr: Option<vk::FnGetSemaphoreWin32HandleKHR>,
    pub fp_import_semaphore_win32_handle_khr: Option<vk::FnImportSemaphoreWin32HandleKHR>,
    pub fp_get_semaphore_fd_khr: Option<vk::FnGetSemaphoreFdKHR>,
    pub fp_import_semaphore_fd_khr: Option<vk::FnImportSemaphoreFdKHR>,
    pub fp_get_semaphore_zircon_handle_fuchsia: Option<vk::FnGetSemaphoreZirconHandleFUCHSIA>,
    pub fp_import_semaphore_zircon_handle_fuchsia: Option<vk::FnImportSemaphoreZirconHandleFUCHSIA>,
    pub fp_get_fence_win32_handle_khr: Option<vk::FnGetFenceWin32HandleKHR>,
    pub fp_import_fence_win32_handle_khr: Option<vk::FnImportFenceWin32HandleKHR>,
    pub fp_get_fence_fd_khr: Option<vk::FnGetFenceFdKHR>,
    pub fp_import_fence_fd_khr: Option<vk::FnImportFenceFdKHR>,
    pub fp_acquire_winrt_display_nv: Option<vk::FnAcquireWinrtDisplayNV>,
    pub fp_get_winrt_display_nv: Option<vk::FnGetWinrtDisplayNV>,
    pub fp_display_power_control_ext: Option<vk::FnDisplayPowerControlEXT>,
    pub fp_register_device_event_ext: Option<vk::FnRegisterDeviceEventEXT>,
    pub fp_register_display_event_ext: Option<vk::FnRegisterDisplayEventEXT>,
    pub fp_get_swapchain_counter_ext: Option<vk::FnGetSwapchainCounterEXT>,
    pub fp_get_device_group_peer_memory_features: Option<vk::FnGetDeviceGroupPeerMemoryFeatures>,
    pub fp_bind_buffer_memory2: Option<vk::FnBindBufferMemory2>,
    pub fp_bind_image_memory2: Option<vk::FnBindImageMemory2>,
    pub fp_cmd_set_device_mask: Option<vk::FnCmdSetDeviceMask>,
    pub fp_get_device_group_present_capabilities_khr: Option<vk::FnGetDeviceGroupPresentCapabilitiesKHR>,
    pub fp_get_device_group_surface_present_modes_khr: Option<vk::FnGetDeviceGroupSurfacePresentModesKHR>,
    pub fp_acquire_next_image2_khr: Option<vk::FnAcquireNextImage2KHR>,
    pub fp_cmd_dispatch_base: Option<vk::FnCmdDispatchBase>,
    pub fp_get_physical_device_present_rectangles_khr: Option<vk::FnGetPhysicalDevicePresentRectanglesKHR>,
    pub fp_create_descriptor_update_template: Option<vk::FnCreateDescriptorUpdateTemplate>,
    pub fp_destroy_descriptor_update_template: Option<vk::FnDestroyDescriptorUpdateTemplate>,
    pub fp_update_descriptor_set_with_template: Option<vk::FnUpdateDescriptorSetWithTemplate>,
    pub fp_cmd_push_descriptor_set_with_template: Option<vk::FnCmdPushDescriptorSetWithTemplate>,
    pub fp_set_hdr_metadata_ext: Option<vk::FnSetHdrMetadataEXT>,
    pub fp_get_swapchain_status_khr: Option<vk::FnGetSwapchainStatusKHR>,
    pub fp_get_refresh_cycle_duration_google: Option<vk::FnGetRefreshCycleDurationGOOGLE>,
    pub fp_get_past_presentation_timing_google: Option<vk::FnGetPastPresentationTimingGOOGLE>,
    pub fp_cmd_set_viewport_w_scaling_nv: Option<vk::FnCmdSetViewportWScalingNV>,
    pub fp_cmd_set_discard_rectangle_ext: Option<vk::FnCmdSetDiscardRectangleEXT>,
    pub fp_cmd_set_discard_rectangle_enable_ext: Option<vk::FnCmdSetDiscardRectangleEnableEXT>,
    pub fp_cmd_set_discard_rectangle_mode_ext: Option<vk::FnCmdSetDiscardRectangleModeEXT>,
    pub fp_cmd_set_sample_locations_ext: Option<vk::FnCmdSetSampleLocationsEXT>,
    pub fp_get_physical_device_multisample_properties_ext: Option<vk::FnGetPhysicalDeviceMultisamplePropertiesEXT>,
    pub fp_get_buffer_memory_requirements2: Option<vk::FnGetBufferMemoryRequirements2>,
    pub fp_get_image_memory_requirements2: Option<vk::FnGetImageMemoryRequirements2>,
    pub fp_get_image_sparse_memory_requirements2: Option<vk::FnGetImageSparseMemoryRequirements2>,
    pub fp_get_device_buffer_memory_requirements: Option<vk::FnGetDeviceBufferMemoryRequirements>,
    pub fp_get_device_image_memory_requirements: Option<vk::FnGetDeviceImageMemoryRequirements>,
    pub fp_get_device_image_sparse_memory_requirements: Option<vk::FnGetDeviceImageSparseMemoryRequirements>,
    pub fp_create_sampler_ycbcr_conversion: Option<vk::FnCreateSamplerYcbcrConversion>,
    pub fp_destroy_sampler_ycbcr_conversion: Option<vk::FnDestroySamplerYcbcrConversion>,
    pub fp_get_device_queue2: Option<vk::FnGetDeviceQueue2>,
    pub fp_create_validation_cache_ext: Option<vk::FnCreateValidationCacheEXT>,
    pub fp_destroy_validation_cache_ext: Option<vk::FnDestroyValidationCacheEXT>,
    pub fp_get_validation_cache_data_ext: Option<vk::FnGetValidationCacheDataEXT>,
    pub fp_merge_validation_caches_ext: Option<vk::FnMergeValidationCachesEXT>,
    pub fp_get_descriptor_set_layout_support: Option<vk::FnGetDescriptorSetLayoutSupport>,
    pub fp_get_shader_info_amd: Option<vk::FnGetShaderInfoAMD>,
    pub fp_set_local_dimming_amd: Option<vk::FnSetLocalDimmingAMD>,
    pub fp_get_physical_device_calibrateable_time_domains_khr:
        Option<vk::FnGetPhysicalDeviceCalibrateableTimeDomainsKHR>,
    pub fp_get_calibrated_timestamps_khr: Option<vk::FnGetCalibratedTimestampsKHR>,
    pub fp_get_memory_host_pointer_properties_ext: Option<vk::FnGetMemoryHostPointerPropertiesEXT>,
    pub fp_cmd_write_buffer_marker_amd: Option<vk::FnCmdWriteBufferMarkerAMD>,
    pub fp_create_render_pass2: Option<vk::FnCreateRenderPass2>,
    pub fp_cmd_begin_render_pass2: Option<vk::FnCmdBeginRenderPass2>,
    pub fp_cmd_next_subpass2: Option<vk::FnCmdNextSubpass2>,
    pub fp_cmd_end_render_pass2: Option<vk::FnCmdEndRenderPass2>,
    pub fp_get_semaphore_counter_value: Option<vk::FnGetSemaphoreCounterValue>,
    pub fp_wait_semaphores: Option<vk::FnWaitSemaphores>,
    pub fp_signal_semaphore: Option<vk::FnSignalSemaphore>,
    pub fp_get_android_hardware_buffer_properties_android: Option<vk::FnGetAndroidHardwareBufferPropertiesANDROID>,
    pub fp_get_memory_android_hardware_buffer_android: Option<vk::FnGetMemoryAndroidHardwareBufferANDROID>,
    pub fp_cmd_draw_indirect_count: Option<vk::FnCmdDrawIndirectCount>,
    pub fp_cmd_draw_indexed_indirect_count: Option<vk::FnCmdDrawIndexedIndirectCount>,
    pub fp_cmd_set_checkpoint_nv: Option<vk::FnCmdSetCheckpointNV>,
    pub fp_get_queue_checkpoint_data_nv: Option<vk::FnGetQueueCheckpointDataNV>,
    pub fp_cmd_bind_transform_feedback_buffers_ext: Option<vk::FnCmdBindTransformFeedbackBuffersEXT>,
    pub fp_cmd_begin_transform_feedback_ext: Option<vk::FnCmdBeginTransformFeedbackEXT>,
    pub fp_cmd_end_transform_feedback_ext: Option<vk::FnCmdEndTransformFeedbackEXT>,
    pub fp_cmd_begin_query_indexed_ext: Option<vk::FnCmdBeginQueryIndexedEXT>,
    pub fp_cmd_end_query_indexed_ext: Option<vk::FnCmdEndQueryIndexedEXT>,
    pub fp_cmd_draw_indirect_byte_count_ext: Option<vk::FnCmdDrawIndirectByteCountEXT>,
    pub fp_cmd_set_exclusive_scissor_nv: Option<vk::FnCmdSetExclusiveScissorNV>,
    pub fp_cmd_set_exclusive_scissor_enable_nv: Option<vk::FnCmdSetExclusiveScissorEnableNV>,
    pub fp_cmd_bind_shading_rate_image_nv: Option<vk::FnCmdBindShadingRateImageNV>,
    pub fp_cmd_set_viewport_shading_rate_palette_nv: Option<vk::FnCmdSetViewportShadingRatePaletteNV>,
    pub fp_cmd_set_coarse_sample_order_nv: Option<vk::FnCmdSetCoarseSampleOrderNV>,
    pub fp_cmd_draw_mesh_tasks_nv: Option<vk::FnCmdDrawMeshTasksNV>,
    pub fp_cmd_draw_mesh_tasks_indirect_nv: Option<vk::FnCmdDrawMeshTasksIndirectNV>,
    pub fp_cmd_draw_mesh_tasks_indirect_count_nv: Option<vk::FnCmdDrawMeshTasksIndirectCountNV>,
    pub fp_cmd_draw_mesh_tasks_ext: Option<vk::FnCmdDrawMeshTasksEXT>,
    pub fp_cmd_draw_mesh_tasks_indirect_ext: Option<vk::FnCmdDrawMeshTasksIndirectEXT>,
    pub fp_cmd_draw_mesh_tasks_indirect_count_ext: Option<vk::FnCmdDrawMeshTasksIndirectCountEXT>,
    pub fp_compile_deferred_nv: Option<vk::FnCompileDeferredNV>,
    pub fp_create_acceleration_structure_nv: Option<vk::FnCreateAccelerationStructureNV>,
    pub fp_cmd_bind_invocation_mask_huawei: Option<vk::FnCmdBindInvocationMaskHUAWEI>,
    pub fp_destroy_acceleration_structure_khr: Option<vk::FnDestroyAccelerationStructureKHR>,
    pub fp_destroy_acceleration_structure_nv: Option<vk::FnDestroyAccelerationStructureNV>,
    pub fp_get_acceleration_structure_memory_requirements_nv:
        Option<vk::FnGetAccelerationStructureMemoryRequirementsNV>,
    pub fp_bind_acceleration_structure_memory_nv: Option<vk::FnBindAccelerationStructureMemoryNV>,
    pub fp_cmd_copy_acceleration_structure_nv: Option<vk::FnCmdCopyAccelerationStructureNV>,
    pub fp_cmd_copy_acceleration_structure_khr: Option<vk::FnCmdCopyAccelerationStructureKHR>,
    pub fp_copy_acceleration_structure_khr: Option<vk::FnCopyAccelerationStructureKHR>,
    pub fp_cmd_copy_acceleration_structure_to_memory_khr: Option<vk::FnCmdCopyAccelerationStructureToMemoryKHR>,
    pub fp_copy_acceleration_structure_to_memory_khr: Option<vk::FnCopyAccelerationStructureToMemoryKHR>,
    pub fp_cmd_copy_memory_to_acceleration_structure_khr: Option<vk::FnCmdCopyMemoryToAccelerationStructureKHR>,
    pub fp_copy_memory_to_acceleration_structure_khr: Option<vk::FnCopyMemoryToAccelerationStructureKHR>,
    pub fp_cmd_write_acceleration_structures_properties_khr: Option<vk::FnCmdWriteAccelerationStructuresPropertiesKHR>,
    pub fp_cmd_write_acceleration_structures_properties_nv: Option<vk::FnCmdWriteAccelerationStructuresPropertiesNV>,
    pub fp_cmd_build_acceleration_structure_nv: Option<vk::FnCmdBuildAccelerationStructureNV>,
    pub fp_write_acceleration_structures_properties_khr: Option<vk::FnWriteAccelerationStructuresPropertiesKHR>,
    pub fp_cmd_trace_rays_khr: Option<vk::FnCmdTraceRaysKHR>,
    pub fp_cmd_trace_rays_nv: Option<vk::FnCmdTraceRaysNV>,
    pub fp_get_ray_tracing_shader_group_handles_khr: Option<vk::FnGetRayTracingShaderGroupHandlesKHR>,
    pub fp_get_ray_tracing_capture_replay_shader_group_handles_khr:
        Option<vk::FnGetRayTracingCaptureReplayShaderGroupHandlesKHR>,
    pub fp_get_acceleration_structure_handle_nv: Option<vk::FnGetAccelerationStructureHandleNV>,
    pub fp_create_ray_tracing_pipelines_nv: Option<vk::FnCreateRayTracingPipelinesNV>,
    pub fp_create_ray_tracing_pipelines_khr: Option<vk::FnCreateRayTracingPipelinesKHR>,
    pub fp_get_physical_device_cooperative_matrix_properties_nv:
        Option<vk::FnGetPhysicalDeviceCooperativeMatrixPropertiesNV>,
    pub fp_cmd_trace_rays_indirect_khr: Option<vk::FnCmdTraceRaysIndirectKHR>,
    pub fp_cmd_trace_rays_indirect2_khr: Option<vk::FnCmdTraceRaysIndirect2KHR>,
    pub fp_get_cluster_acceleration_structure_build_sizes_nv: Option<vk::FnGetClusterAccelerationStructureBuildSizesNV>,
    pub fp_cmd_build_cluster_acceleration_structure_indirect_nv:
        Option<vk::FnCmdBuildClusterAccelerationStructureIndirectNV>,
    pub fp_get_device_acceleration_structure_compatibility_khr:
        Option<vk::FnGetDeviceAccelerationStructureCompatibilityKHR>,
    pub fp_get_ray_tracing_shader_group_stack_size_khr: Option<vk::FnGetRayTracingShaderGroupStackSizeKHR>,
    pub fp_cmd_set_ray_tracing_pipeline_stack_size_khr: Option<vk::FnCmdSetRayTracingPipelineStackSizeKHR>,
    pub fp_get_image_view_handle_nvx: Option<vk::FnGetImageViewHandleNVX>,
    pub fp_get_image_view_handle64_nvx: Option<vk::FnGetImageViewHandle64NVX>,
    pub fp_get_image_view_address_nvx: Option<vk::FnGetImageViewAddressNVX>,
    pub fp_get_device_combined_image_sampler_index_nvx: Option<vk::FnGetDeviceCombinedImageSamplerIndexNVX>,
    pub fp_get_physical_device_surface_present_modes2_ext: Option<vk::FnGetPhysicalDeviceSurfacePresentModes2EXT>,
    pub fp_get_device_group_surface_present_modes2_ext: Option<vk::FnGetDeviceGroupSurfacePresentModes2EXT>,
    pub fp_acquire_full_screen_exclusive_mode_ext: Option<vk::FnAcquireFullScreenExclusiveModeEXT>,
    pub fp_release_full_screen_exclusive_mode_ext: Option<vk::FnReleaseFullScreenExclusiveModeEXT>,
    pub fp_enumerate_physical_device_queue_family_performance_query_counters_khr:
        Option<vk::FnEnumeratePhysicalDeviceQueueFamilyPerformanceQueryCountersKHR>,
    pub fp_get_physical_device_queue_family_performance_query_passes_khr:
        Option<vk::FnGetPhysicalDeviceQueueFamilyPerformanceQueryPassesKHR>,
    pub fp_acquire_profiling_lock_khr: Option<vk::FnAcquireProfilingLockKHR>,
    pub fp_release_profiling_lock_khr: Option<vk::FnReleaseProfilingLockKHR>,
    pub fp_get_image_drm_format_modifier_properties_ext: Option<vk::FnGetImageDrmFormatModifierPropertiesEXT>,
    pub fp_get_buffer_opaque_capture_address: Option<vk::FnGetBufferOpaqueCaptureAddress>,
    pub fp_get_buffer_device_address: Option<vk::FnGetBufferDeviceAddress>,
    pub fp_get_physical_device_supported_framebuffer_mixed_samples_combinations_nv:
        Option<vk::FnGetPhysicalDeviceSupportedFramebufferMixedSamplesCombinationsNV>,
    pub fp_initialize_performance_api_intel: Option<vk::FnInitializePerformanceApiINTEL>,
    pub fp_uninitialize_performance_api_intel: Option<vk::FnUninitializePerformanceApiINTEL>,
    pub fp_cmd_set_performance_marker_intel: Option<vk::FnCmdSetPerformanceMarkerINTEL>,
    pub fp_cmd_set_performance_stream_marker_intel: Option<vk::FnCmdSetPerformanceStreamMarkerINTEL>,
    pub fp_cmd_set_performance_override_intel: Option<vk::FnCmdSetPerformanceOverrideINTEL>,
    pub fp_acquire_performance_configuration_intel: Option<vk::FnAcquirePerformanceConfigurationINTEL>,
    pub fp_release_performance_configuration_intel: Option<vk::FnReleasePerformanceConfigurationINTEL>,
    pub fp_queue_set_performance_configuration_intel: Option<vk::FnQueueSetPerformanceConfigurationINTEL>,
    pub fp_get_performance_parameter_intel: Option<vk::FnGetPerformanceParameterINTEL>,
    pub fp_get_device_memory_opaque_capture_address: Option<vk::FnGetDeviceMemoryOpaqueCaptureAddress>,
    pub fp_get_pipeline_executable_properties_khr: Option<vk::FnGetPipelineExecutablePropertiesKHR>,
    pub fp_get_pipeline_executable_statistics_khr: Option<vk::FnGetPipelineExecutableStatisticsKHR>,
    pub fp_get_pipeline_executable_internal_representations_khr:
        Option<vk::FnGetPipelineExecutableInternalRepresentationsKHR>,
    pub fp_cmd_set_line_stipple: Option<vk::FnCmdSetLineStipple>,
    pub fp_get_physical_device_tool_properties: Option<vk::FnGetPhysicalDeviceToolProperties>,
    pub fp_create_acceleration_structure_khr: Option<vk::FnCreateAccelerationStructureKHR>,
    pub fp_cmd_build_acceleration_structures_khr: Option<vk::FnCmdBuildAccelerationStructuresKHR>,
    pub fp_cmd_build_acceleration_structures_indirect_khr: Option<vk::FnCmdBuildAccelerationStructuresIndirectKHR>,
    pub fp_build_acceleration_structures_khr: Option<vk::FnBuildAccelerationStructuresKHR>,
    pub fp_get_acceleration_structure_device_address_khr: Option<vk::FnGetAccelerationStructureDeviceAddressKHR>,
    pub fp_create_deferred_operation_khr: Option<vk::FnCreateDeferredOperationKHR>,
    pub fp_destroy_deferred_operation_khr: Option<vk::FnDestroyDeferredOperationKHR>,
    pub fp_get_deferred_operation_max_concurrency_khr: Option<vk::FnGetDeferredOperationMaxConcurrencyKHR>,
    pub fp_get_deferred_operation_result_khr: Option<vk::FnGetDeferredOperationResultKHR>,
    pub fp_deferred_operation_join_khr: Option<vk::FnDeferredOperationJoinKHR>,
    pub fp_get_pipeline_indirect_memory_requirements_nv: Option<vk::FnGetPipelineIndirectMemoryRequirementsNV>,
    pub fp_get_pipeline_indirect_device_address_nv: Option<vk::FnGetPipelineIndirectDeviceAddressNV>,
    pub fp_anti_lag_update_amd: Option<vk::FnAntiLagUpdateAMD>,
    pub fp_cmd_set_cull_mode: Option<vk::FnCmdSetCullMode>,
    pub fp_cmd_set_front_face: Option<vk::FnCmdSetFrontFace>,
    pub fp_cmd_set_primitive_topology: Option<vk::FnCmdSetPrimitiveTopology>,
    pub fp_cmd_set_viewport_with_count: Option<vk::FnCmdSetViewportWithCount>,
    pub fp_cmd_set_scissor_with_count: Option<vk::FnCmdSetScissorWithCount>,
    pub fp_cmd_bind_index_buffer2: Option<vk::FnCmdBindIndexBuffer2>,
    pub fp_cmd_bind_vertex_buffers2: Option<vk::FnCmdBindVertexBuffers2>,
    pub fp_cmd_set_depth_test_enable: Option<vk::FnCmdSetDepthTestEnable>,
    pub fp_cmd_set_depth_write_enable: Option<vk::FnCmdSetDepthWriteEnable>,
    pub fp_cmd_set_depth_compare_op: Option<vk::FnCmdSetDepthCompareOp>,
    pub fp_cmd_set_depth_bounds_test_enable: Option<vk::FnCmdSetDepthBoundsTestEnable>,
    pub fp_cmd_set_stencil_test_enable: Option<vk::FnCmdSetStencilTestEnable>,
    pub fp_cmd_set_stencil_op: Option<vk::FnCmdSetStencilOp>,
    pub fp_cmd_set_patch_control_points_ext: Option<vk::FnCmdSetPatchControlPointsEXT>,
    pub fp_cmd_set_rasterizer_discard_enable: Option<vk::FnCmdSetRasterizerDiscardEnable>,
    pub fp_cmd_set_depth_bias_enable: Option<vk::FnCmdSetDepthBiasEnable>,
    pub fp_cmd_set_logic_op_ext: Option<vk::FnCmdSetLogicOpEXT>,
    pub fp_cmd_set_primitive_restart_enable: Option<vk::FnCmdSetPrimitiveRestartEnable>,
    pub fp_cmd_set_tessellation_domain_origin_ext: Option<vk::FnCmdSetTessellationDomainOriginEXT>,
    pub fp_cmd_set_depth_clamp_enable_ext: Option<vk::FnCmdSetDepthClampEnableEXT>,
    pub fp_cmd_set_polygon_mode_ext: Option<vk::FnCmdSetPolygonModeEXT>,
    pub fp_cmd_set_rasterization_samples_ext: Option<vk::FnCmdSetRasterizationSamplesEXT>,
    pub fp_cmd_set_sample_mask_ext: Option<vk::FnCmdSetSampleMaskEXT>,
    pub fp_cmd_set_alpha_to_coverage_enable_ext: Option<vk::FnCmdSetAlphaToCoverageEnableEXT>,
    pub fp_cmd_set_alpha_to_one_enable_ext: Option<vk::FnCmdSetAlphaToOneEnableEXT>,
    pub fp_cmd_set_logic_op_enable_ext: Option<vk::FnCmdSetLogicOpEnableEXT>,
    pub fp_cmd_set_color_blend_enable_ext: Option<vk::FnCmdSetColorBlendEnableEXT>,
    pub fp_cmd_set_color_blend_equation_ext: Option<vk::FnCmdSetColorBlendEquationEXT>,
    pub fp_cmd_set_color_write_mask_ext: Option<vk::FnCmdSetColorWriteMaskEXT>,
    pub fp_cmd_set_rasterization_stream_ext: Option<vk::FnCmdSetRasterizationStreamEXT>,
    pub fp_cmd_set_conservative_rasterization_mode_ext: Option<vk::FnCmdSetConservativeRasterizationModeEXT>,
    pub fp_cmd_set_extra_primitive_overestimation_size_ext: Option<vk::FnCmdSetExtraPrimitiveOverestimationSizeEXT>,
    pub fp_cmd_set_depth_clip_enable_ext: Option<vk::FnCmdSetDepthClipEnableEXT>,
    pub fp_cmd_set_sample_locations_enable_ext: Option<vk::FnCmdSetSampleLocationsEnableEXT>,
    pub fp_cmd_set_color_blend_advanced_ext: Option<vk::FnCmdSetColorBlendAdvancedEXT>,
    pub fp_cmd_set_provoking_vertex_mode_ext: Option<vk::FnCmdSetProvokingVertexModeEXT>,
    pub fp_cmd_set_line_rasterization_mode_ext: Option<vk::FnCmdSetLineRasterizationModeEXT>,
    pub fp_cmd_set_line_stipple_enable_ext: Option<vk::FnCmdSetLineStippleEnableEXT>,
    pub fp_cmd_set_depth_clip_negative_one_to_one_ext: Option<vk::FnCmdSetDepthClipNegativeOneToOneEXT>,
    pub fp_cmd_set_viewport_w_scaling_enable_nv: Option<vk::FnCmdSetViewportWScalingEnableNV>,
    pub fp_cmd_set_viewport_swizzle_nv: Option<vk::FnCmdSetViewportSwizzleNV>,
    pub fp_cmd_set_coverage_to_color_enable_nv: Option<vk::FnCmdSetCoverageToColorEnableNV>,
    pub fp_cmd_set_coverage_to_color_location_nv: Option<vk::FnCmdSetCoverageToColorLocationNV>,
    pub fp_cmd_set_coverage_modulation_mode_nv: Option<vk::FnCmdSetCoverageModulationModeNV>,
    pub fp_cmd_set_coverage_modulation_table_enable_nv: Option<vk::FnCmdSetCoverageModulationTableEnableNV>,
    pub fp_cmd_set_coverage_modulation_table_nv: Option<vk::FnCmdSetCoverageModulationTableNV>,
    pub fp_cmd_set_shading_rate_image_enable_nv: Option<vk::FnCmdSetShadingRateImageEnableNV>,
    pub fp_cmd_set_coverage_reduction_mode_nv: Option<vk::FnCmdSetCoverageReductionModeNV>,
    pub fp_cmd_set_representative_fragment_test_enable_nv: Option<vk::FnCmdSetRepresentativeFragmentTestEnableNV>,
    pub fp_create_private_data_slot: Option<vk::FnCreatePrivateDataSlot>,
    pub fp_destroy_private_data_slot: Option<vk::FnDestroyPrivateDataSlot>,
    pub fp_set_private_data: Option<vk::FnSetPrivateData>,
    pub fp_get_private_data: Option<vk::FnGetPrivateData>,
    pub fp_cmd_copy_buffer2: Option<vk::FnCmdCopyBuffer2>,
    pub fp_cmd_copy_image2: Option<vk::FnCmdCopyImage2>,
    pub fp_cmd_blit_image2: Option<vk::FnCmdBlitImage2>,
    pub fp_cmd_copy_buffer_to_image2: Option<vk::FnCmdCopyBufferToImage2>,
    pub fp_cmd_copy_image_to_buffer2: Option<vk::FnCmdCopyImageToBuffer2>,
    pub fp_cmd_resolve_image2: Option<vk::FnCmdResolveImage2>,
    pub fp_cmd_set_fragment_shading_rate_khr: Option<vk::FnCmdSetFragmentShadingRateKHR>,
    pub fp_get_physical_device_fragment_shading_rates_khr: Option<vk::FnGetPhysicalDeviceFragmentShadingRatesKHR>,
    pub fp_cmd_set_fragment_shading_rate_enum_nv: Option<vk::FnCmdSetFragmentShadingRateEnumNV>,
    pub fp_get_acceleration_structure_build_sizes_khr: Option<vk::FnGetAccelerationStructureBuildSizesKHR>,
    pub fp_cmd_set_vertex_input_ext: Option<vk::FnCmdSetVertexInputEXT>,
    pub fp_cmd_set_color_write_enable_ext: Option<vk::FnCmdSetColorWriteEnableEXT>,
    pub fp_cmd_set_event2: Option<vk::FnCmdSetEvent2>,
    pub fp_cmd_reset_event2: Option<vk::FnCmdResetEvent2>,
    pub fp_cmd_wait_events2: Option<vk::FnCmdWaitEvents2>,
    pub fp_cmd_pipeline_barrier2: Option<vk::FnCmdPipelineBarrier2>,
    pub fp_queue_submit2: Option<vk::FnQueueSubmit2>,
    pub fp_cmd_write_timestamp2: Option<vk::FnCmdWriteTimestamp2>,
    pub fp_cmd_write_buffer_marker2_amd: Option<vk::FnCmdWriteBufferMarker2AMD>,
    pub fp_get_queue_checkpoint_data2_nv: Option<vk::FnGetQueueCheckpointData2NV>,
    pub fp_copy_memory_to_image: Option<vk::FnCopyMemoryToImage>,
    pub fp_copy_image_to_memory: Option<vk::FnCopyImageToMemory>,
    pub fp_copy_image_to_image: Option<vk::FnCopyImageToImage>,
    pub fp_transition_image_layout: Option<vk::FnTransitionImageLayout>,
    pub fp_cmd_decompress_memory_nv: Option<vk::FnCmdDecompressMemoryNV>,
    pub fp_cmd_decompress_memory_indirect_count_nv: Option<vk::FnCmdDecompressMemoryIndirectCountNV>,
    pub fp_get_partitioned_acceleration_structures_build_sizes_nv:
        Option<vk::FnGetPartitionedAccelerationStructuresBuildSizesNV>,
    pub fp_cmd_build_partitioned_acceleration_structures_nv: Option<vk::FnCmdBuildPartitionedAccelerationStructuresNV>,
    pub fp_cmd_decompress_memory_ext: Option<vk::FnCmdDecompressMemoryEXT>,
    pub fp_cmd_decompress_memory_indirect_count_ext: Option<vk::FnCmdDecompressMemoryIndirectCountEXT>,
    pub fp_create_cu_module_nvx: Option<vk::FnCreateCuModuleNVX>,
    pub fp_create_cu_function_nvx: Option<vk::FnCreateCuFunctionNVX>,
    pub fp_destroy_cu_module_nvx: Option<vk::FnDestroyCuModuleNVX>,
    pub fp_destroy_cu_function_nvx: Option<vk::FnDestroyCuFunctionNVX>,
    pub fp_cmd_cu_launch_kernel_nvx: Option<vk::FnCmdCuLaunchKernelNVX>,
    pub fp_get_descriptor_set_layout_size_ext: Option<vk::FnGetDescriptorSetLayoutSizeEXT>,
    pub fp_get_descriptor_set_layout_binding_offset_ext: Option<vk::FnGetDescriptorSetLayoutBindingOffsetEXT>,
    pub fp_get_descriptor_ext: Option<vk::FnGetDescriptorEXT>,
    pub fp_cmd_bind_descriptor_buffers_ext: Option<vk::FnCmdBindDescriptorBuffersEXT>,
    pub fp_cmd_set_descriptor_buffer_offsets_ext: Option<vk::FnCmdSetDescriptorBufferOffsetsEXT>,
    pub fp_cmd_bind_descriptor_buffer_embedded_samplers_ext: Option<vk::FnCmdBindDescriptorBufferEmbeddedSamplersEXT>,
    pub fp_get_buffer_opaque_capture_descriptor_data_ext: Option<vk::FnGetBufferOpaqueCaptureDescriptorDataEXT>,
    pub fp_get_image_opaque_capture_descriptor_data_ext: Option<vk::FnGetImageOpaqueCaptureDescriptorDataEXT>,
    pub fp_get_image_view_opaque_capture_descriptor_data_ext: Option<vk::FnGetImageViewOpaqueCaptureDescriptorDataEXT>,
    pub fp_get_sampler_opaque_capture_descriptor_data_ext: Option<vk::FnGetSamplerOpaqueCaptureDescriptorDataEXT>,
    pub fp_get_acceleration_structure_opaque_capture_descriptor_data_ext:
        Option<vk::FnGetAccelerationStructureOpaqueCaptureDescriptorDataEXT>,
    pub fp_set_device_memory_priority_ext: Option<vk::FnSetDeviceMemoryPriorityEXT>,
    pub fp_wait_for_present2_khr: Option<vk::FnWaitForPresent2KHR>,
    pub fp_wait_for_present_khr: Option<vk::FnWaitForPresentKHR>,
    pub fp_create_buffer_collection_fuchsia: Option<vk::FnCreateBufferCollectionFUCHSIA>,
    pub fp_set_buffer_collection_buffer_constraints_fuchsia: Option<vk::FnSetBufferCollectionBufferConstraintsFUCHSIA>,
    pub fp_set_buffer_collection_image_constraints_fuchsia: Option<vk::FnSetBufferCollectionImageConstraintsFUCHSIA>,
    pub fp_destroy_buffer_collection_fuchsia: Option<vk::FnDestroyBufferCollectionFUCHSIA>,
    pub fp_get_buffer_collection_properties_fuchsia: Option<vk::FnGetBufferCollectionPropertiesFUCHSIA>,
    pub fp_create_cuda_module_nv: Option<vk::FnCreateCudaModuleNV>,
    pub fp_get_cuda_module_cache_nv: Option<vk::FnGetCudaModuleCacheNV>,
    pub fp_create_cuda_function_nv: Option<vk::FnCreateCudaFunctionNV>,
    pub fp_destroy_cuda_module_nv: Option<vk::FnDestroyCudaModuleNV>,
    pub fp_destroy_cuda_function_nv: Option<vk::FnDestroyCudaFunctionNV>,
    pub fp_cmd_cuda_launch_kernel_nv: Option<vk::FnCmdCudaLaunchKernelNV>,
    pub fp_cmd_begin_rendering: Option<vk::FnCmdBeginRendering>,
    pub fp_cmd_end_rendering: Option<vk::FnCmdEndRendering>,
    pub fp_cmd_end_rendering2_khr: Option<vk::FnCmdEndRendering2KHR>,
    pub fp_get_descriptor_set_layout_host_mapping_info_valve: Option<vk::FnGetDescriptorSetLayoutHostMappingInfoVALVE>,
    pub fp_get_descriptor_set_host_mapping_valve: Option<vk::FnGetDescriptorSetHostMappingVALVE>,
    pub fp_create_micromap_ext: Option<vk::FnCreateMicromapEXT>,
    pub fp_cmd_build_micromaps_ext: Option<vk::FnCmdBuildMicromapsEXT>,
    pub fp_build_micromaps_ext: Option<vk::FnBuildMicromapsEXT>,
    pub fp_destroy_micromap_ext: Option<vk::FnDestroyMicromapEXT>,
    pub fp_cmd_copy_micromap_ext: Option<vk::FnCmdCopyMicromapEXT>,
    pub fp_copy_micromap_ext: Option<vk::FnCopyMicromapEXT>,
    pub fp_cmd_copy_micromap_to_memory_ext: Option<vk::FnCmdCopyMicromapToMemoryEXT>,
    pub fp_copy_micromap_to_memory_ext: Option<vk::FnCopyMicromapToMemoryEXT>,
    pub fp_cmd_copy_memory_to_micromap_ext: Option<vk::FnCmdCopyMemoryToMicromapEXT>,
    pub fp_copy_memory_to_micromap_ext: Option<vk::FnCopyMemoryToMicromapEXT>,
    pub fp_cmd_write_micromaps_properties_ext: Option<vk::FnCmdWriteMicromapsPropertiesEXT>,
    pub fp_write_micromaps_properties_ext: Option<vk::FnWriteMicromapsPropertiesEXT>,
    pub fp_get_device_micromap_compatibility_ext: Option<vk::FnGetDeviceMicromapCompatibilityEXT>,
    pub fp_get_micromap_build_sizes_ext: Option<vk::FnGetMicromapBuildSizesEXT>,
    pub fp_get_shader_module_identifier_ext: Option<vk::FnGetShaderModuleIdentifierEXT>,
    pub fp_get_shader_module_create_info_identifier_ext: Option<vk::FnGetShaderModuleCreateInfoIdentifierEXT>,
    pub fp_get_image_subresource_layout2: Option<vk::FnGetImageSubresourceLayout2>,
    pub fp_get_pipeline_properties_ext: Option<vk::FnGetPipelinePropertiesEXT>,
    pub fp_export_metal_objects_ext: Option<vk::FnExportMetalObjectsEXT>,
    pub fp_cmd_bind_tile_memory_qcom: Option<vk::FnCmdBindTileMemoryQCOM>,
    pub fp_get_framebuffer_tile_properties_qcom: Option<vk::FnGetFramebufferTilePropertiesQCOM>,
    pub fp_get_dynamic_rendering_tile_properties_qcom: Option<vk::FnGetDynamicRenderingTilePropertiesQCOM>,
    pub fp_get_physical_device_optical_flow_image_formats_nv: Option<vk::FnGetPhysicalDeviceOpticalFlowImageFormatsNV>,
    pub fp_create_optical_flow_session_nv: Option<vk::FnCreateOpticalFlowSessionNV>,
    pub fp_destroy_optical_flow_session_nv: Option<vk::FnDestroyOpticalFlowSessionNV>,
    pub fp_bind_optical_flow_session_image_nv: Option<vk::FnBindOpticalFlowSessionImageNV>,
    pub fp_cmd_optical_flow_execute_nv: Option<vk::FnCmdOpticalFlowExecuteNV>,
    pub fp_get_device_fault_info_ext: Option<vk::FnGetDeviceFaultInfoEXT>,
    pub fp_cmd_set_depth_bias2_ext: Option<vk::FnCmdSetDepthBias2EXT>,
    pub fp_release_swapchain_images_khr: Option<vk::FnReleaseSwapchainImagesKHR>,
    pub fp_get_device_image_subresource_layout: Option<vk::FnGetDeviceImageSubresourceLayout>,
    pub fp_map_memory2: Option<vk::FnMapMemory2>,
    pub fp_unmap_memory2: Option<vk::FnUnmapMemory2>,
    pub fp_create_shaders_ext: Option<vk::FnCreateShadersEXT>,
    pub fp_destroy_shader_ext: Option<vk::FnDestroyShaderEXT>,
    pub fp_get_shader_binary_data_ext: Option<vk::FnGetShaderBinaryDataEXT>,
    pub fp_cmd_bind_shaders_ext: Option<vk::FnCmdBindShadersEXT>,
    pub fp_set_swapchain_present_timing_queue_size_ext: Option<vk::FnSetSwapchainPresentTimingQueueSizeEXT>,
    pub fp_get_swapchain_timing_properties_ext: Option<vk::FnGetSwapchainTimingPropertiesEXT>,
    pub fp_get_swapchain_time_domain_properties_ext: Option<vk::FnGetSwapchainTimeDomainPropertiesEXT>,
    pub fp_get_past_presentation_timing_ext: Option<vk::FnGetPastPresentationTimingEXT>,
    pub fp_get_physical_device_cooperative_matrix_properties_khr:
        Option<vk::FnGetPhysicalDeviceCooperativeMatrixPropertiesKHR>,
    pub fp_get_execution_graph_pipeline_scratch_size_amdx: Option<vk::FnGetExecutionGraphPipelineScratchSizeAMDX>,
    pub fp_get_execution_graph_pipeline_node_index_amdx: Option<vk::FnGetExecutionGraphPipelineNodeIndexAMDX>,
    pub fp_create_execution_graph_pipelines_amdx: Option<vk::FnCreateExecutionGraphPipelinesAMDX>,
    pub fp_cmd_initialize_graph_scratch_memory_amdx: Option<vk::FnCmdInitializeGraphScratchMemoryAMDX>,
    pub fp_cmd_dispatch_graph_amdx: Option<vk::FnCmdDispatchGraphAMDX>,
    pub fp_cmd_dispatch_graph_indirect_amdx: Option<vk::FnCmdDispatchGraphIndirectAMDX>,
    pub fp_cmd_dispatch_graph_indirect_count_amdx: Option<vk::FnCmdDispatchGraphIndirectCountAMDX>,
    pub fp_cmd_bind_descriptor_sets2: Option<vk::FnCmdBindDescriptorSets2>,
    pub fp_cmd_push_constants2: Option<vk::FnCmdPushConstants2>,
    pub fp_cmd_push_descriptor_set2: Option<vk::FnCmdPushDescriptorSet2>,
    pub fp_cmd_push_descriptor_set_with_template2: Option<vk::FnCmdPushDescriptorSetWithTemplate2>,
    pub fp_cmd_set_descriptor_buffer_offsets2_ext: Option<vk::FnCmdSetDescriptorBufferOffsets2EXT>,
    pub fp_cmd_bind_descriptor_buffer_embedded_samplers2_ext: Option<vk::FnCmdBindDescriptorBufferEmbeddedSamplers2EXT>,
    pub fp_set_latency_sleep_mode_nv: Option<vk::FnSetLatencySleepModeNV>,
    pub fp_latency_sleep_nv: Option<vk::FnLatencySleepNV>,
    pub fp_set_latency_marker_nv: Option<vk::FnSetLatencyMarkerNV>,
    pub fp_get_latency_timings_nv: Option<vk::FnGetLatencyTimingsNV>,
    pub fp_queue_notify_out_of_band_nv: Option<vk::FnQueueNotifyOutOfBandNV>,
    pub fp_cmd_set_rendering_attachment_locations: Option<vk::FnCmdSetRenderingAttachmentLocations>,
    pub fp_cmd_set_rendering_input_attachment_indices: Option<vk::FnCmdSetRenderingInputAttachmentIndices>,
    pub fp_cmd_set_depth_clamp_range_ext: Option<vk::FnCmdSetDepthClampRangeEXT>,
    pub fp_get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv:
        Option<vk::FnGetPhysicalDeviceCooperativeMatrixFlexibleDimensionsPropertiesNV>,
    pub fp_get_memory_metal_handle_ext: Option<vk::FnGetMemoryMetalHandleEXT>,
    pub fp_get_memory_metal_handle_properties_ext: Option<vk::FnGetMemoryMetalHandlePropertiesEXT>,
    pub fp_get_physical_device_cooperative_vector_properties_nv:
        Option<vk::FnGetPhysicalDeviceCooperativeVectorPropertiesNV>,
    pub fp_convert_cooperative_vector_matrix_nv: Option<vk::FnConvertCooperativeVectorMatrixNV>,
    pub fp_cmd_convert_cooperative_vector_matrix_nv: Option<vk::FnCmdConvertCooperativeVectorMatrixNV>,
    pub fp_cmd_dispatch_tile_qcom: Option<vk::FnCmdDispatchTileQCOM>,
    pub fp_cmd_begin_per_tile_execution_qcom: Option<vk::FnCmdBeginPerTileExecutionQCOM>,
    pub fp_cmd_end_per_tile_execution_qcom: Option<vk::FnCmdEndPerTileExecutionQCOM>,
    pub fp_create_external_compute_queue_nv: Option<vk::FnCreateExternalComputeQueueNV>,
    pub fp_destroy_external_compute_queue_nv: Option<vk::FnDestroyExternalComputeQueueNV>,
    pub fp_get_external_compute_queue_data_nv: Option<vk::FnGetExternalComputeQueueDataNV>,
    pub fp_create_tensor_arm: Option<vk::FnCreateTensorARM>,
    pub fp_destroy_tensor_arm: Option<vk::FnDestroyTensorARM>,
    pub fp_create_tensor_view_arm: Option<vk::FnCreateTensorViewARM>,
    pub fp_destroy_tensor_view_arm: Option<vk::FnDestroyTensorViewARM>,
    pub fp_get_tensor_memory_requirements_arm: Option<vk::FnGetTensorMemoryRequirementsARM>,
    pub fp_bind_tensor_memory_arm: Option<vk::FnBindTensorMemoryARM>,
    pub fp_get_device_tensor_memory_requirements_arm: Option<vk::FnGetDeviceTensorMemoryRequirementsARM>,
    pub fp_cmd_copy_tensor_arm: Option<vk::FnCmdCopyTensorARM>,
    pub fp_get_tensor_opaque_capture_descriptor_data_arm: Option<vk::FnGetTensorOpaqueCaptureDescriptorDataARM>,
    pub fp_get_tensor_view_opaque_capture_descriptor_data_arm:
        Option<vk::FnGetTensorViewOpaqueCaptureDescriptorDataARM>,
    pub fp_get_physical_device_external_tensor_properties_arm:
        Option<vk::FnGetPhysicalDeviceExternalTensorPropertiesARM>,
    pub fp_create_data_graph_pipelines_arm: Option<vk::FnCreateDataGraphPipelinesARM>,
    pub fp_create_data_graph_pipeline_session_arm: Option<vk::FnCreateDataGraphPipelineSessionARM>,
    pub fp_get_data_graph_pipeline_session_bind_point_requirements_arm:
        Option<vk::FnGetDataGraphPipelineSessionBindPointRequirementsARM>,
    pub fp_get_data_graph_pipeline_session_memory_requirements_arm:
        Option<vk::FnGetDataGraphPipelineSessionMemoryRequirementsARM>,
    pub fp_bind_data_graph_pipeline_session_memory_arm: Option<vk::FnBindDataGraphPipelineSessionMemoryARM>,
    pub fp_destroy_data_graph_pipeline_session_arm: Option<vk::FnDestroyDataGraphPipelineSessionARM>,
    pub fp_cmd_dispatch_data_graph_arm: Option<vk::FnCmdDispatchDataGraphARM>,
    pub fp_get_data_graph_pipeline_available_properties_arm: Option<vk::FnGetDataGraphPipelineAvailablePropertiesARM>,
    pub fp_get_data_graph_pipeline_properties_arm: Option<vk::FnGetDataGraphPipelinePropertiesARM>,
    pub fp_get_physical_device_queue_family_data_graph_properties_arm:
        Option<vk::FnGetPhysicalDeviceQueueFamilyDataGraphPropertiesARM>,
    pub fp_get_physical_device_queue_family_data_graph_processing_engine_properties_arm:
        Option<vk::FnGetPhysicalDeviceQueueFamilyDataGraphProcessingEnginePropertiesARM>,
    pub fp_get_native_buffer_properties_ohos: Option<vk::FnGetNativeBufferPropertiesOHOS>,
    pub fp_get_memory_native_buffer_ohos: Option<vk::FnGetMemoryNativeBufferOHOS>,
    pub fp_enumerate_physical_device_queue_family_performance_counters_by_region_arm:
        Option<vk::FnEnumeratePhysicalDeviceQueueFamilyPerformanceCountersByRegionARM>,
    pub fp_cmd_set_compute_occupancy_priority_nv: Option<vk::FnCmdSetComputeOccupancyPriorityNV>,
    pub fp_write_sampler_descriptors_ext: Option<vk::FnWriteSamplerDescriptorsEXT>,
    pub fp_write_resource_descriptors_ext: Option<vk::FnWriteResourceDescriptorsEXT>,
    pub fp_cmd_bind_sampler_heap_ext: Option<vk::FnCmdBindSamplerHeapEXT>,
    pub fp_cmd_bind_resource_heap_ext: Option<vk::FnCmdBindResourceHeapEXT>,
    pub fp_cmd_push_data_ext: Option<vk::FnCmdPushDataEXT>,
    pub fp_register_custom_border_color_ext: Option<vk::FnRegisterCustomBorderColorEXT>,
    pub fp_unregister_custom_border_color_ext: Option<vk::FnUnregisterCustomBorderColorEXT>,
    pub fp_get_image_opaque_capture_data_ext: Option<vk::FnGetImageOpaqueCaptureDataEXT>,
    pub fp_get_physical_device_descriptor_size_ext: Option<vk::FnGetPhysicalDeviceDescriptorSizeEXT>,
    pub fp_get_tensor_opaque_capture_data_arm: Option<vk::FnGetTensorOpaqueCaptureDataARM>,
}
impl Device {
    #[allow(clippy::cognitive_complexity, clippy::nonminimal_bool)]
    pub unsafe fn load(
        globals: &Globals,
        instance: &Instance,
        device: vk::Device,
        create_info: &vk::DeviceCreateInfo,
    ) -> LoadResult<Self> {
        let mut extensions = DeviceExtensions::new(instance.extensions.core_version);
        if create_info.enabled_extension_count != 0 {
            for &name_ptr in slice::from_raw_parts(
                create_info.pp_enabled_extension_names,
                create_info.enabled_extension_count as usize,
            ) {
                extensions.enable_by_name(CStr::from_ptr(name_ptr));
            }
        }
        Ok(Self {
            handle: device,
            extensions,
            fp_destroy_device: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyDevice")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyDevice"))?,
            ),
            fp_get_device_queue: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceQueue")
                    .ok_or(LoadError::MissingSymbol(c"vkGetDeviceQueue"))?,
            ),
            fp_queue_submit: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkQueueSubmit")
                    .ok_or(LoadError::MissingSymbol(c"vkQueueSubmit"))?,
            ),
            fp_queue_wait_idle: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkQueueWaitIdle")
                    .ok_or(LoadError::MissingSymbol(c"vkQueueWaitIdle"))?,
            ),
            fp_device_wait_idle: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDeviceWaitIdle")
                    .ok_or(LoadError::MissingSymbol(c"vkDeviceWaitIdle"))?,
            ),
            fp_allocate_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkAllocateMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkAllocateMemory"))?,
            ),
            fp_free_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkFreeMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkFreeMemory"))?,
            ),
            fp_map_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkMapMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkMapMemory"))?,
            ),
            fp_unmap_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkUnmapMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkUnmapMemory"))?,
            ),
            fp_flush_mapped_memory_ranges: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkFlushMappedMemoryRanges")
                    .ok_or(LoadError::MissingSymbol(c"vkFlushMappedMemoryRanges"))?,
            ),
            fp_invalidate_mapped_memory_ranges: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkInvalidateMappedMemoryRanges")
                    .ok_or(LoadError::MissingSymbol(c"vkInvalidateMappedMemoryRanges"))?,
            ),
            fp_get_device_memory_commitment: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceMemoryCommitment")
                    .ok_or(LoadError::MissingSymbol(c"vkGetDeviceMemoryCommitment"))?,
            ),
            fp_get_buffer_memory_requirements: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetBufferMemoryRequirements")
                    .ok_or(LoadError::MissingSymbol(c"vkGetBufferMemoryRequirements"))?,
            ),
            fp_bind_buffer_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkBindBufferMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkBindBufferMemory"))?,
            ),
            fp_get_image_memory_requirements: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetImageMemoryRequirements")
                    .ok_or(LoadError::MissingSymbol(c"vkGetImageMemoryRequirements"))?,
            ),
            fp_bind_image_memory: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkBindImageMemory")
                    .ok_or(LoadError::MissingSymbol(c"vkBindImageMemory"))?,
            ),
            fp_get_image_sparse_memory_requirements: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetImageSparseMemoryRequirements")
                    .ok_or(LoadError::MissingSymbol(c"vkGetImageSparseMemoryRequirements"))?,
            ),
            fp_queue_bind_sparse: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkQueueBindSparse")
                    .ok_or(LoadError::MissingSymbol(c"vkQueueBindSparse"))?,
            ),
            fp_create_fence: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateFence")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateFence"))?,
            ),
            fp_destroy_fence: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyFence")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyFence"))?,
            ),
            fp_reset_fences: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkResetFences")
                    .ok_or(LoadError::MissingSymbol(c"vkResetFences"))?,
            ),
            fp_get_fence_status: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetFenceStatus")
                    .ok_or(LoadError::MissingSymbol(c"vkGetFenceStatus"))?,
            ),
            fp_wait_for_fences: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkWaitForFences")
                    .ok_or(LoadError::MissingSymbol(c"vkWaitForFences"))?,
            ),
            fp_create_semaphore: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateSemaphore")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateSemaphore"))?,
            ),
            fp_destroy_semaphore: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroySemaphore")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroySemaphore"))?,
            ),
            fp_create_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateEvent"))?,
            ),
            fp_destroy_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyEvent"))?,
            ),
            fp_get_event_status: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetEventStatus")
                    .ok_or(LoadError::MissingSymbol(c"vkGetEventStatus"))?,
            ),
            fp_set_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkSetEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkSetEvent"))?,
            ),
            fp_reset_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkResetEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkResetEvent"))?,
            ),
            fp_create_query_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateQueryPool")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateQueryPool"))?,
            ),
            fp_destroy_query_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyQueryPool")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyQueryPool"))?,
            ),
            fp_get_query_pool_results: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetQueryPoolResults")
                    .ok_or(LoadError::MissingSymbol(c"vkGetQueryPoolResults"))?,
            ),
            fp_reset_query_pool: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkResetQueryPool")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_query_reset {
                instance
                    .get_device_proc_addr(device, c"vkResetQueryPoolEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateBuffer"))?,
            ),
            fp_destroy_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyBuffer"))?,
            ),
            fp_create_buffer_view: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateBufferView")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateBufferView"))?,
            ),
            fp_destroy_buffer_view: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyBufferView")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyBufferView"))?,
            ),
            fp_create_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateImage"))?,
            ),
            fp_destroy_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyImage")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyImage"))?,
            ),
            fp_get_image_subresource_layout: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetImageSubresourceLayout")
                    .ok_or(LoadError::MissingSymbol(c"vkGetImageSubresourceLayout"))?,
            ),
            fp_create_image_view: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateImageView")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateImageView"))?,
            ),
            fp_destroy_image_view: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyImageView")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyImageView"))?,
            ),
            fp_create_shader_module: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateShaderModule")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateShaderModule"))?,
            ),
            fp_destroy_shader_module: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyShaderModule")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyShaderModule"))?,
            ),
            fp_create_pipeline_cache: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreatePipelineCache")
                    .ok_or(LoadError::MissingSymbol(c"vkCreatePipelineCache"))?,
            ),
            fp_destroy_pipeline_cache: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyPipelineCache")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyPipelineCache"))?,
            ),
            fp_get_pipeline_cache_data: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineCacheData")
                    .ok_or(LoadError::MissingSymbol(c"vkGetPipelineCacheData"))?,
            ),
            fp_merge_pipeline_caches: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkMergePipelineCaches")
                    .ok_or(LoadError::MissingSymbol(c"vkMergePipelineCaches"))?,
            ),
            fp_create_pipeline_binaries_khr: if extensions.khr_pipeline_binary {
                instance
                    .get_device_proc_addr(device, c"vkCreatePipelineBinariesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_pipeline_binary_khr: if extensions.khr_pipeline_binary {
                instance
                    .get_device_proc_addr(device, c"vkDestroyPipelineBinaryKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_key_khr: if extensions.khr_pipeline_binary {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineKeyKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_binary_data_khr: if extensions.khr_pipeline_binary {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineBinaryDataKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_captured_pipeline_data_khr: if extensions.khr_pipeline_binary {
                instance
                    .get_device_proc_addr(device, c"vkReleaseCapturedPipelineDataKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_graphics_pipelines: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateGraphicsPipelines")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateGraphicsPipelines"))?,
            ),
            fp_create_compute_pipelines: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateComputePipelines")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateComputePipelines"))?,
            ),
            fp_get_device_subpass_shading_max_workgroup_size_huawei: if extensions.huawei_subpass_shading {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceSubpassShadingMaxWorkgroupSizeHUAWEI")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_pipeline: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyPipeline")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyPipeline"))?,
            ),
            fp_create_pipeline_layout: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreatePipelineLayout")
                    .ok_or(LoadError::MissingSymbol(c"vkCreatePipelineLayout"))?,
            ),
            fp_destroy_pipeline_layout: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyPipelineLayout")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyPipelineLayout"))?,
            ),
            fp_create_sampler: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateSampler")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateSampler"))?,
            ),
            fp_destroy_sampler: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroySampler")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroySampler"))?,
            ),
            fp_create_descriptor_set_layout: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateDescriptorSetLayout")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateDescriptorSetLayout"))?,
            ),
            fp_destroy_descriptor_set_layout: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyDescriptorSetLayout")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyDescriptorSetLayout"))?,
            ),
            fp_create_descriptor_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateDescriptorPool")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateDescriptorPool"))?,
            ),
            fp_destroy_descriptor_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyDescriptorPool")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyDescriptorPool"))?,
            ),
            fp_reset_descriptor_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkResetDescriptorPool")
                    .ok_or(LoadError::MissingSymbol(c"vkResetDescriptorPool"))?,
            ),
            fp_allocate_descriptor_sets: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkAllocateDescriptorSets")
                    .ok_or(LoadError::MissingSymbol(c"vkAllocateDescriptorSets"))?,
            ),
            fp_free_descriptor_sets: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkFreeDescriptorSets")
                    .ok_or(LoadError::MissingSymbol(c"vkFreeDescriptorSets"))?,
            ),
            fp_update_descriptor_sets: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkUpdateDescriptorSets")
                    .ok_or(LoadError::MissingSymbol(c"vkUpdateDescriptorSets"))?,
            ),
            fp_create_framebuffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateFramebuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateFramebuffer"))?,
            ),
            fp_destroy_framebuffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyFramebuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyFramebuffer"))?,
            ),
            fp_create_render_pass: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateRenderPass")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateRenderPass"))?,
            ),
            fp_destroy_render_pass: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyRenderPass")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyRenderPass"))?,
            ),
            fp_get_render_area_granularity: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkGetRenderAreaGranularity")
                    .ok_or(LoadError::MissingSymbol(c"vkGetRenderAreaGranularity"))?,
            ),
            fp_get_rendering_area_granularity: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetRenderingAreaGranularity")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance5 {
                instance
                    .get_device_proc_addr(device, c"vkGetRenderingAreaGranularityKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_command_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCreateCommandPool")
                    .ok_or(LoadError::MissingSymbol(c"vkCreateCommandPool"))?,
            ),
            fp_destroy_command_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkDestroyCommandPool")
                    .ok_or(LoadError::MissingSymbol(c"vkDestroyCommandPool"))?,
            ),
            fp_reset_command_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkResetCommandPool")
                    .ok_or(LoadError::MissingSymbol(c"vkResetCommandPool"))?,
            ),
            fp_allocate_command_buffers: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkAllocateCommandBuffers")
                    .ok_or(LoadError::MissingSymbol(c"vkAllocateCommandBuffers"))?,
            ),
            fp_free_command_buffers: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkFreeCommandBuffers")
                    .ok_or(LoadError::MissingSymbol(c"vkFreeCommandBuffers"))?,
            ),
            fp_begin_command_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkBeginCommandBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkBeginCommandBuffer"))?,
            ),
            fp_end_command_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkEndCommandBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkEndCommandBuffer"))?,
            ),
            fp_reset_command_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkResetCommandBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkResetCommandBuffer"))?,
            ),
            fp_cmd_bind_pipeline: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBindPipeline")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBindPipeline"))?,
            ),
            fp_cmd_set_attachment_feedback_loop_enable_ext: if extensions.ext_attachment_feedback_loop_dynamic_state {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetAttachmentFeedbackLoopEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewport")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetViewport"))?,
            ),
            fp_cmd_set_scissor: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetScissor")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetScissor"))?,
            ),
            fp_cmd_set_line_width: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineWidth")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetLineWidth"))?,
            ),
            fp_cmd_set_depth_bias: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBias")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetDepthBias"))?,
            ),
            fp_cmd_set_blend_constants: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetBlendConstants")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetBlendConstants"))?,
            ),
            fp_cmd_set_depth_bounds: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBounds")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetDepthBounds"))?,
            ),
            fp_cmd_set_stencil_compare_mask: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilCompareMask")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetStencilCompareMask"))?,
            ),
            fp_cmd_set_stencil_write_mask: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilWriteMask")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetStencilWriteMask"))?,
            ),
            fp_cmd_set_stencil_reference: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilReference")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetStencilReference"))?,
            ),
            fp_cmd_bind_descriptor_sets: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorSets")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBindDescriptorSets"))?,
            ),
            fp_cmd_bind_index_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBindIndexBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBindIndexBuffer"))?,
            ),
            fp_cmd_bind_vertex_buffers: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBindVertexBuffers")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBindVertexBuffers"))?,
            ),
            fp_cmd_draw: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDraw")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDraw"))?,
            ),
            fp_cmd_draw_indexed: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndexed")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDrawIndexed"))?,
            ),
            fp_cmd_draw_multi_ext: if extensions.ext_multi_draw {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMultiEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_multi_indexed_ext: if extensions.ext_multi_draw {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMultiIndexedEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_indirect: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndirect")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDrawIndirect"))?,
            ),
            fp_cmd_draw_indexed_indirect: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndexedIndirect")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDrawIndexedIndirect"))?,
            ),
            fp_cmd_dispatch: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatch")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDispatch"))?,
            ),
            fp_cmd_dispatch_indirect: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchIndirect")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdDispatchIndirect"))?,
            ),
            fp_cmd_subpass_shading_huawei: if extensions.huawei_subpass_shading {
                instance
                    .get_device_proc_addr(device, c"vkCmdSubpassShadingHUAWEI")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_cluster_huawei: if extensions.huawei_cluster_culling_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawClusterHUAWEI")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_cluster_indirect_huawei: if extensions.huawei_cluster_culling_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawClusterIndirectHUAWEI")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_update_pipeline_indirect_buffer_nv: if extensions.nv_device_generated_commands_compute {
                instance
                    .get_device_proc_addr(device, c"vkCmdUpdatePipelineIndirectBufferNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdCopyBuffer"))?,
            ),
            fp_cmd_copy_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdCopyImage"))?,
            ),
            fp_cmd_blit_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBlitImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBlitImage"))?,
            ),
            fp_cmd_copy_buffer_to_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBufferToImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdCopyBufferToImage"))?,
            ),
            fp_cmd_copy_image_to_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImageToBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdCopyImageToBuffer"))?,
            ),
            fp_cmd_copy_memory_indirect_nv: if extensions.nv_copy_memory_indirect {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryIndirectNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_memory_indirect_khr: if extensions.khr_copy_memory_indirect {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryIndirectKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_memory_to_image_indirect_nv: if extensions.nv_copy_memory_indirect {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryToImageIndirectNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_memory_to_image_indirect_khr: if extensions.khr_copy_memory_indirect {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryToImageIndirectKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_update_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdUpdateBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdUpdateBuffer"))?,
            ),
            fp_cmd_fill_buffer: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdFillBuffer")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdFillBuffer"))?,
            ),
            fp_cmd_clear_color_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdClearColorImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdClearColorImage"))?,
            ),
            fp_cmd_clear_depth_stencil_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdClearDepthStencilImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdClearDepthStencilImage"))?,
            ),
            fp_cmd_clear_attachments: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdClearAttachments")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdClearAttachments"))?,
            ),
            fp_cmd_resolve_image: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdResolveImage")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdResolveImage"))?,
            ),
            fp_cmd_set_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdSetEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdSetEvent"))?,
            ),
            fp_cmd_reset_event: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdResetEvent")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdResetEvent"))?,
            ),
            fp_cmd_wait_events: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdWaitEvents")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdWaitEvents"))?,
            ),
            fp_cmd_pipeline_barrier: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdPipelineBarrier")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdPipelineBarrier"))?,
            ),
            fp_cmd_begin_query: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginQuery")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBeginQuery"))?,
            ),
            fp_cmd_end_query: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdEndQuery")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdEndQuery"))?,
            ),
            fp_cmd_begin_conditional_rendering_ext: if extensions.ext_conditional_rendering {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginConditionalRenderingEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_conditional_rendering_ext: if extensions.ext_conditional_rendering {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndConditionalRenderingEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_custom_resolve_ext: if extensions.ext_custom_resolve
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) || extensions.khr_dynamic_rendering)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginCustomResolveEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_reset_query_pool: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdResetQueryPool")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdResetQueryPool"))?,
            ),
            fp_cmd_write_timestamp: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteTimestamp")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdWriteTimestamp"))?,
            ),
            fp_cmd_copy_query_pool_results: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyQueryPoolResults")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdCopyQueryPoolResults"))?,
            ),
            fp_cmd_push_constants: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdPushConstants")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdPushConstants"))?,
            ),
            fp_cmd_begin_render_pass: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginRenderPass")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdBeginRenderPass"))?,
            ),
            fp_cmd_next_subpass: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdNextSubpass")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdNextSubpass"))?,
            ),
            fp_cmd_end_render_pass: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRenderPass")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdEndRenderPass"))?,
            ),
            fp_cmd_execute_commands: mem::transmute(
                instance
                    .get_device_proc_addr(device, c"vkCmdExecuteCommands")
                    .ok_or(LoadError::MissingSymbol(c"vkCmdExecuteCommands"))?,
            ),
            fp_create_shared_swapchains_khr: if extensions.khr_display_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkCreateSharedSwapchainsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_swapchain_khr: if extensions.khr_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkCreateSwapchainKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_swapchain_khr: if extensions.khr_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkDestroySwapchainKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_swapchain_images_khr: if extensions.khr_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkGetSwapchainImagesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_next_image_khr: if extensions.khr_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkAcquireNextImageKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_present_khr: if extensions.khr_swapchain {
                instance
                    .get_device_proc_addr(device, c"vkQueuePresentKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_debug_marker_set_object_name_ext: if extensions.ext_debug_marker {
                instance
                    .get_device_proc_addr(device, c"vkDebugMarkerSetObjectNameEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_debug_marker_set_object_tag_ext: if extensions.ext_debug_marker {
                instance
                    .get_device_proc_addr(device, c"vkDebugMarkerSetObjectTagEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_debug_marker_begin_ext: if extensions.ext_debug_marker {
                instance
                    .get_device_proc_addr(device, c"vkCmdDebugMarkerBeginEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_debug_marker_end_ext: if extensions.ext_debug_marker {
                instance
                    .get_device_proc_addr(device, c"vkCmdDebugMarkerEndEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_debug_marker_insert_ext: if extensions.ext_debug_marker {
                instance
                    .get_device_proc_addr(device, c"vkCmdDebugMarkerInsertEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_win32_handle_nv: if extensions.nv_external_memory_win32 {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryWin32HandleNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_execute_generated_commands_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCmdExecuteGeneratedCommandsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_preprocess_generated_commands_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCmdPreprocessGeneratedCommandsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_pipeline_shader_group_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindPipelineShaderGroupNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_generated_commands_memory_requirements_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkGetGeneratedCommandsMemoryRequirementsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_indirect_commands_layout_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCreateIndirectCommandsLayoutNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_indirect_commands_layout_nv: if extensions.nv_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkDestroyIndirectCommandsLayoutNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_execute_generated_commands_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCmdExecuteGeneratedCommandsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_preprocess_generated_commands_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCmdPreprocessGeneratedCommandsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_generated_commands_memory_requirements_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkGetGeneratedCommandsMemoryRequirementsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_indirect_commands_layout_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCreateIndirectCommandsLayoutEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_indirect_commands_layout_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkDestroyIndirectCommandsLayoutEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_indirect_execution_set_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkCreateIndirectExecutionSetEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_indirect_execution_set_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkDestroyIndirectExecutionSetEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_update_indirect_execution_set_pipeline_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkUpdateIndirectExecutionSetPipelineEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_update_indirect_execution_set_shader_ext: if extensions.ext_device_generated_commands {
                instance
                    .get_device_proc_addr(device, c"vkUpdateIndirectExecutionSetShaderEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_descriptor_set: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSet")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_push_descriptor {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSetKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_trim_command_pool: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkTrimCommandPool")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance1 {
                instance
                    .get_device_proc_addr(device, c"vkTrimCommandPoolKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_win32_handle_khr: if extensions.khr_external_memory_win32 {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryWin32HandleKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_win32_handle_properties_khr: if extensions.khr_external_memory_win32 {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryWin32HandlePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_fd_khr: if extensions.khr_external_memory_fd {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryFdKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_fd_properties_khr: if extensions.khr_external_memory_fd {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryFdPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_zircon_handle_fuchsia: if extensions.fuchsia_external_memory {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryZirconHandleFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_zircon_handle_properties_fuchsia: if extensions.fuchsia_external_memory {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryZirconHandlePropertiesFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_remote_address_nv: if extensions.nv_external_memory_rdma {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryRemoteAddressNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_semaphore_win32_handle_khr: if extensions.khr_external_semaphore_win32 {
                instance
                    .get_device_proc_addr(device, c"vkGetSemaphoreWin32HandleKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_import_semaphore_win32_handle_khr: if extensions.khr_external_semaphore_win32 {
                instance
                    .get_device_proc_addr(device, c"vkImportSemaphoreWin32HandleKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_semaphore_fd_khr: if extensions.khr_external_semaphore_fd {
                instance
                    .get_device_proc_addr(device, c"vkGetSemaphoreFdKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_import_semaphore_fd_khr: if extensions.khr_external_semaphore_fd {
                instance
                    .get_device_proc_addr(device, c"vkImportSemaphoreFdKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_semaphore_zircon_handle_fuchsia: if extensions.fuchsia_external_semaphore {
                instance
                    .get_device_proc_addr(device, c"vkGetSemaphoreZirconHandleFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_import_semaphore_zircon_handle_fuchsia: if extensions.fuchsia_external_semaphore {
                instance
                    .get_device_proc_addr(device, c"vkImportSemaphoreZirconHandleFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_fence_win32_handle_khr: if extensions.khr_external_fence_win32 {
                instance
                    .get_device_proc_addr(device, c"vkGetFenceWin32HandleKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_import_fence_win32_handle_khr: if extensions.khr_external_fence_win32 {
                instance
                    .get_device_proc_addr(device, c"vkImportFenceWin32HandleKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_fence_fd_khr: if extensions.khr_external_fence_fd {
                instance
                    .get_device_proc_addr(device, c"vkGetFenceFdKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_import_fence_fd_khr: if extensions.khr_external_fence_fd {
                instance
                    .get_device_proc_addr(device, c"vkImportFenceFdKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_winrt_display_nv: if extensions.nv_acquire_winrt_display {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkAcquireWinrtDisplayNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_winrt_display_nv: if extensions.nv_acquire_winrt_display {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetWinrtDisplayNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_display_power_control_ext: if extensions.ext_display_control {
                instance
                    .get_device_proc_addr(device, c"vkDisplayPowerControlEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_register_device_event_ext: if extensions.ext_display_control {
                instance
                    .get_device_proc_addr(device, c"vkRegisterDeviceEventEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_register_display_event_ext: if extensions.ext_display_control {
                instance
                    .get_device_proc_addr(device, c"vkRegisterDisplayEventEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_swapchain_counter_ext: if extensions.ext_display_control {
                instance
                    .get_device_proc_addr(device, c"vkGetSwapchainCounterEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_group_peer_memory_features: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceGroupPeerMemoryFeatures")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_device_group {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceGroupPeerMemoryFeaturesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_buffer_memory2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkBindBufferMemory2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_bind_memory2 {
                instance
                    .get_device_proc_addr(device, c"vkBindBufferMemory2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_image_memory2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkBindImageMemory2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_bind_memory2 {
                instance
                    .get_device_proc_addr(device, c"vkBindImageMemory2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_device_mask: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDeviceMask")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_device_group {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDeviceMaskKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_group_present_capabilities_khr: if (extensions.khr_swapchain
                && extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0))
                || (extensions.khr_device_group && instance.extensions.khr_surface)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceGroupPresentCapabilitiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_group_surface_present_modes_khr: if (extensions.khr_swapchain
                && extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0))
                || (extensions.khr_device_group && instance.extensions.khr_surface)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceGroupSurfacePresentModesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_next_image2_khr: if (extensions.khr_swapchain
                && extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0))
                || (extensions.khr_device_group && extensions.khr_swapchain)
            {
                instance
                    .get_device_proc_addr(device, c"vkAcquireNextImage2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_base: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchBase")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_device_group {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchBaseKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_present_rectangles_khr: if (extensions.khr_swapchain
                && extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0))
                || (extensions.khr_device_group && instance.extensions.khr_surface)
            {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDevicePresentRectanglesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_descriptor_update_template: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCreateDescriptorUpdateTemplate")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_descriptor_update_template {
                instance
                    .get_device_proc_addr(device, c"vkCreateDescriptorUpdateTemplateKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_descriptor_update_template: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkDestroyDescriptorUpdateTemplate")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_descriptor_update_template {
                instance
                    .get_device_proc_addr(device, c"vkDestroyDescriptorUpdateTemplateKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_update_descriptor_set_with_template: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkUpdateDescriptorSetWithTemplate")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_descriptor_update_template {
                instance
                    .get_device_proc_addr(device, c"vkUpdateDescriptorSetWithTemplateKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_descriptor_set_with_template: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSetWithTemplate")
                    .map(|f| mem::transmute(f))
            } else if (extensions.khr_push_descriptor
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0)
                    || extensions.khr_descriptor_update_template))
                || (extensions.khr_descriptor_update_template && extensions.khr_push_descriptor)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSetWithTemplateKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_hdr_metadata_ext: if extensions.ext_hdr_metadata {
                instance
                    .get_device_proc_addr(device, c"vkSetHdrMetadataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_swapchain_status_khr: if extensions.khr_shared_presentable_image {
                instance
                    .get_device_proc_addr(device, c"vkGetSwapchainStatusKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_refresh_cycle_duration_google: if extensions.google_display_timing {
                instance
                    .get_device_proc_addr(device, c"vkGetRefreshCycleDurationGOOGLE")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_past_presentation_timing_google: if extensions.google_display_timing {
                instance
                    .get_device_proc_addr(device, c"vkGetPastPresentationTimingGOOGLE")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport_w_scaling_nv: if extensions.nv_clip_space_w_scaling {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportWScalingNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_discard_rectangle_ext: if extensions.ext_discard_rectangles {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDiscardRectangleEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_discard_rectangle_enable_ext: if extensions.ext_discard_rectangles {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDiscardRectangleEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_discard_rectangle_mode_ext: if extensions.ext_discard_rectangles {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDiscardRectangleModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_sample_locations_ext: if extensions.ext_sample_locations {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetSampleLocationsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_multisample_properties_ext: if extensions.ext_sample_locations {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceMultisamplePropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_buffer_memory_requirements2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferMemoryRequirements2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_memory_requirements2 {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferMemoryRequirements2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_memory_requirements2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetImageMemoryRequirements2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_memory_requirements2 {
                instance
                    .get_device_proc_addr(device, c"vkGetImageMemoryRequirements2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_sparse_memory_requirements2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetImageSparseMemoryRequirements2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_get_memory_requirements2 {
                instance
                    .get_device_proc_addr(device, c"vkGetImageSparseMemoryRequirements2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_buffer_memory_requirements: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceBufferMemoryRequirements")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance4 {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceBufferMemoryRequirementsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_image_memory_requirements: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageMemoryRequirements")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance4 {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageMemoryRequirementsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_image_sparse_memory_requirements: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 3, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageSparseMemoryRequirements")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance4 {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageSparseMemoryRequirementsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_sampler_ycbcr_conversion: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCreateSamplerYcbcrConversion")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_sampler_ycbcr_conversion {
                instance
                    .get_device_proc_addr(device, c"vkCreateSamplerYcbcrConversionKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_sampler_ycbcr_conversion: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkDestroySamplerYcbcrConversion")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_sampler_ycbcr_conversion {
                instance
                    .get_device_proc_addr(device, c"vkDestroySamplerYcbcrConversionKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_queue2: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceQueue2")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_validation_cache_ext: if extensions.ext_validation_cache {
                instance
                    .get_device_proc_addr(device, c"vkCreateValidationCacheEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_validation_cache_ext: if extensions.ext_validation_cache {
                instance
                    .get_device_proc_addr(device, c"vkDestroyValidationCacheEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_validation_cache_data_ext: if extensions.ext_validation_cache {
                instance
                    .get_device_proc_addr(device, c"vkGetValidationCacheDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_merge_validation_caches_ext: if extensions.ext_validation_cache {
                instance
                    .get_device_proc_addr(device, c"vkMergeValidationCachesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_set_layout_support: if extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetLayoutSupport")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance3 {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetLayoutSupportKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_shader_info_amd: if extensions.amd_shader_info {
                instance
                    .get_device_proc_addr(device, c"vkGetShaderInfoAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_local_dimming_amd: if extensions.amd_display_native_hdr {
                instance
                    .get_device_proc_addr(device, c"vkSetLocalDimmingAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_calibrateable_time_domains_khr: if extensions.khr_calibrated_timestamps {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceCalibrateableTimeDomainsKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_calibrated_timestamps {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceCalibrateableTimeDomainsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_calibrated_timestamps_khr: if extensions.khr_calibrated_timestamps {
                instance
                    .get_device_proc_addr(device, c"vkGetCalibratedTimestampsKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_calibrated_timestamps {
                instance
                    .get_device_proc_addr(device, c"vkGetCalibratedTimestampsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_host_pointer_properties_ext: if extensions.ext_external_memory_host {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryHostPointerPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_buffer_marker_amd: if extensions.amd_buffer_marker {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteBufferMarkerAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_render_pass2: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCreateRenderPass2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_create_renderpass2 {
                instance
                    .get_device_proc_addr(device, c"vkCreateRenderPass2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_render_pass2: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginRenderPass2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_create_renderpass2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginRenderPass2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_next_subpass2: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdNextSubpass2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_create_renderpass2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdNextSubpass2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_render_pass2: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRenderPass2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_create_renderpass2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRenderPass2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_semaphore_counter_value: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetSemaphoreCounterValue")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_timeline_semaphore {
                instance
                    .get_device_proc_addr(device, c"vkGetSemaphoreCounterValueKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_wait_semaphores: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkWaitSemaphores")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_timeline_semaphore {
                instance
                    .get_device_proc_addr(device, c"vkWaitSemaphoresKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_signal_semaphore: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkSignalSemaphore")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_timeline_semaphore {
                instance
                    .get_device_proc_addr(device, c"vkSignalSemaphoreKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_android_hardware_buffer_properties_android: if extensions
                .android_external_memory_android_hardware_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkGetAndroidHardwareBufferPropertiesANDROID")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_android_hardware_buffer_android: if extensions.android_external_memory_android_hardware_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryAndroidHardwareBufferANDROID")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_indirect_count: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndirectCount")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_draw_indirect_count {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndirectCountKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.amd_draw_indirect_count {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndirectCountAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_indexed_indirect_count: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndexedIndirectCount")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_draw_indirect_count {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndexedIndirectCountKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.amd_draw_indirect_count {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndexedIndirectCountAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_checkpoint_nv: if extensions.nv_device_diagnostic_checkpoints {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCheckpointNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_queue_checkpoint_data_nv: if extensions.nv_device_diagnostic_checkpoints {
                instance
                    .get_device_proc_addr(device, c"vkGetQueueCheckpointDataNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_transform_feedback_buffers_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindTransformFeedbackBuffersEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_transform_feedback_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginTransformFeedbackEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_transform_feedback_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndTransformFeedbackEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_query_indexed_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginQueryIndexedEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_query_indexed_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndQueryIndexedEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_indirect_byte_count_ext: if extensions.ext_transform_feedback {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawIndirectByteCountEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_exclusive_scissor_nv: if extensions.nv_scissor_exclusive {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetExclusiveScissorNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_exclusive_scissor_enable_nv: if extensions.nv_scissor_exclusive {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetExclusiveScissorEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_shading_rate_image_nv: if extensions.nv_shading_rate_image {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindShadingRateImageNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport_shading_rate_palette_nv: if extensions.nv_shading_rate_image {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportShadingRatePaletteNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coarse_sample_order_nv: if extensions.nv_shading_rate_image {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoarseSampleOrderNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_nv: if extensions.nv_mesh_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_indirect_nv: if extensions.nv_mesh_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksIndirectNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_indirect_count_nv: if extensions.nv_mesh_shader
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || extensions.khr_draw_indirect_count
                    || extensions.amd_draw_indirect_count)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksIndirectCountNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_ext: if extensions.ext_mesh_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_indirect_ext: if extensions.ext_mesh_shader {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksIndirectEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_draw_mesh_tasks_indirect_count_ext: if extensions.ext_mesh_shader
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0)
                    || extensions.khr_draw_indirect_count
                    || extensions.amd_draw_indirect_count)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdDrawMeshTasksIndirectCountEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_compile_deferred_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCompileDeferredNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_acceleration_structure_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCreateAccelerationStructureNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_invocation_mask_huawei: if extensions.huawei_invocation_mask {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindInvocationMaskHUAWEI")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkDestroyAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_acceleration_structure_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkDestroyAccelerationStructureNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_acceleration_structure_memory_requirements_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkGetAccelerationStructureMemoryRequirementsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_acceleration_structure_memory_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkBindAccelerationStructureMemoryNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_acceleration_structure_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyAccelerationStructureNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCopyAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_acceleration_structure_to_memory_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyAccelerationStructureToMemoryKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_acceleration_structure_to_memory_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCopyAccelerationStructureToMemoryKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_memory_to_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryToAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_memory_to_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCopyMemoryToAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_acceleration_structures_properties_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteAccelerationStructuresPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_acceleration_structures_properties_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteAccelerationStructuresPropertiesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_acceleration_structure_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildAccelerationStructureNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_write_acceleration_structures_properties_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkWriteAccelerationStructuresPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_trace_rays_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkCmdTraceRaysKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_trace_rays_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCmdTraceRaysNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_ray_tracing_shader_group_handles_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkGetRayTracingShaderGroupHandlesKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkGetRayTracingShaderGroupHandlesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_ray_tracing_capture_replay_shader_group_handles_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkGetRayTracingCaptureReplayShaderGroupHandlesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_acceleration_structure_handle_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkGetAccelerationStructureHandleNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_ray_tracing_pipelines_nv: if extensions.nv_ray_tracing {
                instance
                    .get_device_proc_addr(device, c"vkCreateRayTracingPipelinesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_ray_tracing_pipelines_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkCreateRayTracingPipelinesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_cooperative_matrix_properties_nv: if extensions.nv_cooperative_matrix {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceCooperativeMatrixPropertiesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_trace_rays_indirect_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkCmdTraceRaysIndirectKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_trace_rays_indirect2_khr: if extensions.khr_ray_tracing_maintenance1
                && extensions.khr_ray_tracing_pipeline
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdTraceRaysIndirect2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_cluster_acceleration_structure_build_sizes_nv: if extensions.nv_cluster_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkGetClusterAccelerationStructureBuildSizesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_cluster_acceleration_structure_indirect_nv: if extensions.nv_cluster_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildClusterAccelerationStructureIndirectNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_acceleration_structure_compatibility_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceAccelerationStructureCompatibilityKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_ray_tracing_shader_group_stack_size_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkGetRayTracingShaderGroupStackSizeKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_ray_tracing_pipeline_stack_size_khr: if extensions.khr_ray_tracing_pipeline {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRayTracingPipelineStackSizeKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_view_handle_nvx: if extensions.nvx_image_view_handle {
                instance
                    .get_device_proc_addr(device, c"vkGetImageViewHandleNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_view_handle64_nvx: if extensions.nvx_image_view_handle {
                instance
                    .get_device_proc_addr(device, c"vkGetImageViewHandle64NVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_view_address_nvx: if extensions.nvx_image_view_handle {
                instance
                    .get_device_proc_addr(device, c"vkGetImageViewAddressNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_combined_image_sampler_index_nvx: if extensions.nvx_image_view_handle {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceCombinedImageSamplerIndexNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_surface_present_modes2_ext: if extensions.ext_full_screen_exclusive {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceSurfacePresentModes2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_group_surface_present_modes2_ext: if extensions.ext_full_screen_exclusive
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) || extensions.khr_device_group)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceGroupSurfacePresentModes2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_full_screen_exclusive_mode_ext: if extensions.ext_full_screen_exclusive {
                instance
                    .get_device_proc_addr(device, c"vkAcquireFullScreenExclusiveModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_full_screen_exclusive_mode_ext: if extensions.ext_full_screen_exclusive {
                instance
                    .get_device_proc_addr(device, c"vkReleaseFullScreenExclusiveModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_enumerate_physical_device_queue_family_performance_query_counters_khr: if extensions
                .khr_performance_query
            {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkEnumeratePhysicalDeviceQueueFamilyPerformanceQueryCountersKHR",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_queue_family_performance_query_passes_khr: if extensions.khr_performance_query {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkGetPhysicalDeviceQueueFamilyPerformanceQueryPassesKHR",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_profiling_lock_khr: if extensions.khr_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkAcquireProfilingLockKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_profiling_lock_khr: if extensions.khr_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkReleaseProfilingLockKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_drm_format_modifier_properties_ext: if extensions.ext_image_drm_format_modifier {
                instance
                    .get_device_proc_addr(device, c"vkGetImageDrmFormatModifierPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_buffer_opaque_capture_address: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferOpaqueCaptureAddress")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_buffer_device_address {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferOpaqueCaptureAddressKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_buffer_device_address: if extensions.core_version >= vk::Version::from_raw_parts(1, 2, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferDeviceAddress")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_buffer_device_address {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferDeviceAddressKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_buffer_device_address {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferDeviceAddressEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_supported_framebuffer_mixed_samples_combinations_nv: if extensions
                .nv_coverage_reduction_mode
            {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkGetPhysicalDeviceSupportedFramebufferMixedSamplesCombinationsNV",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_initialize_performance_api_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkInitializePerformanceApiINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_uninitialize_performance_api_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkUninitializePerformanceApiINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_performance_marker_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPerformanceMarkerINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_performance_stream_marker_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPerformanceStreamMarkerINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_performance_override_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPerformanceOverrideINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_acquire_performance_configuration_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkAcquirePerformanceConfigurationINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_performance_configuration_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkReleasePerformanceConfigurationINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_set_performance_configuration_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkQueueSetPerformanceConfigurationINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_performance_parameter_intel: if extensions.intel_performance_query {
                instance
                    .get_device_proc_addr(device, c"vkGetPerformanceParameterINTEL")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_memory_opaque_capture_address: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 2, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceMemoryOpaqueCaptureAddress")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_buffer_device_address {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceMemoryOpaqueCaptureAddressKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_executable_properties_khr: if extensions.khr_pipeline_executable_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineExecutablePropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_executable_statistics_khr: if extensions.khr_pipeline_executable_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineExecutableStatisticsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_executable_internal_representations_khr: if extensions.khr_pipeline_executable_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineExecutableInternalRepresentationsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_line_stipple: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineStipple")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_line_rasterization {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineStippleKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_line_rasterization {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineStippleEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_tool_properties: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceToolProperties")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_tooling_info {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceToolPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_acceleration_structure_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCreateAccelerationStructureKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_acceleration_structures_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildAccelerationStructuresKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_acceleration_structures_indirect_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildAccelerationStructuresIndirectKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_build_acceleration_structures_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkBuildAccelerationStructuresKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_acceleration_structure_device_address_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkGetAccelerationStructureDeviceAddressKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_deferred_operation_khr: if extensions.khr_deferred_host_operations {
                instance
                    .get_device_proc_addr(device, c"vkCreateDeferredOperationKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_deferred_operation_khr: if extensions.khr_deferred_host_operations {
                instance
                    .get_device_proc_addr(device, c"vkDestroyDeferredOperationKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_deferred_operation_max_concurrency_khr: if extensions.khr_deferred_host_operations {
                instance
                    .get_device_proc_addr(device, c"vkGetDeferredOperationMaxConcurrencyKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_deferred_operation_result_khr: if extensions.khr_deferred_host_operations {
                instance
                    .get_device_proc_addr(device, c"vkGetDeferredOperationResultKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_deferred_operation_join_khr: if extensions.khr_deferred_host_operations {
                instance
                    .get_device_proc_addr(device, c"vkDeferredOperationJoinKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_indirect_memory_requirements_nv: if extensions.nv_device_generated_commands_compute {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineIndirectMemoryRequirementsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_indirect_device_address_nv: if extensions.nv_device_generated_commands_compute {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelineIndirectDeviceAddressNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_anti_lag_update_amd: if extensions.amd_anti_lag {
                instance
                    .get_device_proc_addr(device, c"vkAntiLagUpdateAMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_cull_mode: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCullMode")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCullModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_front_face: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetFrontFace")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetFrontFaceEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_primitive_topology: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPrimitiveTopology")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPrimitiveTopologyEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport_with_count: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportWithCount")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportWithCountEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_scissor_with_count: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetScissorWithCount")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetScissorWithCountEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_index_buffer2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindIndexBuffer2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance5 {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindIndexBuffer2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_vertex_buffers2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindVertexBuffers2")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindVertexBuffers2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_test_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthTestEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthTestEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_write_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthWriteEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthWriteEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_compare_op: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthCompareOp")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthCompareOpEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_bounds_test_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBoundsTestEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBoundsTestEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_stencil_test_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilTestEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilTestEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_stencil_op: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilOp")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetStencilOpEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_patch_control_points_ext: if extensions.ext_extended_dynamic_state2
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPatchControlPointsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_rasterizer_discard_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRasterizerDiscardEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state2 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRasterizerDiscardEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_bias_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBiasEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state2 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBiasEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_logic_op_ext: if extensions.ext_extended_dynamic_state2 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLogicOpEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_primitive_restart_enable: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPrimitiveRestartEnable")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_extended_dynamic_state2 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPrimitiveRestartEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_tessellation_domain_origin_ext: if (extensions.ext_extended_dynamic_state3
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 1, 0) || extensions.khr_maintenance2))
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetTessellationDomainOriginEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_clamp_enable_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthClampEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_polygon_mode_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetPolygonModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_rasterization_samples_ext: if extensions.ext_extended_dynamic_state3
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRasterizationSamplesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_sample_mask_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetSampleMaskEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_alpha_to_coverage_enable_ext: if extensions.ext_extended_dynamic_state3
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetAlphaToCoverageEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_alpha_to_one_enable_ext: if extensions.ext_extended_dynamic_state3
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetAlphaToOneEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_logic_op_enable_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLogicOpEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_color_blend_enable_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetColorBlendEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_color_blend_equation_ext: if extensions.ext_extended_dynamic_state3
                || extensions.ext_shader_object
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetColorBlendEquationEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_color_write_mask_ext: if extensions.ext_extended_dynamic_state3 || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetColorWriteMaskEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_rasterization_stream_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_transform_feedback)
                || (extensions.ext_shader_object && extensions.ext_transform_feedback)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRasterizationStreamEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_conservative_rasterization_mode_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_conservative_rasterization)
                || (extensions.ext_shader_object && extensions.ext_conservative_rasterization)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetConservativeRasterizationModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_extra_primitive_overestimation_size_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_conservative_rasterization)
                || (extensions.ext_shader_object && extensions.ext_conservative_rasterization)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetExtraPrimitiveOverestimationSizeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_clip_enable_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_depth_clip_enable)
                || (extensions.ext_shader_object && extensions.ext_depth_clip_enable)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthClipEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_sample_locations_enable_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_sample_locations)
                || (extensions.ext_shader_object && extensions.ext_sample_locations)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetSampleLocationsEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_color_blend_advanced_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_blend_operation_advanced)
                || (extensions.ext_shader_object && extensions.ext_blend_operation_advanced)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetColorBlendAdvancedEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_provoking_vertex_mode_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_provoking_vertex)
                || (extensions.ext_shader_object && extensions.ext_provoking_vertex)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetProvokingVertexModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_line_rasterization_mode_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_line_rasterization)
                || (extensions.ext_shader_object && extensions.ext_line_rasterization)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineRasterizationModeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_line_stipple_enable_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_line_rasterization)
                || (extensions.ext_shader_object && extensions.ext_line_rasterization)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetLineStippleEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_clip_negative_one_to_one_ext: if (extensions.ext_extended_dynamic_state3
                && extensions.ext_depth_clip_control)
                || (extensions.ext_shader_object && extensions.ext_depth_clip_control)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthClipNegativeOneToOneEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport_w_scaling_enable_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_clip_space_w_scaling)
                || (extensions.ext_shader_object && extensions.nv_clip_space_w_scaling)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportWScalingEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_viewport_swizzle_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_viewport_swizzle)
                || (extensions.ext_shader_object && extensions.nv_viewport_swizzle)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetViewportSwizzleNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_to_color_enable_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_fragment_coverage_to_color)
                || (extensions.ext_shader_object && extensions.nv_fragment_coverage_to_color)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageToColorEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_to_color_location_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_fragment_coverage_to_color)
                || (extensions.ext_shader_object && extensions.nv_fragment_coverage_to_color)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageToColorLocationNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_modulation_mode_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_framebuffer_mixed_samples)
                || (extensions.ext_shader_object && extensions.nv_framebuffer_mixed_samples)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageModulationModeNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_modulation_table_enable_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_framebuffer_mixed_samples)
                || (extensions.ext_shader_object && extensions.nv_framebuffer_mixed_samples)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageModulationTableEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_modulation_table_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_framebuffer_mixed_samples)
                || (extensions.ext_shader_object && extensions.nv_framebuffer_mixed_samples)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageModulationTableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_shading_rate_image_enable_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_shading_rate_image)
                || (extensions.ext_shader_object && extensions.nv_shading_rate_image)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetShadingRateImageEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_coverage_reduction_mode_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_coverage_reduction_mode)
                || (extensions.ext_shader_object && extensions.nv_coverage_reduction_mode)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetCoverageReductionModeNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_representative_fragment_test_enable_nv: if (extensions.ext_extended_dynamic_state3
                && extensions.nv_representative_fragment_test)
                || (extensions.ext_shader_object && extensions.nv_representative_fragment_test)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRepresentativeFragmentTestEnableNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_private_data_slot: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCreatePrivateDataSlot")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_private_data {
                instance
                    .get_device_proc_addr(device, c"vkCreatePrivateDataSlotEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_private_data_slot: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkDestroyPrivateDataSlot")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_private_data {
                instance
                    .get_device_proc_addr(device, c"vkDestroyPrivateDataSlotEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_private_data: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkSetPrivateData")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_private_data {
                instance
                    .get_device_proc_addr(device, c"vkSetPrivateDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_private_data: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetPrivateData")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_private_data {
                instance
                    .get_device_proc_addr(device, c"vkGetPrivateDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_buffer2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBuffer2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBuffer2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_image2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImage2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImage2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_blit_image2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBlitImage2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdBlitImage2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_buffer_to_image2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBufferToImage2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyBufferToImage2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_image_to_buffer2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImageToBuffer2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyImageToBuffer2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_resolve_image2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdResolveImage2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_copy_commands2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdResolveImage2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_fragment_shading_rate_khr: if extensions.khr_fragment_shading_rate {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetFragmentShadingRateKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_fragment_shading_rates_khr: if extensions.khr_fragment_shading_rate {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceFragmentShadingRatesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_fragment_shading_rate_enum_nv: if extensions.nv_fragment_shading_rate_enums {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetFragmentShadingRateEnumNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_acceleration_structure_build_sizes_khr: if extensions.khr_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkGetAccelerationStructureBuildSizesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_vertex_input_ext: if extensions.ext_vertex_input_dynamic_state || extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetVertexInputEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_color_write_enable_ext: if extensions.ext_color_write_enable {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetColorWriteEnableEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_event2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetEvent2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetEvent2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_reset_event2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdResetEvent2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdResetEvent2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_wait_events2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdWaitEvents2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdWaitEvents2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_pipeline_barrier2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdPipelineBarrier2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdPipelineBarrier2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_submit2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkQueueSubmit2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkQueueSubmit2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_timestamp2: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteTimestamp2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_synchronization2 {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteTimestamp2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_buffer_marker2_amd: if extensions.amd_buffer_marker
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) || extensions.khr_synchronization2)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteBufferMarker2AMD")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_queue_checkpoint_data2_nv: if extensions.nv_device_diagnostic_checkpoints
                && (extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) || extensions.khr_synchronization2)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetQueueCheckpointData2NV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_memory_to_image: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCopyMemoryToImage")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_image_copy {
                instance
                    .get_device_proc_addr(device, c"vkCopyMemoryToImageEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_image_to_memory: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCopyImageToMemory")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_image_copy {
                instance
                    .get_device_proc_addr(device, c"vkCopyImageToMemoryEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_image_to_image: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCopyImageToImage")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_image_copy {
                instance
                    .get_device_proc_addr(device, c"vkCopyImageToImageEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_transition_image_layout: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkTransitionImageLayout")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_image_copy {
                instance
                    .get_device_proc_addr(device, c"vkTransitionImageLayoutEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_decompress_memory_nv: if extensions.nv_memory_decompression {
                instance
                    .get_device_proc_addr(device, c"vkCmdDecompressMemoryNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_decompress_memory_indirect_count_nv: if extensions.nv_memory_decompression {
                instance
                    .get_device_proc_addr(device, c"vkCmdDecompressMemoryIndirectCountNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_partitioned_acceleration_structures_build_sizes_nv: if extensions
                .nv_partitioned_acceleration_structure
            {
                instance
                    .get_device_proc_addr(device, c"vkGetPartitionedAccelerationStructuresBuildSizesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_partitioned_acceleration_structures_nv: if extensions.nv_partitioned_acceleration_structure {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildPartitionedAccelerationStructuresNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_decompress_memory_ext: if extensions.ext_memory_decompression {
                instance
                    .get_device_proc_addr(device, c"vkCmdDecompressMemoryEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_decompress_memory_indirect_count_ext: if extensions.ext_memory_decompression {
                instance
                    .get_device_proc_addr(device, c"vkCmdDecompressMemoryIndirectCountEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_cu_module_nvx: if extensions.nvx_binary_import {
                instance
                    .get_device_proc_addr(device, c"vkCreateCuModuleNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_cu_function_nvx: if extensions.nvx_binary_import {
                instance
                    .get_device_proc_addr(device, c"vkCreateCuFunctionNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_cu_module_nvx: if extensions.nvx_binary_import {
                instance
                    .get_device_proc_addr(device, c"vkDestroyCuModuleNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_cu_function_nvx: if extensions.nvx_binary_import {
                instance
                    .get_device_proc_addr(device, c"vkDestroyCuFunctionNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_cu_launch_kernel_nvx: if extensions.nvx_binary_import {
                instance
                    .get_device_proc_addr(device, c"vkCmdCuLaunchKernelNVX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_set_layout_size_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetLayoutSizeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_set_layout_binding_offset_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetLayoutBindingOffsetEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_descriptor_buffers_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorBuffersEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_descriptor_buffer_offsets_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDescriptorBufferOffsetsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_descriptor_buffer_embedded_samplers_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorBufferEmbeddedSamplersEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_buffer_opaque_capture_descriptor_data_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferOpaqueCaptureDescriptorDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_opaque_capture_descriptor_data_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetImageOpaqueCaptureDescriptorDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_view_opaque_capture_descriptor_data_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetImageViewOpaqueCaptureDescriptorDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_sampler_opaque_capture_descriptor_data_ext: if extensions.ext_descriptor_buffer {
                instance
                    .get_device_proc_addr(device, c"vkGetSamplerOpaqueCaptureDescriptorDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_acceleration_structure_opaque_capture_descriptor_data_ext: if extensions.ext_descriptor_buffer
                && (extensions.khr_acceleration_structure || extensions.nv_ray_tracing)
            {
                instance
                    .get_device_proc_addr(device, c"vkGetAccelerationStructureOpaqueCaptureDescriptorDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_device_memory_priority_ext: if extensions.ext_pageable_device_local_memory {
                instance
                    .get_device_proc_addr(device, c"vkSetDeviceMemoryPriorityEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_wait_for_present2_khr: if extensions.khr_present_wait2 {
                instance
                    .get_device_proc_addr(device, c"vkWaitForPresent2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_wait_for_present_khr: if extensions.khr_present_wait {
                instance
                    .get_device_proc_addr(device, c"vkWaitForPresentKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_buffer_collection_fuchsia: if extensions.fuchsia_buffer_collection {
                instance
                    .get_device_proc_addr(device, c"vkCreateBufferCollectionFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_buffer_collection_buffer_constraints_fuchsia: if extensions.fuchsia_buffer_collection {
                instance
                    .get_device_proc_addr(device, c"vkSetBufferCollectionBufferConstraintsFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_buffer_collection_image_constraints_fuchsia: if extensions.fuchsia_buffer_collection {
                instance
                    .get_device_proc_addr(device, c"vkSetBufferCollectionImageConstraintsFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_buffer_collection_fuchsia: if extensions.fuchsia_buffer_collection {
                instance
                    .get_device_proc_addr(device, c"vkDestroyBufferCollectionFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_buffer_collection_properties_fuchsia: if extensions.fuchsia_buffer_collection {
                instance
                    .get_device_proc_addr(device, c"vkGetBufferCollectionPropertiesFUCHSIA")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_cuda_module_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkCreateCudaModuleNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_cuda_module_cache_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkGetCudaModuleCacheNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_cuda_function_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkCreateCudaFunctionNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_cuda_module_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkDestroyCudaModuleNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_cuda_function_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkDestroyCudaFunctionNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_cuda_launch_kernel_nv: if extensions.nv_cuda_kernel_launch {
                instance
                    .get_device_proc_addr(device, c"vkCmdCudaLaunchKernelNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_rendering: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginRendering")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_dynamic_rendering {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginRenderingKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_rendering: if extensions.core_version >= vk::Version::from_raw_parts(1, 3, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRendering")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_dynamic_rendering {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRenderingKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_rendering2_khr: if extensions.khr_maintenance10 {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRendering2KHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_fragment_density_map_offset {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndRendering2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_set_layout_host_mapping_info_valve: if extensions.valve_descriptor_set_host_mapping {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetLayoutHostMappingInfoVALVE")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_descriptor_set_host_mapping_valve: if extensions.valve_descriptor_set_host_mapping {
                instance
                    .get_device_proc_addr(device, c"vkGetDescriptorSetHostMappingVALVE")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCreateMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_build_micromaps_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCmdBuildMicromapsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_build_micromaps_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkBuildMicromapsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkDestroyMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCopyMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_micromap_to_memory_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMicromapToMemoryEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_micromap_to_memory_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCopyMicromapToMemoryEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_memory_to_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyMemoryToMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_copy_memory_to_micromap_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCopyMemoryToMicromapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_write_micromaps_properties_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkCmdWriteMicromapsPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_write_micromaps_properties_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkWriteMicromapsPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_micromap_compatibility_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceMicromapCompatibilityEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_micromap_build_sizes_ext: if extensions.ext_opacity_micromap {
                instance
                    .get_device_proc_addr(device, c"vkGetMicromapBuildSizesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_shader_module_identifier_ext: if extensions.ext_shader_module_identifier {
                instance
                    .get_device_proc_addr(device, c"vkGetShaderModuleIdentifierEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_shader_module_create_info_identifier_ext: if extensions.ext_shader_module_identifier {
                instance
                    .get_device_proc_addr(device, c"vkGetShaderModuleCreateInfoIdentifierEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_subresource_layout2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetImageSubresourceLayout2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance5 {
                instance
                    .get_device_proc_addr(device, c"vkGetImageSubresourceLayout2KHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_host_image_copy || extensions.ext_image_compression_control {
                instance
                    .get_device_proc_addr(device, c"vkGetImageSubresourceLayout2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_pipeline_properties_ext: if extensions.ext_pipeline_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetPipelinePropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_export_metal_objects_ext: if extensions.ext_metal_objects {
                instance
                    .get_device_proc_addr(device, c"vkExportMetalObjectsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_tile_memory_qcom: if extensions.qcom_tile_memory_heap {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindTileMemoryQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_framebuffer_tile_properties_qcom: if extensions.qcom_tile_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetFramebufferTilePropertiesQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_dynamic_rendering_tile_properties_qcom: if extensions.qcom_tile_properties {
                instance
                    .get_device_proc_addr(device, c"vkGetDynamicRenderingTilePropertiesQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_optical_flow_image_formats_nv: if extensions.nv_optical_flow {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceOpticalFlowImageFormatsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_optical_flow_session_nv: if extensions.nv_optical_flow {
                instance
                    .get_device_proc_addr(device, c"vkCreateOpticalFlowSessionNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_optical_flow_session_nv: if extensions.nv_optical_flow {
                instance
                    .get_device_proc_addr(device, c"vkDestroyOpticalFlowSessionNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_optical_flow_session_image_nv: if extensions.nv_optical_flow {
                instance
                    .get_device_proc_addr(device, c"vkBindOpticalFlowSessionImageNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_optical_flow_execute_nv: if extensions.nv_optical_flow {
                instance
                    .get_device_proc_addr(device, c"vkCmdOpticalFlowExecuteNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_fault_info_ext: if extensions.ext_device_fault {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceFaultInfoEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_bias2_ext: if extensions.ext_depth_bias_control {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthBias2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_release_swapchain_images_khr: if extensions.khr_swapchain_maintenance1 {
                instance
                    .get_device_proc_addr(device, c"vkReleaseSwapchainImagesKHR")
                    .map(|f| mem::transmute(f))
            } else if extensions.ext_swapchain_maintenance1 {
                instance
                    .get_device_proc_addr(device, c"vkReleaseSwapchainImagesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_image_subresource_layout: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageSubresourceLayout")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance5 {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceImageSubresourceLayoutKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_map_memory2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkMapMemory2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_map_memory2 {
                instance
                    .get_device_proc_addr(device, c"vkMapMemory2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_unmap_memory2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkUnmapMemory2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_map_memory2 {
                instance
                    .get_device_proc_addr(device, c"vkUnmapMemory2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_shaders_ext: if extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCreateShadersEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_shader_ext: if extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkDestroyShaderEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_shader_binary_data_ext: if extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkGetShaderBinaryDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_shaders_ext: if extensions.ext_shader_object {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindShadersEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_swapchain_present_timing_queue_size_ext: if extensions.ext_present_timing {
                instance
                    .get_device_proc_addr(device, c"vkSetSwapchainPresentTimingQueueSizeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_swapchain_timing_properties_ext: if extensions.ext_present_timing {
                instance
                    .get_device_proc_addr(device, c"vkGetSwapchainTimingPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_swapchain_time_domain_properties_ext: if extensions.ext_present_timing {
                instance
                    .get_device_proc_addr(device, c"vkGetSwapchainTimeDomainPropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_past_presentation_timing_ext: if extensions.ext_present_timing {
                instance
                    .get_device_proc_addr(device, c"vkGetPastPresentationTimingEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_cooperative_matrix_properties_khr: if extensions.khr_cooperative_matrix {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceCooperativeMatrixPropertiesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_execution_graph_pipeline_scratch_size_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkGetExecutionGraphPipelineScratchSizeAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_execution_graph_pipeline_node_index_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkGetExecutionGraphPipelineNodeIndexAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_execution_graph_pipelines_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkCreateExecutionGraphPipelinesAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_initialize_graph_scratch_memory_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkCmdInitializeGraphScratchMemoryAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_graph_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchGraphAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_graph_indirect_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchGraphIndirectAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_graph_indirect_count_amdx: if extensions.amdx_shader_enqueue {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchGraphIndirectCountAMDX")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_descriptor_sets2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorSets2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance6 {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorSets2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_constants2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushConstants2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance6 {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushConstants2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_descriptor_set2: if extensions.core_version >= vk::Version::from_raw_parts(1, 4, 0) {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSet2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance6 && extensions.khr_push_descriptor {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSet2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_descriptor_set_with_template2: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 4, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSetWithTemplate2")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_maintenance6 && extensions.khr_push_descriptor {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDescriptorSetWithTemplate2KHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_descriptor_buffer_offsets2_ext: if extensions.khr_maintenance6
                && extensions.ext_descriptor_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDescriptorBufferOffsets2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_descriptor_buffer_embedded_samplers2_ext: if extensions.khr_maintenance6
                && extensions.ext_descriptor_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindDescriptorBufferEmbeddedSamplers2EXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_latency_sleep_mode_nv: if extensions.nv_low_latency2 {
                instance
                    .get_device_proc_addr(device, c"vkSetLatencySleepModeNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_latency_sleep_nv: if extensions.nv_low_latency2 {
                instance
                    .get_device_proc_addr(device, c"vkLatencySleepNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_set_latency_marker_nv: if extensions.nv_low_latency2 {
                instance
                    .get_device_proc_addr(device, c"vkSetLatencyMarkerNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_latency_timings_nv: if extensions.nv_low_latency2 {
                instance
                    .get_device_proc_addr(device, c"vkGetLatencyTimingsNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_queue_notify_out_of_band_nv: if extensions.nv_low_latency2 {
                instance
                    .get_device_proc_addr(device, c"vkQueueNotifyOutOfBandNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_rendering_attachment_locations: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 4, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRenderingAttachmentLocations")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_dynamic_rendering_local_read {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRenderingAttachmentLocationsKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_rendering_input_attachment_indices: if extensions.core_version
                >= vk::Version::from_raw_parts(1, 4, 0)
            {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRenderingInputAttachmentIndices")
                    .map(|f| mem::transmute(f))
            } else if extensions.khr_dynamic_rendering_local_read {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetRenderingInputAttachmentIndicesKHR")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_depth_clamp_range_ext: if extensions.ext_depth_clamp_control {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetDepthClampRangeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv: if extensions
                .nv_cooperative_matrix2
            {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkGetPhysicalDeviceCooperativeMatrixFlexibleDimensionsPropertiesNV",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_metal_handle_ext: if extensions.ext_external_memory_metal {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryMetalHandleEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_metal_handle_properties_ext: if extensions.ext_external_memory_metal {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryMetalHandlePropertiesEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_cooperative_vector_properties_nv: if extensions.nv_cooperative_vector {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceCooperativeVectorPropertiesNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_convert_cooperative_vector_matrix_nv: if extensions.nv_cooperative_vector {
                instance
                    .get_device_proc_addr(device, c"vkConvertCooperativeVectorMatrixNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_convert_cooperative_vector_matrix_nv: if extensions.nv_cooperative_vector {
                instance
                    .get_device_proc_addr(device, c"vkCmdConvertCooperativeVectorMatrixNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_tile_qcom: if extensions.qcom_tile_shading {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchTileQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_begin_per_tile_execution_qcom: if extensions.qcom_tile_shading {
                instance
                    .get_device_proc_addr(device, c"vkCmdBeginPerTileExecutionQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_end_per_tile_execution_qcom: if extensions.qcom_tile_shading {
                instance
                    .get_device_proc_addr(device, c"vkCmdEndPerTileExecutionQCOM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_external_compute_queue_nv: if extensions.nv_external_compute_queue {
                instance
                    .get_device_proc_addr(device, c"vkCreateExternalComputeQueueNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_external_compute_queue_nv: if extensions.nv_external_compute_queue {
                instance
                    .get_device_proc_addr(device, c"vkDestroyExternalComputeQueueNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_external_compute_queue_data_nv: if extensions.nv_external_compute_queue {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetExternalComputeQueueDataNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_tensor_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkCreateTensorARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_tensor_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkDestroyTensorARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_tensor_view_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkCreateTensorViewARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_tensor_view_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkDestroyTensorViewARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_tensor_memory_requirements_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkGetTensorMemoryRequirementsARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_tensor_memory_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkBindTensorMemoryARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_device_tensor_memory_requirements_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkGetDeviceTensorMemoryRequirementsARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_copy_tensor_arm: if extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkCmdCopyTensorARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_tensor_opaque_capture_descriptor_data_arm: if extensions.arm_tensors
                && extensions.ext_descriptor_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkGetTensorOpaqueCaptureDescriptorDataARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_tensor_view_opaque_capture_descriptor_data_arm: if extensions.arm_tensors
                && extensions.ext_descriptor_buffer
            {
                instance
                    .get_device_proc_addr(device, c"vkGetTensorViewOpaqueCaptureDescriptorDataARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_external_tensor_properties_arm: if extensions.arm_tensors {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceExternalTensorPropertiesARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_data_graph_pipelines_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkCreateDataGraphPipelinesARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_create_data_graph_pipeline_session_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkCreateDataGraphPipelineSessionARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_data_graph_pipeline_session_bind_point_requirements_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkGetDataGraphPipelineSessionBindPointRequirementsARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_data_graph_pipeline_session_memory_requirements_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkGetDataGraphPipelineSessionMemoryRequirementsARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_bind_data_graph_pipeline_session_memory_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkBindDataGraphPipelineSessionMemoryARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_destroy_data_graph_pipeline_session_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkDestroyDataGraphPipelineSessionARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_dispatch_data_graph_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkCmdDispatchDataGraphARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_data_graph_pipeline_available_properties_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkGetDataGraphPipelineAvailablePropertiesARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_data_graph_pipeline_properties_arm: if extensions.arm_data_graph {
                instance
                    .get_device_proc_addr(device, c"vkGetDataGraphPipelinePropertiesARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_queue_family_data_graph_properties_arm: if extensions.arm_data_graph {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceQueueFamilyDataGraphPropertiesARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_queue_family_data_graph_processing_engine_properties_arm: if extensions
                .arm_data_graph
            {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkGetPhysicalDeviceQueueFamilyDataGraphProcessingEnginePropertiesARM",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_native_buffer_properties_ohos: if extensions.ohos_external_memory {
                instance
                    .get_device_proc_addr(device, c"vkGetNativeBufferPropertiesOHOS")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_memory_native_buffer_ohos: if extensions.ohos_external_memory {
                instance
                    .get_device_proc_addr(device, c"vkGetMemoryNativeBufferOHOS")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_enumerate_physical_device_queue_family_performance_counters_by_region_arm: if extensions
                .arm_performance_counters_by_region
            {
                globals
                    .get_instance_proc_addr(
                        instance.handle,
                        c"vkEnumeratePhysicalDeviceQueueFamilyPerformanceCountersByRegionARM",
                    )
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_set_compute_occupancy_priority_nv: if extensions.nv_compute_occupancy_priority {
                instance
                    .get_device_proc_addr(device, c"vkCmdSetComputeOccupancyPriorityNV")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_write_sampler_descriptors_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkWriteSamplerDescriptorsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_write_resource_descriptors_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkWriteResourceDescriptorsEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_sampler_heap_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindSamplerHeapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_bind_resource_heap_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkCmdBindResourceHeapEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_cmd_push_data_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkCmdPushDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_register_custom_border_color_ext: if extensions.ext_descriptor_heap && extensions.ext_custom_border_color
            {
                instance
                    .get_device_proc_addr(device, c"vkRegisterCustomBorderColorEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_unregister_custom_border_color_ext: if extensions.ext_descriptor_heap
                && extensions.ext_custom_border_color
            {
                instance
                    .get_device_proc_addr(device, c"vkUnregisterCustomBorderColorEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_image_opaque_capture_data_ext: if extensions.ext_descriptor_heap {
                instance
                    .get_device_proc_addr(device, c"vkGetImageOpaqueCaptureDataEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_physical_device_descriptor_size_ext: if extensions.ext_descriptor_heap {
                globals
                    .get_instance_proc_addr(instance.handle, c"vkGetPhysicalDeviceDescriptorSizeEXT")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
            fp_get_tensor_opaque_capture_data_arm: if extensions.ext_descriptor_heap && extensions.arm_tensors {
                instance
                    .get_device_proc_addr(device, c"vkGetTensorOpaqueCaptureDataARM")
                    .map(|f| mem::transmute(f))
            } else {
                None
            },
        })
    }
    pub unsafe fn destroy_device(&self, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_device;
        (fp)(self.handle, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_device_queue(&self, queue_family_index: u32, queue_index: u32) -> vk::Queue {
        let fp = self.fp_get_device_queue;
        let mut p_queue = MaybeUninit::<_>::uninit();
        (fp)(self.handle, queue_family_index, queue_index, p_queue.as_mut_ptr());
        p_queue.assume_init()
    }
    pub unsafe fn queue_submit(&self, queue: vk::Queue, p_submits: &[vk::SubmitInfo], fence: vk::Fence) -> Result<()> {
        let fp = self.fp_queue_submit;
        let submit_count = p_submits.len() as u32;
        let err = (fp)(queue, submit_count, p_submits.as_ptr(), fence);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn queue_wait_idle(&self, queue: vk::Queue) -> Result<()> {
        let fp = self.fp_queue_wait_idle;
        let err = (fp)(queue);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn device_wait_idle(&self) -> Result<()> {
        let fp = self.fp_device_wait_idle;
        let err = (fp)(self.handle);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn allocate_memory(
        &self,
        p_allocate_info: &vk::MemoryAllocateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DeviceMemory> {
        let fp = self.fp_allocate_memory;
        let mut p_memory = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_allocate_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_memory.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_memory.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn free_memory(&self, memory: vk::DeviceMemory, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_free_memory;
        (fp)(self.handle, memory, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn map_memory(
        &self,
        memory: vk::DeviceMemory,
        offset: vk::DeviceSize,
        size: vk::DeviceSize,
        flags: vk::MemoryMapFlags,
    ) -> Result<*mut c_void> {
        let fp = self.fp_map_memory;
        let mut pp_data = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, memory, offset, size, flags, pp_data.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(pp_data.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn unmap_memory(&self, memory: vk::DeviceMemory) {
        let fp = self.fp_unmap_memory;
        (fp)(self.handle, memory)
    }
    pub unsafe fn flush_mapped_memory_ranges(&self, p_memory_ranges: &[vk::MappedMemoryRange]) -> Result<()> {
        let fp = self.fp_flush_mapped_memory_ranges;
        let memory_range_count = p_memory_ranges.len() as u32;
        let err = (fp)(self.handle, memory_range_count, p_memory_ranges.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn invalidate_mapped_memory_ranges(&self, p_memory_ranges: &[vk::MappedMemoryRange]) -> Result<()> {
        let fp = self.fp_invalidate_mapped_memory_ranges;
        let memory_range_count = p_memory_ranges.len() as u32;
        let err = (fp)(self.handle, memory_range_count, p_memory_ranges.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_memory_commitment(&self, memory: vk::DeviceMemory) -> vk::DeviceSize {
        let fp = self.fp_get_device_memory_commitment;
        let mut p_committed_memory_in_bytes = MaybeUninit::<_>::uninit();
        (fp)(self.handle, memory, p_committed_memory_in_bytes.as_mut_ptr());
        p_committed_memory_in_bytes.assume_init()
    }
    pub unsafe fn get_buffer_memory_requirements(&self, buffer: vk::Buffer) -> vk::MemoryRequirements {
        let fp = self.fp_get_buffer_memory_requirements;
        let mut p_memory_requirements = MaybeUninit::<_>::uninit();
        (fp)(self.handle, buffer, p_memory_requirements.as_mut_ptr());
        p_memory_requirements.assume_init()
    }
    pub unsafe fn bind_buffer_memory(
        &self,
        buffer: vk::Buffer,
        memory: vk::DeviceMemory,
        memory_offset: vk::DeviceSize,
    ) -> Result<()> {
        let fp = self.fp_bind_buffer_memory;
        let err = (fp)(self.handle, buffer, memory, memory_offset);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_image_memory_requirements(&self, image: vk::Image) -> vk::MemoryRequirements {
        let fp = self.fp_get_image_memory_requirements;
        let mut p_memory_requirements = MaybeUninit::<_>::uninit();
        (fp)(self.handle, image, p_memory_requirements.as_mut_ptr());
        p_memory_requirements.assume_init()
    }
    pub unsafe fn bind_image_memory(
        &self,
        image: vk::Image,
        memory: vk::DeviceMemory,
        memory_offset: vk::DeviceSize,
    ) -> Result<()> {
        let fp = self.fp_bind_image_memory;
        let err = (fp)(self.handle, image, memory, memory_offset);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_image_sparse_memory_requirements(
        &self,
        image: vk::Image,
        p_sparse_memory_requirement_count: &mut u32,
        p_sparse_memory_requirements: *mut vk::SparseImageMemoryRequirements,
    ) {
        let fp = self.fp_get_image_sparse_memory_requirements;
        (fp)(
            self.handle,
            image,
            p_sparse_memory_requirement_count,
            p_sparse_memory_requirements,
        );
    }
    pub unsafe fn get_image_sparse_memory_requirements_to_vec(
        &self,
        image: vk::Image,
    ) -> Vec<vk::SparseImageMemoryRequirements> {
        enumerate_generic_unchecked_to_vec(|len, ptr| self.get_image_sparse_memory_requirements(image, len, ptr))
    }
    pub unsafe fn queue_bind_sparse(
        &self,
        queue: vk::Queue,
        p_bind_info: &[vk::BindSparseInfo],
        fence: vk::Fence,
    ) -> Result<()> {
        let fp = self.fp_queue_bind_sparse;
        let bind_info_count = p_bind_info.len() as u32;
        let err = (fp)(queue, bind_info_count, p_bind_info.as_ptr(), fence);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_fence(
        &self,
        p_create_info: &vk::FenceCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Fence> {
        let fp = self.fp_create_fence;
        let mut p_fence = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_fence.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_fence.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_fence(&self, fence: vk::Fence, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_fence;
        (fp)(self.handle, fence, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn reset_fences(&self, p_fences: &[vk::Fence]) -> Result<()> {
        let fp = self.fp_reset_fences;
        let fence_count = p_fences.len() as u32;
        let err = (fp)(self.handle, fence_count, p_fences.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_fence_status(&self, fence: vk::Fence) -> Result<vk::Result> {
        let fp = self.fp_get_fence_status;
        let err = (fp)(self.handle, fence);
        match err {
            vk::Result::SUCCESS | vk::Result::NOT_READY => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn wait_for_fences(&self, p_fences: &[vk::Fence], wait_all: bool, timeout: u64) -> Result<vk::Result> {
        let fp = self.fp_wait_for_fences;
        let fence_count = p_fences.len() as u32;
        let err = (fp)(
            self.handle,
            fence_count,
            p_fences.as_ptr(),
            if wait_all { vk::TRUE } else { vk::FALSE },
            timeout,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_semaphore(
        &self,
        p_create_info: &vk::SemaphoreCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Semaphore> {
        let fp = self.fp_create_semaphore;
        let mut p_semaphore = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_semaphore.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_semaphore.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_semaphore(&self, semaphore: vk::Semaphore, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_semaphore;
        (fp)(self.handle, semaphore, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_event(
        &self,
        p_create_info: &vk::EventCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Event> {
        let fp = self.fp_create_event;
        let mut p_event = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_event.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_event.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_event(&self, event: vk::Event, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_event;
        (fp)(self.handle, event, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_event_status(&self, event: vk::Event) -> Result<vk::Result> {
        let fp = self.fp_get_event_status;
        let err = (fp)(self.handle, event);
        match err {
            vk::Result::EVENT_SET | vk::Result::EVENT_RESET => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn set_event(&self, event: vk::Event) -> Result<()> {
        let fp = self.fp_set_event;
        let err = (fp)(self.handle, event);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn reset_event(&self, event: vk::Event) -> Result<()> {
        let fp = self.fp_reset_event;
        let err = (fp)(self.handle, event);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_query_pool(
        &self,
        p_create_info: &vk::QueryPoolCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::QueryPool> {
        let fp = self.fp_create_query_pool;
        let mut p_query_pool = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_query_pool.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_query_pool.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_query_pool(&self, query_pool: vk::QueryPool, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_query_pool;
        (fp)(self.handle, query_pool, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_query_pool_results(
        &self,
        query_pool: vk::QueryPool,
        first_query: u32,
        query_count: u32,
        p_data: &mut [u8],
        stride: vk::DeviceSize,
        flags: vk::QueryResultFlags,
    ) -> Result<vk::Result> {
        let fp = self.fp_get_query_pool_results;
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            query_pool,
            first_query,
            query_count,
            data_size,
            p_data.as_mut_ptr() as *mut _,
            stride,
            flags,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::NOT_READY => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn reset_query_pool(&self, query_pool: vk::QueryPool, first_query: u32, query_count: u32) {
        let fp = self.fp_reset_query_pool.expect("vkResetQueryPool is not loaded");
        (fp)(self.handle, query_pool, first_query, query_count)
    }
    pub unsafe fn reset_query_pool_ext(&self, query_pool: vk::QueryPool, first_query: u32, query_count: u32) {
        let fp = self.fp_reset_query_pool.expect("vkResetQueryPoolEXT is not loaded");
        (fp)(self.handle, query_pool, first_query, query_count)
    }
    pub unsafe fn create_buffer(
        &self,
        p_create_info: &vk::BufferCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Buffer> {
        let fp = self.fp_create_buffer;
        let mut p_buffer = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_buffer.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_buffer.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_buffer(&self, buffer: vk::Buffer, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_buffer;
        (fp)(self.handle, buffer, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_buffer_view(
        &self,
        p_create_info: &vk::BufferViewCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::BufferView> {
        let fp = self.fp_create_buffer_view;
        let mut p_view = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_view.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_view.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_buffer_view(
        &self,
        buffer_view: vk::BufferView,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_buffer_view;
        (fp)(self.handle, buffer_view, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_image(
        &self,
        p_create_info: &vk::ImageCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Image> {
        let fp = self.fp_create_image;
        let mut p_image = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_image.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_image.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_image(&self, image: vk::Image, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_image;
        (fp)(self.handle, image, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_image_subresource_layout(
        &self,
        image: vk::Image,
        p_subresource: &vk::ImageSubresource,
    ) -> vk::SubresourceLayout {
        let fp = self.fp_get_image_subresource_layout;
        let mut p_layout = MaybeUninit::<_>::uninit();
        (fp)(self.handle, image, p_subresource, p_layout.as_mut_ptr());
        p_layout.assume_init()
    }
    pub unsafe fn create_image_view(
        &self,
        p_create_info: &vk::ImageViewCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::ImageView> {
        let fp = self.fp_create_image_view;
        let mut p_view = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_view.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_view.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_image_view(&self, image_view: vk::ImageView, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_image_view;
        (fp)(self.handle, image_view, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_shader_module(
        &self,
        p_create_info: &vk::ShaderModuleCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::ShaderModule> {
        let fp = self.fp_create_shader_module;
        let mut p_shader_module = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_shader_module.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_shader_module.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_shader_module(
        &self,
        shader_module: vk::ShaderModule,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_shader_module;
        (fp)(self.handle, shader_module, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_pipeline_cache(
        &self,
        p_create_info: &vk::PipelineCacheCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::PipelineCache> {
        let fp = self.fp_create_pipeline_cache;
        let mut p_pipeline_cache = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipeline_cache.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_pipeline_cache.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_pipeline_cache(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_pipeline_cache;
        (fp)(self.handle, pipeline_cache, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_pipeline_cache_data(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_data_size: &mut usize,
        p_data: *mut c_void,
    ) -> Result<vk::Result> {
        let fp = self.fp_get_pipeline_cache_data;
        let err = (fp)(self.handle, pipeline_cache, p_data_size, p_data);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn merge_pipeline_caches(
        &self,
        dst_cache: vk::PipelineCache,
        p_src_caches: &[vk::PipelineCache],
    ) -> Result<()> {
        let fp = self.fp_merge_pipeline_caches;
        let src_cache_count = p_src_caches.len() as u32;
        let err = (fp)(self.handle, dst_cache, src_cache_count, p_src_caches.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_pipeline_binaries_khr(
        &self,
        p_create_info: &vk::PipelineBinaryCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_binaries: &mut vk::PipelineBinaryHandlesInfoKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_create_pipeline_binaries_khr
            .expect("vkCreatePipelineBinariesKHR is not loaded");
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_binaries,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE | vk::Result::PIPELINE_BINARY_MISSING_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_pipeline_binary_khr(
        &self,
        pipeline_binary: vk::PipelineBinaryKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_pipeline_binary_khr
            .expect("vkDestroyPipelineBinaryKHR is not loaded");
        (fp)(self.handle, pipeline_binary, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_pipeline_key_khr(
        &self,
        p_pipeline_create_info: Option<&vk::PipelineCreateInfoKHR>,
        p_pipeline_key: &mut vk::PipelineBinaryKeyKHR,
    ) -> Result<()> {
        let fp = self.fp_get_pipeline_key_khr.expect("vkGetPipelineKeyKHR is not loaded");
        let err = (fp)(
            self.handle,
            p_pipeline_create_info.map_or(ptr::null(), |r| r),
            p_pipeline_key,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_pipeline_binary_data_khr(
        &self,
        p_info: &vk::PipelineBinaryDataInfoKHR,
        p_pipeline_binary_key: &mut vk::PipelineBinaryKeyKHR,
        p_pipeline_binary_data_size: &mut usize,
        p_pipeline_binary_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_pipeline_binary_data_khr
            .expect("vkGetPipelineBinaryDataKHR is not loaded");
        let err = (fp)(
            self.handle,
            p_info,
            p_pipeline_binary_key,
            p_pipeline_binary_data_size,
            p_pipeline_binary_data,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn release_captured_pipeline_data_khr(
        &self,
        p_info: &vk::ReleaseCapturedPipelineDataInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<()> {
        let fp = self
            .fp_release_captured_pipeline_data_khr
            .expect("vkReleaseCapturedPipelineDataKHR is not loaded");
        let err = (fp)(self.handle, p_info, p_allocator.map_or(ptr::null(), |r| r));
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_graphics_pipelines(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::GraphicsPipelineCreateInfo],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self.fp_create_graphics_pipelines;
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_graphics_pipelines_single(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::GraphicsPipelineCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_graphics_pipelines(
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn create_compute_pipelines(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::ComputePipelineCreateInfo],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self.fp_create_compute_pipelines;
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_compute_pipelines_single(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::ComputePipelineCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_compute_pipelines(
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn get_device_subpass_shading_max_workgroup_size_huawei(
        &self,
        renderpass: vk::RenderPass,
    ) -> Result<vk::Extent2D> {
        let fp = self
            .fp_get_device_subpass_shading_max_workgroup_size_huawei
            .expect("vkGetDeviceSubpassShadingMaxWorkgroupSizeHUAWEI is not loaded");
        let mut p_max_workgroup_size = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, renderpass, p_max_workgroup_size.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_max_workgroup_size.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_pipeline(&self, pipeline: vk::Pipeline, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_pipeline;
        (fp)(self.handle, pipeline, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_pipeline_layout(
        &self,
        p_create_info: &vk::PipelineLayoutCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::PipelineLayout> {
        let fp = self.fp_create_pipeline_layout;
        let mut p_pipeline_layout = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipeline_layout.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_pipeline_layout.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_pipeline_layout(
        &self,
        pipeline_layout: vk::PipelineLayout,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_pipeline_layout;
        (fp)(self.handle, pipeline_layout, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_sampler(
        &self,
        p_create_info: &vk::SamplerCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Sampler> {
        let fp = self.fp_create_sampler;
        let mut p_sampler = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_sampler.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_sampler.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_sampler(&self, sampler: vk::Sampler, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_sampler;
        (fp)(self.handle, sampler, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_descriptor_set_layout(
        &self,
        p_create_info: &vk::DescriptorSetLayoutCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DescriptorSetLayout> {
        let fp = self.fp_create_descriptor_set_layout;
        let mut p_set_layout = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_set_layout.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_set_layout.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_descriptor_set_layout(
        &self,
        descriptor_set_layout: vk::DescriptorSetLayout,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_descriptor_set_layout;
        (fp)(
            self.handle,
            descriptor_set_layout,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn create_descriptor_pool(
        &self,
        p_create_info: &vk::DescriptorPoolCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DescriptorPool> {
        let fp = self.fp_create_descriptor_pool;
        let mut p_descriptor_pool = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_descriptor_pool.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_descriptor_pool.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_descriptor_pool(
        &self,
        descriptor_pool: vk::DescriptorPool,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_descriptor_pool;
        (fp)(self.handle, descriptor_pool, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn reset_descriptor_pool(
        &self,
        descriptor_pool: vk::DescriptorPool,
        flags: vk::DescriptorPoolResetFlags,
    ) -> Result<()> {
        let fp = self.fp_reset_descriptor_pool;
        let err = (fp)(self.handle, descriptor_pool, flags);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn allocate_descriptor_sets(
        &self,
        p_allocate_info: &vk::DescriptorSetAllocateInfo,
        p_descriptor_sets: &mut [vk::DescriptorSet],
    ) -> Result<()> {
        let fp = self.fp_allocate_descriptor_sets;
        assert_eq!(p_allocate_info.descriptor_set_count as usize, p_descriptor_sets.len());
        let err = (fp)(self.handle, p_allocate_info, p_descriptor_sets.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn allocate_descriptor_sets_single(
        &self,
        p_allocate_info: &vk::DescriptorSetAllocateInfo,
    ) -> Result<vk::DescriptorSet> {
        let mut p_descriptor_sets = Default::default();
        self.allocate_descriptor_sets(p_allocate_info, slice::from_mut(&mut p_descriptor_sets))
            .map(|_| p_descriptor_sets)
    }
    pub unsafe fn free_descriptor_sets(
        &self,
        descriptor_pool: vk::DescriptorPool,
        p_descriptor_sets: &[vk::DescriptorSet],
    ) -> Result<()> {
        let fp = self.fp_free_descriptor_sets;
        let descriptor_set_count = p_descriptor_sets.len() as u32;
        let err = (fp)(
            self.handle,
            descriptor_pool,
            descriptor_set_count,
            p_descriptor_sets.as_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn update_descriptor_sets(
        &self,
        p_descriptor_writes: &[vk::WriteDescriptorSet],
        p_descriptor_copies: &[vk::CopyDescriptorSet],
    ) {
        let fp = self.fp_update_descriptor_sets;
        let descriptor_write_count = p_descriptor_writes.len() as u32;
        let descriptor_copy_count = p_descriptor_copies.len() as u32;
        (fp)(
            self.handle,
            descriptor_write_count,
            p_descriptor_writes.as_ptr(),
            descriptor_copy_count,
            p_descriptor_copies.as_ptr(),
        )
    }
    pub unsafe fn create_framebuffer(
        &self,
        p_create_info: &vk::FramebufferCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Framebuffer> {
        let fp = self.fp_create_framebuffer;
        let mut p_framebuffer = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_framebuffer.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_framebuffer.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_framebuffer(
        &self,
        framebuffer: vk::Framebuffer,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_framebuffer;
        (fp)(self.handle, framebuffer, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_render_pass(
        &self,
        p_create_info: &vk::RenderPassCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::RenderPass> {
        let fp = self.fp_create_render_pass;
        let mut p_render_pass = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_render_pass.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_render_pass.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_render_pass(
        &self,
        render_pass: vk::RenderPass,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_render_pass;
        (fp)(self.handle, render_pass, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_render_area_granularity(&self, render_pass: vk::RenderPass) -> vk::Extent2D {
        let fp = self.fp_get_render_area_granularity;
        let mut p_granularity = MaybeUninit::<_>::uninit();
        (fp)(self.handle, render_pass, p_granularity.as_mut_ptr());
        p_granularity.assume_init()
    }
    pub unsafe fn get_rendering_area_granularity(&self, p_rendering_area_info: &vk::RenderingAreaInfo) -> vk::Extent2D {
        let fp = self
            .fp_get_rendering_area_granularity
            .expect("vkGetRenderingAreaGranularity is not loaded");
        let mut p_granularity = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_rendering_area_info, p_granularity.as_mut_ptr());
        p_granularity.assume_init()
    }
    pub unsafe fn get_rendering_area_granularity_khr(
        &self,
        p_rendering_area_info: &vk::RenderingAreaInfo,
    ) -> vk::Extent2D {
        let fp = self
            .fp_get_rendering_area_granularity
            .expect("vkGetRenderingAreaGranularityKHR is not loaded");
        let mut p_granularity = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_rendering_area_info, p_granularity.as_mut_ptr());
        p_granularity.assume_init()
    }
    pub unsafe fn create_command_pool(
        &self,
        p_create_info: &vk::CommandPoolCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::CommandPool> {
        let fp = self.fp_create_command_pool;
        let mut p_command_pool = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_command_pool.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_command_pool.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_command_pool(
        &self,
        command_pool: vk::CommandPool,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self.fp_destroy_command_pool;
        (fp)(self.handle, command_pool, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn reset_command_pool(
        &self,
        command_pool: vk::CommandPool,
        flags: vk::CommandPoolResetFlags,
    ) -> Result<()> {
        let fp = self.fp_reset_command_pool;
        let err = (fp)(self.handle, command_pool, flags);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn allocate_command_buffers(
        &self,
        p_allocate_info: &vk::CommandBufferAllocateInfo,
        p_command_buffers: &mut [vk::CommandBuffer],
    ) -> Result<()> {
        let fp = self.fp_allocate_command_buffers;
        assert_eq!(p_allocate_info.command_buffer_count as usize, p_command_buffers.len());
        let err = (fp)(self.handle, p_allocate_info, p_command_buffers.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn allocate_command_buffers_single(
        &self,
        p_allocate_info: &vk::CommandBufferAllocateInfo,
    ) -> Result<vk::CommandBuffer> {
        let mut p_command_buffers = Default::default();
        self.allocate_command_buffers(p_allocate_info, slice::from_mut(&mut p_command_buffers))
            .map(|_| p_command_buffers)
    }
    pub unsafe fn free_command_buffers(&self, command_pool: vk::CommandPool, p_command_buffers: &[vk::CommandBuffer]) {
        let fp = self.fp_free_command_buffers;
        let command_buffer_count = p_command_buffers.len() as u32;
        (fp)(
            self.handle,
            command_pool,
            command_buffer_count,
            p_command_buffers.as_ptr(),
        )
    }
    pub unsafe fn begin_command_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        p_begin_info: &vk::CommandBufferBeginInfo,
    ) -> Result<()> {
        let fp = self.fp_begin_command_buffer;
        let err = (fp)(command_buffer, p_begin_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn end_command_buffer(&self, command_buffer: vk::CommandBuffer) -> Result<()> {
        let fp = self.fp_end_command_buffer;
        let err = (fp)(command_buffer);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn reset_command_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        flags: vk::CommandBufferResetFlags,
    ) -> Result<()> {
        let fp = self.fp_reset_command_buffer;
        let err = (fp)(command_buffer, flags);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_bind_pipeline(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        pipeline: vk::Pipeline,
    ) {
        let fp = self.fp_cmd_bind_pipeline;
        (fp)(command_buffer, pipeline_bind_point, pipeline)
    }
    pub unsafe fn cmd_set_attachment_feedback_loop_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        aspect_mask: vk::ImageAspectFlags,
    ) {
        let fp = self
            .fp_cmd_set_attachment_feedback_loop_enable_ext
            .expect("vkCmdSetAttachmentFeedbackLoopEnableEXT is not loaded");
        (fp)(command_buffer, aspect_mask)
    }
    pub unsafe fn cmd_set_viewport(
        &self,
        command_buffer: vk::CommandBuffer,
        first_viewport: u32,
        p_viewports: &[vk::Viewport],
    ) {
        let fp = self.fp_cmd_set_viewport;
        let viewport_count = p_viewports.len() as u32;
        (fp)(command_buffer, first_viewport, viewport_count, p_viewports.as_ptr())
    }
    pub unsafe fn cmd_set_scissor(
        &self,
        command_buffer: vk::CommandBuffer,
        first_scissor: u32,
        p_scissors: &[vk::Rect2D],
    ) {
        let fp = self.fp_cmd_set_scissor;
        let scissor_count = p_scissors.len() as u32;
        (fp)(command_buffer, first_scissor, scissor_count, p_scissors.as_ptr())
    }
    pub unsafe fn cmd_set_line_width(&self, command_buffer: vk::CommandBuffer, line_width: f32) {
        let fp = self.fp_cmd_set_line_width;
        (fp)(command_buffer, line_width)
    }
    pub unsafe fn cmd_set_depth_bias(
        &self,
        command_buffer: vk::CommandBuffer,
        depth_bias_constant_factor: f32,
        depth_bias_clamp: f32,
        depth_bias_slope_factor: f32,
    ) {
        let fp = self.fp_cmd_set_depth_bias;
        (fp)(
            command_buffer,
            depth_bias_constant_factor,
            depth_bias_clamp,
            depth_bias_slope_factor,
        )
    }
    pub unsafe fn cmd_set_blend_constants(&self, command_buffer: vk::CommandBuffer, blend_constants: *const f32) {
        let fp = self.fp_cmd_set_blend_constants;
        (fp)(command_buffer, blend_constants)
    }
    pub unsafe fn cmd_set_depth_bounds(
        &self,
        command_buffer: vk::CommandBuffer,
        min_depth_bounds: f32,
        max_depth_bounds: f32,
    ) {
        let fp = self.fp_cmd_set_depth_bounds;
        (fp)(command_buffer, min_depth_bounds, max_depth_bounds)
    }
    pub unsafe fn cmd_set_stencil_compare_mask(
        &self,
        command_buffer: vk::CommandBuffer,
        face_mask: vk::StencilFaceFlags,
        compare_mask: u32,
    ) {
        let fp = self.fp_cmd_set_stencil_compare_mask;
        (fp)(command_buffer, face_mask, compare_mask)
    }
    pub unsafe fn cmd_set_stencil_write_mask(
        &self,
        command_buffer: vk::CommandBuffer,
        face_mask: vk::StencilFaceFlags,
        write_mask: u32,
    ) {
        let fp = self.fp_cmd_set_stencil_write_mask;
        (fp)(command_buffer, face_mask, write_mask)
    }
    pub unsafe fn cmd_set_stencil_reference(
        &self,
        command_buffer: vk::CommandBuffer,
        face_mask: vk::StencilFaceFlags,
        reference: u32,
    ) {
        let fp = self.fp_cmd_set_stencil_reference;
        (fp)(command_buffer, face_mask, reference)
    }
    pub unsafe fn cmd_bind_descriptor_sets(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        layout: vk::PipelineLayout,
        first_set: u32,
        p_descriptor_sets: &[vk::DescriptorSet],
        p_dynamic_offsets: &[u32],
    ) {
        let fp = self.fp_cmd_bind_descriptor_sets;
        let descriptor_set_count = p_descriptor_sets.len() as u32;
        let dynamic_offset_count = p_dynamic_offsets.len() as u32;
        (fp)(
            command_buffer,
            pipeline_bind_point,
            layout,
            first_set,
            descriptor_set_count,
            p_descriptor_sets.as_ptr(),
            dynamic_offset_count,
            p_dynamic_offsets.as_ptr(),
        )
    }
    pub unsafe fn cmd_bind_index_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        index_type: vk::IndexType,
    ) {
        let fp = self.fp_cmd_bind_index_buffer;
        (fp)(command_buffer, buffer, offset, index_type)
    }
    pub unsafe fn cmd_bind_vertex_buffers(
        &self,
        command_buffer: vk::CommandBuffer,
        first_binding: u32,
        p_buffers: &[vk::Buffer],
        p_offsets: &[vk::DeviceSize],
    ) {
        let fp = self.fp_cmd_bind_vertex_buffers;
        let binding_count = p_buffers.len() as u32;
        assert_eq!(binding_count, p_offsets.len() as u32);
        (fp)(
            command_buffer,
            first_binding,
            binding_count,
            p_buffers.as_ptr(),
            p_offsets.as_ptr(),
        )
    }
    pub unsafe fn cmd_draw(
        &self,
        command_buffer: vk::CommandBuffer,
        vertex_count: u32,
        instance_count: u32,
        first_vertex: u32,
        first_instance: u32,
    ) {
        let fp = self.fp_cmd_draw;
        (fp)(
            command_buffer,
            vertex_count,
            instance_count,
            first_vertex,
            first_instance,
        )
    }
    pub unsafe fn cmd_draw_indexed(
        &self,
        command_buffer: vk::CommandBuffer,
        index_count: u32,
        instance_count: u32,
        first_index: u32,
        vertex_offset: i32,
        first_instance: u32,
    ) {
        let fp = self.fp_cmd_draw_indexed;
        (fp)(
            command_buffer,
            index_count,
            instance_count,
            first_index,
            vertex_offset,
            first_instance,
        )
    }
    pub unsafe fn cmd_draw_multi_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_vertex_info: &[vk::MultiDrawInfoEXT],
        instance_count: u32,
        first_instance: u32,
        stride: u32,
    ) {
        let fp = self.fp_cmd_draw_multi_ext.expect("vkCmdDrawMultiEXT is not loaded");
        let draw_count = p_vertex_info.len() as u32;
        (fp)(
            command_buffer,
            draw_count,
            p_vertex_info.as_ptr(),
            instance_count,
            first_instance,
            stride,
        )
    }
    pub unsafe fn cmd_draw_multi_indexed_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_index_info: &[vk::MultiDrawIndexedInfoEXT],
        instance_count: u32,
        first_instance: u32,
        stride: u32,
        p_vertex_offset: Option<&i32>,
    ) {
        let fp = self
            .fp_cmd_draw_multi_indexed_ext
            .expect("vkCmdDrawMultiIndexedEXT is not loaded");
        let draw_count = p_index_info.len() as u32;
        (fp)(
            command_buffer,
            draw_count,
            p_index_info.as_ptr(),
            instance_count,
            first_instance,
            stride,
            p_vertex_offset.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn cmd_draw_indirect(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        draw_count: u32,
        stride: u32,
    ) {
        let fp = self.fp_cmd_draw_indirect;
        (fp)(command_buffer, buffer, offset, draw_count, stride)
    }
    pub unsafe fn cmd_draw_indexed_indirect(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        draw_count: u32,
        stride: u32,
    ) {
        let fp = self.fp_cmd_draw_indexed_indirect;
        (fp)(command_buffer, buffer, offset, draw_count, stride)
    }
    pub unsafe fn cmd_dispatch(
        &self,
        command_buffer: vk::CommandBuffer,
        group_count_x: u32,
        group_count_y: u32,
        group_count_z: u32,
    ) {
        let fp = self.fp_cmd_dispatch;
        (fp)(command_buffer, group_count_x, group_count_y, group_count_z)
    }
    pub unsafe fn cmd_dispatch_indirect(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
    ) {
        let fp = self.fp_cmd_dispatch_indirect;
        (fp)(command_buffer, buffer, offset)
    }
    pub unsafe fn cmd_subpass_shading_huawei(&self, command_buffer: vk::CommandBuffer) {
        let fp = self
            .fp_cmd_subpass_shading_huawei
            .expect("vkCmdSubpassShadingHUAWEI is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_draw_cluster_huawei(
        &self,
        command_buffer: vk::CommandBuffer,
        group_count_x: u32,
        group_count_y: u32,
        group_count_z: u32,
    ) {
        let fp = self
            .fp_cmd_draw_cluster_huawei
            .expect("vkCmdDrawClusterHUAWEI is not loaded");
        (fp)(command_buffer, group_count_x, group_count_y, group_count_z)
    }
    pub unsafe fn cmd_draw_cluster_indirect_huawei(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
    ) {
        let fp = self
            .fp_cmd_draw_cluster_indirect_huawei
            .expect("vkCmdDrawClusterIndirectHUAWEI is not loaded");
        (fp)(command_buffer, buffer, offset)
    }
    pub unsafe fn cmd_update_pipeline_indirect_buffer_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        pipeline: vk::Pipeline,
    ) {
        let fp = self
            .fp_cmd_update_pipeline_indirect_buffer_nv
            .expect("vkCmdUpdatePipelineIndirectBufferNV is not loaded");
        (fp)(command_buffer, pipeline_bind_point, pipeline)
    }
    pub unsafe fn cmd_copy_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        src_buffer: vk::Buffer,
        dst_buffer: vk::Buffer,
        p_regions: &[vk::BufferCopy],
    ) {
        let fp = self.fp_cmd_copy_buffer;
        let region_count = p_regions.len() as u32;
        (fp)(command_buffer, src_buffer, dst_buffer, region_count, p_regions.as_ptr())
    }
    pub unsafe fn cmd_copy_image(
        &self,
        command_buffer: vk::CommandBuffer,
        src_image: vk::Image,
        src_image_layout: vk::ImageLayout,
        dst_image: vk::Image,
        dst_image_layout: vk::ImageLayout,
        p_regions: &[vk::ImageCopy],
    ) {
        let fp = self.fp_cmd_copy_image;
        let region_count = p_regions.len() as u32;
        (fp)(
            command_buffer,
            src_image,
            src_image_layout,
            dst_image,
            dst_image_layout,
            region_count,
            p_regions.as_ptr(),
        )
    }
    pub unsafe fn cmd_blit_image(
        &self,
        command_buffer: vk::CommandBuffer,
        src_image: vk::Image,
        src_image_layout: vk::ImageLayout,
        dst_image: vk::Image,
        dst_image_layout: vk::ImageLayout,
        p_regions: &[vk::ImageBlit],
        filter: vk::Filter,
    ) {
        let fp = self.fp_cmd_blit_image;
        let region_count = p_regions.len() as u32;
        (fp)(
            command_buffer,
            src_image,
            src_image_layout,
            dst_image,
            dst_image_layout,
            region_count,
            p_regions.as_ptr(),
            filter,
        )
    }
    pub unsafe fn cmd_copy_buffer_to_image(
        &self,
        command_buffer: vk::CommandBuffer,
        src_buffer: vk::Buffer,
        dst_image: vk::Image,
        dst_image_layout: vk::ImageLayout,
        p_regions: &[vk::BufferImageCopy],
    ) {
        let fp = self.fp_cmd_copy_buffer_to_image;
        let region_count = p_regions.len() as u32;
        (fp)(
            command_buffer,
            src_buffer,
            dst_image,
            dst_image_layout,
            region_count,
            p_regions.as_ptr(),
        )
    }
    pub unsafe fn cmd_copy_image_to_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        src_image: vk::Image,
        src_image_layout: vk::ImageLayout,
        dst_buffer: vk::Buffer,
        p_regions: &[vk::BufferImageCopy],
    ) {
        let fp = self.fp_cmd_copy_image_to_buffer;
        let region_count = p_regions.len() as u32;
        (fp)(
            command_buffer,
            src_image,
            src_image_layout,
            dst_buffer,
            region_count,
            p_regions.as_ptr(),
        )
    }
    pub unsafe fn cmd_copy_memory_indirect_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        copy_buffer_address: vk::DeviceAddress,
        copy_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_copy_memory_indirect_nv
            .expect("vkCmdCopyMemoryIndirectNV is not loaded");
        (fp)(command_buffer, copy_buffer_address, copy_count, stride)
    }
    pub unsafe fn cmd_copy_memory_indirect_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_memory_indirect_info: &vk::CopyMemoryIndirectInfoKHR,
    ) {
        let fp = self
            .fp_cmd_copy_memory_indirect_khr
            .expect("vkCmdCopyMemoryIndirectKHR is not loaded");
        (fp)(command_buffer, p_copy_memory_indirect_info)
    }
    pub unsafe fn cmd_copy_memory_to_image_indirect_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        copy_buffer_address: vk::DeviceAddress,
        stride: u32,
        dst_image: vk::Image,
        dst_image_layout: vk::ImageLayout,
        p_image_subresources: &[vk::ImageSubresourceLayers],
    ) {
        let fp = self
            .fp_cmd_copy_memory_to_image_indirect_nv
            .expect("vkCmdCopyMemoryToImageIndirectNV is not loaded");
        let copy_count = p_image_subresources.len() as u32;
        (fp)(
            command_buffer,
            copy_buffer_address,
            copy_count,
            stride,
            dst_image,
            dst_image_layout,
            p_image_subresources.as_ptr(),
        )
    }
    pub unsafe fn cmd_copy_memory_to_image_indirect_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_memory_to_image_indirect_info: &vk::CopyMemoryToImageIndirectInfoKHR,
    ) {
        let fp = self
            .fp_cmd_copy_memory_to_image_indirect_khr
            .expect("vkCmdCopyMemoryToImageIndirectKHR is not loaded");
        (fp)(command_buffer, p_copy_memory_to_image_indirect_info)
    }
    pub unsafe fn cmd_update_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        dst_buffer: vk::Buffer,
        dst_offset: vk::DeviceSize,
        p_data: &[u8],
    ) {
        let fp = self.fp_cmd_update_buffer;
        let data_size = p_data.len() as vk::DeviceSize;
        (fp)(
            command_buffer,
            dst_buffer,
            dst_offset,
            data_size,
            p_data.as_ptr() as *const _,
        )
    }
    pub unsafe fn cmd_fill_buffer(
        &self,
        command_buffer: vk::CommandBuffer,
        dst_buffer: vk::Buffer,
        dst_offset: vk::DeviceSize,
        size: vk::DeviceSize,
        data: u32,
    ) {
        let fp = self.fp_cmd_fill_buffer;
        (fp)(command_buffer, dst_buffer, dst_offset, size, data)
    }
    pub unsafe fn cmd_clear_color_image(
        &self,
        command_buffer: vk::CommandBuffer,
        image: vk::Image,
        image_layout: vk::ImageLayout,
        p_color: &vk::ClearColorValue,
        p_ranges: &[vk::ImageSubresourceRange],
    ) {
        let fp = self.fp_cmd_clear_color_image;
        let range_count = p_ranges.len() as u32;
        (fp)(
            command_buffer,
            image,
            image_layout,
            p_color,
            range_count,
            p_ranges.as_ptr(),
        )
    }
    pub unsafe fn cmd_clear_depth_stencil_image(
        &self,
        command_buffer: vk::CommandBuffer,
        image: vk::Image,
        image_layout: vk::ImageLayout,
        p_depth_stencil: &vk::ClearDepthStencilValue,
        p_ranges: &[vk::ImageSubresourceRange],
    ) {
        let fp = self.fp_cmd_clear_depth_stencil_image;
        let range_count = p_ranges.len() as u32;
        (fp)(
            command_buffer,
            image,
            image_layout,
            p_depth_stencil,
            range_count,
            p_ranges.as_ptr(),
        )
    }
    pub unsafe fn cmd_clear_attachments(
        &self,
        command_buffer: vk::CommandBuffer,
        p_attachments: &[vk::ClearAttachment],
        p_rects: &[vk::ClearRect],
    ) {
        let fp = self.fp_cmd_clear_attachments;
        let attachment_count = p_attachments.len() as u32;
        let rect_count = p_rects.len() as u32;
        (fp)(
            command_buffer,
            attachment_count,
            p_attachments.as_ptr(),
            rect_count,
            p_rects.as_ptr(),
        )
    }
    pub unsafe fn cmd_resolve_image(
        &self,
        command_buffer: vk::CommandBuffer,
        src_image: vk::Image,
        src_image_layout: vk::ImageLayout,
        dst_image: vk::Image,
        dst_image_layout: vk::ImageLayout,
        p_regions: &[vk::ImageResolve],
    ) {
        let fp = self.fp_cmd_resolve_image;
        let region_count = p_regions.len() as u32;
        (fp)(
            command_buffer,
            src_image,
            src_image_layout,
            dst_image,
            dst_image_layout,
            region_count,
            p_regions.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_event(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        stage_mask: vk::PipelineStageFlags,
    ) {
        let fp = self.fp_cmd_set_event;
        (fp)(command_buffer, event, stage_mask)
    }
    pub unsafe fn cmd_reset_event(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        stage_mask: vk::PipelineStageFlags,
    ) {
        let fp = self.fp_cmd_reset_event;
        (fp)(command_buffer, event, stage_mask)
    }
    pub unsafe fn cmd_wait_events(
        &self,
        command_buffer: vk::CommandBuffer,
        p_events: &[vk::Event],
        src_stage_mask: vk::PipelineStageFlags,
        dst_stage_mask: vk::PipelineStageFlags,
        p_memory_barriers: &[vk::MemoryBarrier],
        p_buffer_memory_barriers: &[vk::BufferMemoryBarrier],
        p_image_memory_barriers: &[vk::ImageMemoryBarrier],
    ) {
        let fp = self.fp_cmd_wait_events;
        let event_count = p_events.len() as u32;
        let memory_barrier_count = p_memory_barriers.len() as u32;
        let buffer_memory_barrier_count = p_buffer_memory_barriers.len() as u32;
        let image_memory_barrier_count = p_image_memory_barriers.len() as u32;
        (fp)(
            command_buffer,
            event_count,
            p_events.as_ptr(),
            src_stage_mask,
            dst_stage_mask,
            memory_barrier_count,
            p_memory_barriers.as_ptr(),
            buffer_memory_barrier_count,
            p_buffer_memory_barriers.as_ptr(),
            image_memory_barrier_count,
            p_image_memory_barriers.as_ptr(),
        )
    }
    pub unsafe fn cmd_pipeline_barrier(
        &self,
        command_buffer: vk::CommandBuffer,
        src_stage_mask: vk::PipelineStageFlags,
        dst_stage_mask: vk::PipelineStageFlags,
        dependency_flags: vk::DependencyFlags,
        p_memory_barriers: &[vk::MemoryBarrier],
        p_buffer_memory_barriers: &[vk::BufferMemoryBarrier],
        p_image_memory_barriers: &[vk::ImageMemoryBarrier],
    ) {
        let fp = self.fp_cmd_pipeline_barrier;
        let memory_barrier_count = p_memory_barriers.len() as u32;
        let buffer_memory_barrier_count = p_buffer_memory_barriers.len() as u32;
        let image_memory_barrier_count = p_image_memory_barriers.len() as u32;
        (fp)(
            command_buffer,
            src_stage_mask,
            dst_stage_mask,
            dependency_flags,
            memory_barrier_count,
            p_memory_barriers.as_ptr(),
            buffer_memory_barrier_count,
            p_buffer_memory_barriers.as_ptr(),
            image_memory_barrier_count,
            p_image_memory_barriers.as_ptr(),
        )
    }
    pub unsafe fn cmd_begin_query(
        &self,
        command_buffer: vk::CommandBuffer,
        query_pool: vk::QueryPool,
        query: u32,
        flags: vk::QueryControlFlags,
    ) {
        let fp = self.fp_cmd_begin_query;
        (fp)(command_buffer, query_pool, query, flags)
    }
    pub unsafe fn cmd_end_query(&self, command_buffer: vk::CommandBuffer, query_pool: vk::QueryPool, query: u32) {
        let fp = self.fp_cmd_end_query;
        (fp)(command_buffer, query_pool, query)
    }
    pub unsafe fn cmd_begin_conditional_rendering_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_conditional_rendering_begin: &vk::ConditionalRenderingBeginInfoEXT,
    ) {
        let fp = self
            .fp_cmd_begin_conditional_rendering_ext
            .expect("vkCmdBeginConditionalRenderingEXT is not loaded");
        (fp)(command_buffer, p_conditional_rendering_begin)
    }
    pub unsafe fn cmd_end_conditional_rendering_ext(&self, command_buffer: vk::CommandBuffer) {
        let fp = self
            .fp_cmd_end_conditional_rendering_ext
            .expect("vkCmdEndConditionalRenderingEXT is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_begin_custom_resolve_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_begin_custom_resolve_info: Option<&vk::BeginCustomResolveInfoEXT>,
    ) {
        let fp = self
            .fp_cmd_begin_custom_resolve_ext
            .expect("vkCmdBeginCustomResolveEXT is not loaded");
        (fp)(command_buffer, p_begin_custom_resolve_info.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_reset_query_pool(
        &self,
        command_buffer: vk::CommandBuffer,
        query_pool: vk::QueryPool,
        first_query: u32,
        query_count: u32,
    ) {
        let fp = self.fp_cmd_reset_query_pool;
        (fp)(command_buffer, query_pool, first_query, query_count)
    }
    pub unsafe fn cmd_write_timestamp(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_stage: vk::PipelineStageFlags,
        query_pool: vk::QueryPool,
        query: u32,
    ) {
        let fp = self.fp_cmd_write_timestamp;
        (fp)(command_buffer, pipeline_stage, query_pool, query)
    }
    pub unsafe fn cmd_copy_query_pool_results(
        &self,
        command_buffer: vk::CommandBuffer,
        query_pool: vk::QueryPool,
        first_query: u32,
        query_count: u32,
        dst_buffer: vk::Buffer,
        dst_offset: vk::DeviceSize,
        stride: vk::DeviceSize,
        flags: vk::QueryResultFlags,
    ) {
        let fp = self.fp_cmd_copy_query_pool_results;
        (fp)(
            command_buffer,
            query_pool,
            first_query,
            query_count,
            dst_buffer,
            dst_offset,
            stride,
            flags,
        )
    }
    pub unsafe fn cmd_push_constants(
        &self,
        command_buffer: vk::CommandBuffer,
        layout: vk::PipelineLayout,
        stage_flags: vk::ShaderStageFlags,
        offset: u32,
        p_values: &[u8],
    ) {
        let fp = self.fp_cmd_push_constants;
        let size = p_values.len() as u32;
        (fp)(
            command_buffer,
            layout,
            stage_flags,
            offset,
            size,
            p_values.as_ptr() as *const _,
        )
    }
    pub unsafe fn cmd_begin_render_pass(
        &self,
        command_buffer: vk::CommandBuffer,
        p_render_pass_begin: &vk::RenderPassBeginInfo,
        contents: vk::SubpassContents,
    ) {
        let fp = self.fp_cmd_begin_render_pass;
        (fp)(command_buffer, p_render_pass_begin, contents)
    }
    pub unsafe fn cmd_next_subpass(&self, command_buffer: vk::CommandBuffer, contents: vk::SubpassContents) {
        let fp = self.fp_cmd_next_subpass;
        (fp)(command_buffer, contents)
    }
    pub unsafe fn cmd_end_render_pass(&self, command_buffer: vk::CommandBuffer) {
        let fp = self.fp_cmd_end_render_pass;
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_execute_commands(
        &self,
        command_buffer: vk::CommandBuffer,
        p_command_buffers: &[vk::CommandBuffer],
    ) {
        let fp = self.fp_cmd_execute_commands;
        let command_buffer_count = p_command_buffers.len() as u32;
        (fp)(command_buffer, command_buffer_count, p_command_buffers.as_ptr())
    }
    pub unsafe fn create_shared_swapchains_khr(
        &self,
        p_create_infos: &[vk::SwapchainCreateInfoKHR],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_swapchains: &mut [vk::SwapchainKHR],
    ) -> Result<()> {
        let fp = self
            .fp_create_shared_swapchains_khr
            .expect("vkCreateSharedSwapchainsKHR is not loaded");
        let swapchain_count = p_create_infos.len() as u32;
        assert_eq!(swapchain_count, p_swapchains.len() as u32);
        let err = (fp)(
            self.handle,
            swapchain_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_swapchains.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_shared_swapchains_khr_single(
        &self,
        p_create_infos: &vk::SwapchainCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SwapchainKHR> {
        let mut p_swapchains = Default::default();
        self.create_shared_swapchains_khr(
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_swapchains),
        )
        .map(|_| p_swapchains)
    }
    pub unsafe fn create_swapchain_khr(
        &self,
        p_create_info: &vk::SwapchainCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SwapchainKHR> {
        let fp = self
            .fp_create_swapchain_khr
            .expect("vkCreateSwapchainKHR is not loaded");
        let mut p_swapchain = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_swapchain.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_swapchain.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_swapchain_khr(
        &self,
        swapchain: vk::SwapchainKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_swapchain_khr
            .expect("vkDestroySwapchainKHR is not loaded");
        (fp)(self.handle, swapchain, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_swapchain_images_khr(
        &self,
        swapchain: vk::SwapchainKHR,
        p_swapchain_image_count: &mut u32,
        p_swapchain_images: *mut vk::Image,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_swapchain_images_khr
            .expect("vkGetSwapchainImagesKHR is not loaded");
        let err = (fp)(self.handle, swapchain, p_swapchain_image_count, p_swapchain_images);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_swapchain_images_khr_to_vec(&self, swapchain: vk::SwapchainKHR) -> Result<Vec<vk::Image>> {
        enumerate_generic_to_vec(|len, ptr| self.get_swapchain_images_khr(swapchain, len, ptr))
    }
    pub unsafe fn acquire_next_image_khr(
        &self,
        swapchain: vk::SwapchainKHR,
        timeout: u64,
        semaphore: vk::Semaphore,
        fence: vk::Fence,
        p_image_index: &mut u32,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_acquire_next_image_khr
            .expect("vkAcquireNextImageKHR is not loaded");
        let err = (fp)(self.handle, swapchain, timeout, semaphore, fence, p_image_index);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT | vk::Result::NOT_READY | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn queue_present_khr(
        &self,
        queue: vk::Queue,
        p_present_info: &vk::PresentInfoKHR,
    ) -> Result<vk::Result> {
        let fp = self.fp_queue_present_khr.expect("vkQueuePresentKHR is not loaded");
        let err = (fp)(queue, p_present_info);
        match err {
            vk::Result::SUCCESS | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn debug_marker_set_object_name_ext(
        &self,
        p_name_info: &vk::DebugMarkerObjectNameInfoEXT,
    ) -> Result<()> {
        let fp = self
            .fp_debug_marker_set_object_name_ext
            .expect("vkDebugMarkerSetObjectNameEXT is not loaded");
        let err = (fp)(self.handle, p_name_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn debug_marker_set_object_tag_ext(&self, p_tag_info: &vk::DebugMarkerObjectTagInfoEXT) -> Result<()> {
        let fp = self
            .fp_debug_marker_set_object_tag_ext
            .expect("vkDebugMarkerSetObjectTagEXT is not loaded");
        let err = (fp)(self.handle, p_tag_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_debug_marker_begin_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_marker_info: &vk::DebugMarkerMarkerInfoEXT,
    ) {
        let fp = self
            .fp_cmd_debug_marker_begin_ext
            .expect("vkCmdDebugMarkerBeginEXT is not loaded");
        (fp)(command_buffer, p_marker_info)
    }
    pub unsafe fn cmd_debug_marker_end_ext(&self, command_buffer: vk::CommandBuffer) {
        let fp = self
            .fp_cmd_debug_marker_end_ext
            .expect("vkCmdDebugMarkerEndEXT is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_debug_marker_insert_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_marker_info: &vk::DebugMarkerMarkerInfoEXT,
    ) {
        let fp = self
            .fp_cmd_debug_marker_insert_ext
            .expect("vkCmdDebugMarkerInsertEXT is not loaded");
        (fp)(command_buffer, p_marker_info)
    }
    pub unsafe fn get_memory_win32_handle_nv(
        &self,
        memory: vk::DeviceMemory,
        handle_type: vk::ExternalMemoryHandleTypeFlagsNV,
    ) -> Result<vk::HANDLE> {
        let fp = self
            .fp_get_memory_win32_handle_nv
            .expect("vkGetMemoryWin32HandleNV is not loaded");
        let mut p_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, memory, handle_type, p_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_execute_generated_commands_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        is_preprocessed: bool,
        p_generated_commands_info: &vk::GeneratedCommandsInfoNV,
    ) {
        let fp = self
            .fp_cmd_execute_generated_commands_nv
            .expect("vkCmdExecuteGeneratedCommandsNV is not loaded");
        (fp)(
            command_buffer,
            if is_preprocessed { vk::TRUE } else { vk::FALSE },
            p_generated_commands_info,
        )
    }
    pub unsafe fn cmd_preprocess_generated_commands_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_generated_commands_info: &vk::GeneratedCommandsInfoNV,
    ) {
        let fp = self
            .fp_cmd_preprocess_generated_commands_nv
            .expect("vkCmdPreprocessGeneratedCommandsNV is not loaded");
        (fp)(command_buffer, p_generated_commands_info)
    }
    pub unsafe fn cmd_bind_pipeline_shader_group_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        pipeline: vk::Pipeline,
        group_index: u32,
    ) {
        let fp = self
            .fp_cmd_bind_pipeline_shader_group_nv
            .expect("vkCmdBindPipelineShaderGroupNV is not loaded");
        (fp)(command_buffer, pipeline_bind_point, pipeline, group_index)
    }
    pub unsafe fn get_generated_commands_memory_requirements_nv(
        &self,
        p_info: &vk::GeneratedCommandsMemoryRequirementsInfoNV,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_generated_commands_memory_requirements_nv
            .expect("vkGetGeneratedCommandsMemoryRequirementsNV is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn create_indirect_commands_layout_nv(
        &self,
        p_create_info: &vk::IndirectCommandsLayoutCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::IndirectCommandsLayoutNV> {
        let fp = self
            .fp_create_indirect_commands_layout_nv
            .expect("vkCreateIndirectCommandsLayoutNV is not loaded");
        let mut p_indirect_commands_layout = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_indirect_commands_layout.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_indirect_commands_layout.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_indirect_commands_layout_nv(
        &self,
        indirect_commands_layout: vk::IndirectCommandsLayoutNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_indirect_commands_layout_nv
            .expect("vkDestroyIndirectCommandsLayoutNV is not loaded");
        (fp)(
            self.handle,
            indirect_commands_layout,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn cmd_execute_generated_commands_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        is_preprocessed: bool,
        p_generated_commands_info: &vk::GeneratedCommandsInfoEXT,
    ) {
        let fp = self
            .fp_cmd_execute_generated_commands_ext
            .expect("vkCmdExecuteGeneratedCommandsEXT is not loaded");
        (fp)(
            command_buffer,
            if is_preprocessed { vk::TRUE } else { vk::FALSE },
            p_generated_commands_info,
        )
    }
    pub unsafe fn cmd_preprocess_generated_commands_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_generated_commands_info: &vk::GeneratedCommandsInfoEXT,
        state_command_buffer: vk::CommandBuffer,
    ) {
        let fp = self
            .fp_cmd_preprocess_generated_commands_ext
            .expect("vkCmdPreprocessGeneratedCommandsEXT is not loaded");
        (fp)(command_buffer, p_generated_commands_info, state_command_buffer)
    }
    pub unsafe fn get_generated_commands_memory_requirements_ext(
        &self,
        p_info: &vk::GeneratedCommandsMemoryRequirementsInfoEXT,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_generated_commands_memory_requirements_ext
            .expect("vkGetGeneratedCommandsMemoryRequirementsEXT is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn create_indirect_commands_layout_ext(
        &self,
        p_create_info: &vk::IndirectCommandsLayoutCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::IndirectCommandsLayoutEXT> {
        let fp = self
            .fp_create_indirect_commands_layout_ext
            .expect("vkCreateIndirectCommandsLayoutEXT is not loaded");
        let mut p_indirect_commands_layout = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_indirect_commands_layout.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_indirect_commands_layout.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_indirect_commands_layout_ext(
        &self,
        indirect_commands_layout: vk::IndirectCommandsLayoutEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_indirect_commands_layout_ext
            .expect("vkDestroyIndirectCommandsLayoutEXT is not loaded");
        (fp)(
            self.handle,
            indirect_commands_layout,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn create_indirect_execution_set_ext(
        &self,
        p_create_info: &vk::IndirectExecutionSetCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::IndirectExecutionSetEXT> {
        let fp = self
            .fp_create_indirect_execution_set_ext
            .expect("vkCreateIndirectExecutionSetEXT is not loaded");
        let mut p_indirect_execution_set = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_indirect_execution_set.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_indirect_execution_set.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_indirect_execution_set_ext(
        &self,
        indirect_execution_set: vk::IndirectExecutionSetEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_indirect_execution_set_ext
            .expect("vkDestroyIndirectExecutionSetEXT is not loaded");
        (fp)(
            self.handle,
            indirect_execution_set,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn update_indirect_execution_set_pipeline_ext(
        &self,
        indirect_execution_set: vk::IndirectExecutionSetEXT,
        p_execution_set_writes: &[vk::WriteIndirectExecutionSetPipelineEXT],
    ) {
        let fp = self
            .fp_update_indirect_execution_set_pipeline_ext
            .expect("vkUpdateIndirectExecutionSetPipelineEXT is not loaded");
        let execution_set_write_count = p_execution_set_writes.len() as u32;
        (fp)(
            self.handle,
            indirect_execution_set,
            execution_set_write_count,
            p_execution_set_writes.as_ptr(),
        )
    }
    pub unsafe fn update_indirect_execution_set_shader_ext(
        &self,
        indirect_execution_set: vk::IndirectExecutionSetEXT,
        p_execution_set_writes: &[vk::WriteIndirectExecutionSetShaderEXT],
    ) {
        let fp = self
            .fp_update_indirect_execution_set_shader_ext
            .expect("vkUpdateIndirectExecutionSetShaderEXT is not loaded");
        let execution_set_write_count = p_execution_set_writes.len() as u32;
        (fp)(
            self.handle,
            indirect_execution_set,
            execution_set_write_count,
            p_execution_set_writes.as_ptr(),
        )
    }
    pub unsafe fn cmd_push_descriptor_set(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        layout: vk::PipelineLayout,
        set: u32,
        p_descriptor_writes: &[vk::WriteDescriptorSet],
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set
            .expect("vkCmdPushDescriptorSet is not loaded");
        let descriptor_write_count = p_descriptor_writes.len() as u32;
        (fp)(
            command_buffer,
            pipeline_bind_point,
            layout,
            set,
            descriptor_write_count,
            p_descriptor_writes.as_ptr(),
        )
    }
    pub unsafe fn cmd_push_descriptor_set_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        layout: vk::PipelineLayout,
        set: u32,
        p_descriptor_writes: &[vk::WriteDescriptorSet],
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set
            .expect("vkCmdPushDescriptorSetKHR is not loaded");
        let descriptor_write_count = p_descriptor_writes.len() as u32;
        (fp)(
            command_buffer,
            pipeline_bind_point,
            layout,
            set,
            descriptor_write_count,
            p_descriptor_writes.as_ptr(),
        )
    }
    pub unsafe fn trim_command_pool(&self, command_pool: vk::CommandPool, flags: vk::CommandPoolTrimFlags) {
        let fp = self.fp_trim_command_pool.expect("vkTrimCommandPool is not loaded");
        (fp)(self.handle, command_pool, flags)
    }
    pub unsafe fn trim_command_pool_khr(&self, command_pool: vk::CommandPool, flags: vk::CommandPoolTrimFlags) {
        let fp = self.fp_trim_command_pool.expect("vkTrimCommandPoolKHR is not loaded");
        (fp)(self.handle, command_pool, flags)
    }
    pub unsafe fn get_memory_win32_handle_khr(
        &self,
        p_get_win32_handle_info: &vk::MemoryGetWin32HandleInfoKHR,
    ) -> Result<vk::HANDLE> {
        let fp = self
            .fp_get_memory_win32_handle_khr
            .expect("vkGetMemoryWin32HandleKHR is not loaded");
        let mut p_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_win32_handle_info, p_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_win32_handle_properties_khr(
        &self,
        handle_type: vk::ExternalMemoryHandleTypeFlags,
        handle: vk::HANDLE,
        p_memory_win32_handle_properties: &mut vk::MemoryWin32HandlePropertiesKHR,
    ) -> Result<()> {
        let fp = self
            .fp_get_memory_win32_handle_properties_khr
            .expect("vkGetMemoryWin32HandlePropertiesKHR is not loaded");
        let err = (fp)(self.handle, handle_type, handle, p_memory_win32_handle_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_fd_khr(&self, p_get_fd_info: &vk::MemoryGetFdInfoKHR) -> Result<c_int> {
        let fp = self.fp_get_memory_fd_khr.expect("vkGetMemoryFdKHR is not loaded");
        let mut p_fd = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_fd_info, p_fd.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_fd.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_fd_properties_khr(
        &self,
        handle_type: vk::ExternalMemoryHandleTypeFlags,
        fd: c_int,
        p_memory_fd_properties: &mut vk::MemoryFdPropertiesKHR,
    ) -> Result<()> {
        let fp = self
            .fp_get_memory_fd_properties_khr
            .expect("vkGetMemoryFdPropertiesKHR is not loaded");
        let err = (fp)(self.handle, handle_type, fd, p_memory_fd_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_zircon_handle_fuchsia(
        &self,
        p_get_zircon_handle_info: &vk::MemoryGetZirconHandleInfoFUCHSIA,
    ) -> Result<vk::zx_handle_t> {
        let fp = self
            .fp_get_memory_zircon_handle_fuchsia
            .expect("vkGetMemoryZirconHandleFUCHSIA is not loaded");
        let mut p_zircon_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_zircon_handle_info, p_zircon_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_zircon_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_zircon_handle_properties_fuchsia(
        &self,
        handle_type: vk::ExternalMemoryHandleTypeFlags,
        zircon_handle: vk::zx_handle_t,
        p_memory_zircon_handle_properties: &mut vk::MemoryZirconHandlePropertiesFUCHSIA,
    ) -> Result<()> {
        let fp = self
            .fp_get_memory_zircon_handle_properties_fuchsia
            .expect("vkGetMemoryZirconHandlePropertiesFUCHSIA is not loaded");
        let err = (fp)(
            self.handle,
            handle_type,
            zircon_handle,
            p_memory_zircon_handle_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_remote_address_nv(
        &self,
        p_memory_get_remote_address_info: &vk::MemoryGetRemoteAddressInfoNV,
    ) -> Result<vk::RemoteAddressNV> {
        let fp = self
            .fp_get_memory_remote_address_nv
            .expect("vkGetMemoryRemoteAddressNV is not loaded");
        let mut p_address = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_memory_get_remote_address_info, p_address.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_address.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_semaphore_win32_handle_khr(
        &self,
        p_get_win32_handle_info: &vk::SemaphoreGetWin32HandleInfoKHR,
    ) -> Result<vk::HANDLE> {
        let fp = self
            .fp_get_semaphore_win32_handle_khr
            .expect("vkGetSemaphoreWin32HandleKHR is not loaded");
        let mut p_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_win32_handle_info, p_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn import_semaphore_win32_handle_khr(
        &self,
        p_import_semaphore_win32_handle_info: &vk::ImportSemaphoreWin32HandleInfoKHR,
    ) -> Result<()> {
        let fp = self
            .fp_import_semaphore_win32_handle_khr
            .expect("vkImportSemaphoreWin32HandleKHR is not loaded");
        let err = (fp)(self.handle, p_import_semaphore_win32_handle_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_semaphore_fd_khr(&self, p_get_fd_info: &vk::SemaphoreGetFdInfoKHR) -> Result<c_int> {
        let fp = self.fp_get_semaphore_fd_khr.expect("vkGetSemaphoreFdKHR is not loaded");
        let mut p_fd = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_fd_info, p_fd.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_fd.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn import_semaphore_fd_khr(
        &self,
        p_import_semaphore_fd_info: &vk::ImportSemaphoreFdInfoKHR,
    ) -> Result<()> {
        let fp = self
            .fp_import_semaphore_fd_khr
            .expect("vkImportSemaphoreFdKHR is not loaded");
        let err = (fp)(self.handle, p_import_semaphore_fd_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_semaphore_zircon_handle_fuchsia(
        &self,
        p_get_zircon_handle_info: &vk::SemaphoreGetZirconHandleInfoFUCHSIA,
    ) -> Result<vk::zx_handle_t> {
        let fp = self
            .fp_get_semaphore_zircon_handle_fuchsia
            .expect("vkGetSemaphoreZirconHandleFUCHSIA is not loaded");
        let mut p_zircon_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_zircon_handle_info, p_zircon_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_zircon_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn import_semaphore_zircon_handle_fuchsia(
        &self,
        p_import_semaphore_zircon_handle_info: &vk::ImportSemaphoreZirconHandleInfoFUCHSIA,
    ) -> Result<()> {
        let fp = self
            .fp_import_semaphore_zircon_handle_fuchsia
            .expect("vkImportSemaphoreZirconHandleFUCHSIA is not loaded");
        let err = (fp)(self.handle, p_import_semaphore_zircon_handle_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_fence_win32_handle_khr(
        &self,
        p_get_win32_handle_info: &vk::FenceGetWin32HandleInfoKHR,
    ) -> Result<vk::HANDLE> {
        let fp = self
            .fp_get_fence_win32_handle_khr
            .expect("vkGetFenceWin32HandleKHR is not loaded");
        let mut p_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_win32_handle_info, p_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn import_fence_win32_handle_khr(
        &self,
        p_import_fence_win32_handle_info: &vk::ImportFenceWin32HandleInfoKHR,
    ) -> Result<()> {
        let fp = self
            .fp_import_fence_win32_handle_khr
            .expect("vkImportFenceWin32HandleKHR is not loaded");
        let err = (fp)(self.handle, p_import_fence_win32_handle_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_fence_fd_khr(&self, p_get_fd_info: &vk::FenceGetFdInfoKHR) -> Result<c_int> {
        let fp = self.fp_get_fence_fd_khr.expect("vkGetFenceFdKHR is not loaded");
        let mut p_fd = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_fd_info, p_fd.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_fd.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn import_fence_fd_khr(&self, p_import_fence_fd_info: &vk::ImportFenceFdInfoKHR) -> Result<()> {
        let fp = self.fp_import_fence_fd_khr.expect("vkImportFenceFdKHR is not loaded");
        let err = (fp)(self.handle, p_import_fence_fd_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_winrt_display_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        display: vk::DisplayKHR,
    ) -> Result<()> {
        let fp = self
            .fp_acquire_winrt_display_nv
            .expect("vkAcquireWinrtDisplayNV is not loaded");
        let err = (fp)(physical_device, display);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_winrt_display_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        device_relative_id: u32,
    ) -> Result<vk::DisplayKHR> {
        let fp = self.fp_get_winrt_display_nv.expect("vkGetWinrtDisplayNV is not loaded");
        let mut p_display = MaybeUninit::<_>::uninit();
        let err = (fp)(physical_device, device_relative_id, p_display.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_display.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn display_power_control_ext(
        &self,
        display: vk::DisplayKHR,
        p_display_power_info: &vk::DisplayPowerInfoEXT,
    ) -> Result<()> {
        let fp = self
            .fp_display_power_control_ext
            .expect("vkDisplayPowerControlEXT is not loaded");
        let err = (fp)(self.handle, display, p_display_power_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn register_device_event_ext(
        &self,
        p_device_event_info: &vk::DeviceEventInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Fence> {
        let fp = self
            .fp_register_device_event_ext
            .expect("vkRegisterDeviceEventEXT is not loaded");
        let mut p_fence = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_device_event_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_fence.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_fence.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn register_display_event_ext(
        &self,
        display: vk::DisplayKHR,
        p_display_event_info: &vk::DisplayEventInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::Fence> {
        let fp = self
            .fp_register_display_event_ext
            .expect("vkRegisterDisplayEventEXT is not loaded");
        let mut p_fence = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            display,
            p_display_event_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_fence.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_fence.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_swapchain_counter_ext(
        &self,
        swapchain: vk::SwapchainKHR,
        counter: vk::SurfaceCounterFlagsEXT,
    ) -> Result<u64> {
        let fp = self
            .fp_get_swapchain_counter_ext
            .expect("vkGetSwapchainCounterEXT is not loaded");
        let mut p_counter_value = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, swapchain, counter, p_counter_value.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_counter_value.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_group_peer_memory_features(
        &self,
        heap_index: u32,
        local_device_index: u32,
        remote_device_index: u32,
    ) -> vk::PeerMemoryFeatureFlags {
        let fp = self
            .fp_get_device_group_peer_memory_features
            .expect("vkGetDeviceGroupPeerMemoryFeatures is not loaded");
        let mut p_peer_memory_features = MaybeUninit::<_>::uninit();
        (fp)(
            self.handle,
            heap_index,
            local_device_index,
            remote_device_index,
            p_peer_memory_features.as_mut_ptr(),
        );
        p_peer_memory_features.assume_init()
    }
    pub unsafe fn get_device_group_peer_memory_features_khr(
        &self,
        heap_index: u32,
        local_device_index: u32,
        remote_device_index: u32,
    ) -> vk::PeerMemoryFeatureFlags {
        let fp = self
            .fp_get_device_group_peer_memory_features
            .expect("vkGetDeviceGroupPeerMemoryFeaturesKHR is not loaded");
        let mut p_peer_memory_features = MaybeUninit::<_>::uninit();
        (fp)(
            self.handle,
            heap_index,
            local_device_index,
            remote_device_index,
            p_peer_memory_features.as_mut_ptr(),
        );
        p_peer_memory_features.assume_init()
    }
    pub unsafe fn bind_buffer_memory2(&self, p_bind_infos: &[vk::BindBufferMemoryInfo]) -> Result<()> {
        let fp = self.fp_bind_buffer_memory2.expect("vkBindBufferMemory2 is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn bind_buffer_memory2_khr(&self, p_bind_infos: &[vk::BindBufferMemoryInfo]) -> Result<()> {
        let fp = self
            .fp_bind_buffer_memory2
            .expect("vkBindBufferMemory2KHR is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn bind_image_memory2(&self, p_bind_infos: &[vk::BindImageMemoryInfo]) -> Result<()> {
        let fp = self.fp_bind_image_memory2.expect("vkBindImageMemory2 is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn bind_image_memory2_khr(&self, p_bind_infos: &[vk::BindImageMemoryInfo]) -> Result<()> {
        let fp = self.fp_bind_image_memory2.expect("vkBindImageMemory2KHR is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_set_device_mask(&self, command_buffer: vk::CommandBuffer, device_mask: u32) {
        let fp = self.fp_cmd_set_device_mask.expect("vkCmdSetDeviceMask is not loaded");
        (fp)(command_buffer, device_mask)
    }
    pub unsafe fn cmd_set_device_mask_khr(&self, command_buffer: vk::CommandBuffer, device_mask: u32) {
        let fp = self
            .fp_cmd_set_device_mask
            .expect("vkCmdSetDeviceMaskKHR is not loaded");
        (fp)(command_buffer, device_mask)
    }
    pub unsafe fn get_device_group_present_capabilities_khr(
        &self,
        p_device_group_present_capabilities: &mut vk::DeviceGroupPresentCapabilitiesKHR,
    ) -> Result<()> {
        let fp = self
            .fp_get_device_group_present_capabilities_khr
            .expect("vkGetDeviceGroupPresentCapabilitiesKHR is not loaded");
        let err = (fp)(self.handle, p_device_group_present_capabilities);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_group_surface_present_modes_khr(
        &self,
        surface: vk::SurfaceKHR,
    ) -> Result<vk::DeviceGroupPresentModeFlagsKHR> {
        let fp = self
            .fp_get_device_group_surface_present_modes_khr
            .expect("vkGetDeviceGroupSurfacePresentModesKHR is not loaded");
        let mut p_modes = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, surface, p_modes.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_modes.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_next_image2_khr(
        &self,
        p_acquire_info: &vk::AcquireNextImageInfoKHR,
        p_image_index: &mut u32,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_acquire_next_image2_khr
            .expect("vkAcquireNextImage2KHR is not loaded");
        let err = (fp)(self.handle, p_acquire_info, p_image_index);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT | vk::Result::NOT_READY | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_dispatch_base(
        &self,
        command_buffer: vk::CommandBuffer,
        base_group_x: u32,
        base_group_y: u32,
        base_group_z: u32,
        group_count_x: u32,
        group_count_y: u32,
        group_count_z: u32,
    ) {
        let fp = self.fp_cmd_dispatch_base.expect("vkCmdDispatchBase is not loaded");
        (fp)(
            command_buffer,
            base_group_x,
            base_group_y,
            base_group_z,
            group_count_x,
            group_count_y,
            group_count_z,
        )
    }
    pub unsafe fn cmd_dispatch_base_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        base_group_x: u32,
        base_group_y: u32,
        base_group_z: u32,
        group_count_x: u32,
        group_count_y: u32,
        group_count_z: u32,
    ) {
        let fp = self.fp_cmd_dispatch_base.expect("vkCmdDispatchBaseKHR is not loaded");
        (fp)(
            command_buffer,
            base_group_x,
            base_group_y,
            base_group_z,
            group_count_x,
            group_count_y,
            group_count_z,
        )
    }
    pub unsafe fn get_physical_device_present_rectangles_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
        p_rect_count: &mut u32,
        p_rects: *mut vk::Rect2D,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_present_rectangles_khr
            .expect("vkGetPhysicalDevicePresentRectanglesKHR is not loaded");
        let err = (fp)(physical_device, surface, p_rect_count, p_rects);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_present_rectangles_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        surface: vk::SurfaceKHR,
    ) -> Result<Vec<vk::Rect2D>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_present_rectangles_khr(physical_device, surface, len, ptr)
        })
    }
    pub unsafe fn create_descriptor_update_template(
        &self,
        p_create_info: &vk::DescriptorUpdateTemplateCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DescriptorUpdateTemplate> {
        let fp = self
            .fp_create_descriptor_update_template
            .expect("vkCreateDescriptorUpdateTemplate is not loaded");
        let mut p_descriptor_update_template = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_descriptor_update_template.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_descriptor_update_template.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_descriptor_update_template_khr(
        &self,
        p_create_info: &vk::DescriptorUpdateTemplateCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DescriptorUpdateTemplate> {
        let fp = self
            .fp_create_descriptor_update_template
            .expect("vkCreateDescriptorUpdateTemplateKHR is not loaded");
        let mut p_descriptor_update_template = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_descriptor_update_template.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_descriptor_update_template.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_descriptor_update_template(
        &self,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_descriptor_update_template
            .expect("vkDestroyDescriptorUpdateTemplate is not loaded");
        (fp)(
            self.handle,
            descriptor_update_template,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn destroy_descriptor_update_template_khr(
        &self,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_descriptor_update_template
            .expect("vkDestroyDescriptorUpdateTemplateKHR is not loaded");
        (fp)(
            self.handle,
            descriptor_update_template,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn update_descriptor_set_with_template(
        &self,
        descriptor_set: vk::DescriptorSet,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        p_data: *const c_void,
    ) {
        let fp = self
            .fp_update_descriptor_set_with_template
            .expect("vkUpdateDescriptorSetWithTemplate is not loaded");
        (fp)(self.handle, descriptor_set, descriptor_update_template, p_data)
    }
    pub unsafe fn update_descriptor_set_with_template_khr(
        &self,
        descriptor_set: vk::DescriptorSet,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        p_data: *const c_void,
    ) {
        let fp = self
            .fp_update_descriptor_set_with_template
            .expect("vkUpdateDescriptorSetWithTemplateKHR is not loaded");
        (fp)(self.handle, descriptor_set, descriptor_update_template, p_data)
    }
    pub unsafe fn cmd_push_descriptor_set_with_template(
        &self,
        command_buffer: vk::CommandBuffer,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        layout: vk::PipelineLayout,
        set: u32,
        p_data: *const c_void,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set_with_template
            .expect("vkCmdPushDescriptorSetWithTemplate is not loaded");
        (fp)(command_buffer, descriptor_update_template, layout, set, p_data)
    }
    pub unsafe fn cmd_push_descriptor_set_with_template_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        descriptor_update_template: vk::DescriptorUpdateTemplate,
        layout: vk::PipelineLayout,
        set: u32,
        p_data: *const c_void,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set_with_template
            .expect("vkCmdPushDescriptorSetWithTemplateKHR is not loaded");
        (fp)(command_buffer, descriptor_update_template, layout, set, p_data)
    }
    pub unsafe fn set_hdr_metadata_ext(&self, p_swapchains: &[vk::SwapchainKHR], p_metadata: &[vk::HdrMetadataEXT]) {
        let fp = self.fp_set_hdr_metadata_ext.expect("vkSetHdrMetadataEXT is not loaded");
        let swapchain_count = p_swapchains.len() as u32;
        assert_eq!(swapchain_count, p_metadata.len() as u32);
        (fp)(self.handle, swapchain_count, p_swapchains.as_ptr(), p_metadata.as_ptr())
    }
    pub unsafe fn get_swapchain_status_khr(&self, swapchain: vk::SwapchainKHR) -> Result<vk::Result> {
        let fp = self
            .fp_get_swapchain_status_khr
            .expect("vkGetSwapchainStatusKHR is not loaded");
        let err = (fp)(self.handle, swapchain);
        match err {
            vk::Result::SUCCESS | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_refresh_cycle_duration_google(
        &self,
        swapchain: vk::SwapchainKHR,
    ) -> Result<vk::RefreshCycleDurationGOOGLE> {
        let fp = self
            .fp_get_refresh_cycle_duration_google
            .expect("vkGetRefreshCycleDurationGOOGLE is not loaded");
        let mut p_display_timing_properties = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, swapchain, p_display_timing_properties.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_display_timing_properties.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_past_presentation_timing_google(
        &self,
        swapchain: vk::SwapchainKHR,
        p_presentation_timing_count: &mut u32,
        p_presentation_timings: *mut vk::PastPresentationTimingGOOGLE,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_past_presentation_timing_google
            .expect("vkGetPastPresentationTimingGOOGLE is not loaded");
        let err = (fp)(
            self.handle,
            swapchain,
            p_presentation_timing_count,
            p_presentation_timings,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_past_presentation_timing_google_to_vec(
        &self,
        swapchain: vk::SwapchainKHR,
    ) -> Result<Vec<vk::PastPresentationTimingGOOGLE>> {
        enumerate_generic_to_vec(|len, ptr| self.get_past_presentation_timing_google(swapchain, len, ptr))
    }
    pub unsafe fn cmd_set_viewport_w_scaling_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        first_viewport: u32,
        p_viewport_w_scalings: &[vk::ViewportWScalingNV],
    ) {
        let fp = self
            .fp_cmd_set_viewport_w_scaling_nv
            .expect("vkCmdSetViewportWScalingNV is not loaded");
        let viewport_count = p_viewport_w_scalings.len() as u32;
        (fp)(
            command_buffer,
            first_viewport,
            viewport_count,
            p_viewport_w_scalings.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_discard_rectangle_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_discard_rectangle: u32,
        p_discard_rectangles: &[vk::Rect2D],
    ) {
        let fp = self
            .fp_cmd_set_discard_rectangle_ext
            .expect("vkCmdSetDiscardRectangleEXT is not loaded");
        let discard_rectangle_count = p_discard_rectangles.len() as u32;
        (fp)(
            command_buffer,
            first_discard_rectangle,
            discard_rectangle_count,
            p_discard_rectangles.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_discard_rectangle_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        discard_rectangle_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_discard_rectangle_enable_ext
            .expect("vkCmdSetDiscardRectangleEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if discard_rectangle_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_discard_rectangle_mode_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        discard_rectangle_mode: vk::DiscardRectangleModeEXT,
    ) {
        let fp = self
            .fp_cmd_set_discard_rectangle_mode_ext
            .expect("vkCmdSetDiscardRectangleModeEXT is not loaded");
        (fp)(command_buffer, discard_rectangle_mode)
    }
    pub unsafe fn cmd_set_sample_locations_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_sample_locations_info: &vk::SampleLocationsInfoEXT,
    ) {
        let fp = self
            .fp_cmd_set_sample_locations_ext
            .expect("vkCmdSetSampleLocationsEXT is not loaded");
        (fp)(command_buffer, p_sample_locations_info)
    }
    pub unsafe fn get_physical_device_multisample_properties_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        samples: vk::SampleCountFlags,
        p_multisample_properties: &mut vk::MultisamplePropertiesEXT,
    ) {
        let fp = self
            .fp_get_physical_device_multisample_properties_ext
            .expect("vkGetPhysicalDeviceMultisamplePropertiesEXT is not loaded");
        (fp)(physical_device, samples, p_multisample_properties)
    }
    pub unsafe fn get_buffer_memory_requirements2(
        &self,
        p_info: &vk::BufferMemoryRequirementsInfo2,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_buffer_memory_requirements2
            .expect("vkGetBufferMemoryRequirements2 is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_buffer_memory_requirements2_khr(
        &self,
        p_info: &vk::BufferMemoryRequirementsInfo2,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_buffer_memory_requirements2
            .expect("vkGetBufferMemoryRequirements2KHR is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_image_memory_requirements2(
        &self,
        p_info: &vk::ImageMemoryRequirementsInfo2,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_image_memory_requirements2
            .expect("vkGetImageMemoryRequirements2 is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_image_memory_requirements2_khr(
        &self,
        p_info: &vk::ImageMemoryRequirementsInfo2,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_image_memory_requirements2
            .expect("vkGetImageMemoryRequirements2KHR is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_image_sparse_memory_requirements2(
        &self,
        p_info: &vk::ImageSparseMemoryRequirementsInfo2,
        p_sparse_memory_requirement_count: &mut u32,
        p_sparse_memory_requirements: *mut vk::SparseImageMemoryRequirements2,
    ) {
        let fp = self
            .fp_get_image_sparse_memory_requirements2
            .expect("vkGetImageSparseMemoryRequirements2 is not loaded");
        (fp)(
            self.handle,
            p_info,
            p_sparse_memory_requirement_count,
            p_sparse_memory_requirements,
        );
    }
    pub unsafe fn get_image_sparse_memory_requirements2_to_vec(
        &self,
        p_info: &vk::ImageSparseMemoryRequirementsInfo2,
    ) -> Vec<vk::SparseImageMemoryRequirements2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| self.get_image_sparse_memory_requirements2(p_info, len, ptr))
    }
    pub unsafe fn get_image_sparse_memory_requirements2_khr(
        &self,
        p_info: &vk::ImageSparseMemoryRequirementsInfo2,
        p_sparse_memory_requirement_count: &mut u32,
        p_sparse_memory_requirements: *mut vk::SparseImageMemoryRequirements2,
    ) {
        let fp = self
            .fp_get_image_sparse_memory_requirements2
            .expect("vkGetImageSparseMemoryRequirements2KHR is not loaded");
        (fp)(
            self.handle,
            p_info,
            p_sparse_memory_requirement_count,
            p_sparse_memory_requirements,
        );
    }
    pub unsafe fn get_image_sparse_memory_requirements2_khr_to_vec(
        &self,
        p_info: &vk::ImageSparseMemoryRequirementsInfo2,
    ) -> Vec<vk::SparseImageMemoryRequirements2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| self.get_image_sparse_memory_requirements2_khr(p_info, len, ptr))
    }
    pub unsafe fn get_device_buffer_memory_requirements(
        &self,
        p_info: &vk::DeviceBufferMemoryRequirements,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_buffer_memory_requirements
            .expect("vkGetDeviceBufferMemoryRequirements is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_device_buffer_memory_requirements_khr(
        &self,
        p_info: &vk::DeviceBufferMemoryRequirements,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_buffer_memory_requirements
            .expect("vkGetDeviceBufferMemoryRequirementsKHR is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_device_image_memory_requirements(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_image_memory_requirements
            .expect("vkGetDeviceImageMemoryRequirements is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_device_image_memory_requirements_khr(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_image_memory_requirements
            .expect("vkGetDeviceImageMemoryRequirementsKHR is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn get_device_image_sparse_memory_requirements(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
        p_sparse_memory_requirement_count: &mut u32,
        p_sparse_memory_requirements: *mut vk::SparseImageMemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_image_sparse_memory_requirements
            .expect("vkGetDeviceImageSparseMemoryRequirements is not loaded");
        (fp)(
            self.handle,
            p_info,
            p_sparse_memory_requirement_count,
            p_sparse_memory_requirements,
        );
    }
    pub unsafe fn get_device_image_sparse_memory_requirements_to_vec(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
    ) -> Vec<vk::SparseImageMemoryRequirements2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_device_image_sparse_memory_requirements(p_info, len, ptr)
        })
    }
    pub unsafe fn get_device_image_sparse_memory_requirements_khr(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
        p_sparse_memory_requirement_count: &mut u32,
        p_sparse_memory_requirements: *mut vk::SparseImageMemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_image_sparse_memory_requirements
            .expect("vkGetDeviceImageSparseMemoryRequirementsKHR is not loaded");
        (fp)(
            self.handle,
            p_info,
            p_sparse_memory_requirement_count,
            p_sparse_memory_requirements,
        );
    }
    pub unsafe fn get_device_image_sparse_memory_requirements_khr_to_vec(
        &self,
        p_info: &vk::DeviceImageMemoryRequirements,
    ) -> Vec<vk::SparseImageMemoryRequirements2> {
        enumerate_generic_unchecked_to_vec(|len, ptr| {
            self.get_device_image_sparse_memory_requirements_khr(p_info, len, ptr)
        })
    }
    pub unsafe fn create_sampler_ycbcr_conversion(
        &self,
        p_create_info: &vk::SamplerYcbcrConversionCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SamplerYcbcrConversion> {
        let fp = self
            .fp_create_sampler_ycbcr_conversion
            .expect("vkCreateSamplerYcbcrConversion is not loaded");
        let mut p_ycbcr_conversion = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_ycbcr_conversion.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_ycbcr_conversion.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_sampler_ycbcr_conversion_khr(
        &self,
        p_create_info: &vk::SamplerYcbcrConversionCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::SamplerYcbcrConversion> {
        let fp = self
            .fp_create_sampler_ycbcr_conversion
            .expect("vkCreateSamplerYcbcrConversionKHR is not loaded");
        let mut p_ycbcr_conversion = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_ycbcr_conversion.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_ycbcr_conversion.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_sampler_ycbcr_conversion(
        &self,
        ycbcr_conversion: vk::SamplerYcbcrConversion,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_sampler_ycbcr_conversion
            .expect("vkDestroySamplerYcbcrConversion is not loaded");
        (fp)(self.handle, ycbcr_conversion, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn destroy_sampler_ycbcr_conversion_khr(
        &self,
        ycbcr_conversion: vk::SamplerYcbcrConversion,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_sampler_ycbcr_conversion
            .expect("vkDestroySamplerYcbcrConversionKHR is not loaded");
        (fp)(self.handle, ycbcr_conversion, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_device_queue2(&self, p_queue_info: &vk::DeviceQueueInfo2) -> vk::Queue {
        let fp = self.fp_get_device_queue2.expect("vkGetDeviceQueue2 is not loaded");
        let mut p_queue = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_queue_info, p_queue.as_mut_ptr());
        p_queue.assume_init()
    }
    pub unsafe fn create_validation_cache_ext(
        &self,
        p_create_info: &vk::ValidationCacheCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::ValidationCacheEXT> {
        let fp = self
            .fp_create_validation_cache_ext
            .expect("vkCreateValidationCacheEXT is not loaded");
        let mut p_validation_cache = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_validation_cache.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_validation_cache.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_validation_cache_ext(
        &self,
        validation_cache: vk::ValidationCacheEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_validation_cache_ext
            .expect("vkDestroyValidationCacheEXT is not loaded");
        (fp)(self.handle, validation_cache, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_validation_cache_data_ext(
        &self,
        validation_cache: vk::ValidationCacheEXT,
        p_data_size: &mut usize,
        p_data: *mut c_void,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_validation_cache_data_ext
            .expect("vkGetValidationCacheDataEXT is not loaded");
        let err = (fp)(self.handle, validation_cache, p_data_size, p_data);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn merge_validation_caches_ext(
        &self,
        dst_cache: vk::ValidationCacheEXT,
        p_src_caches: &[vk::ValidationCacheEXT],
    ) -> Result<()> {
        let fp = self
            .fp_merge_validation_caches_ext
            .expect("vkMergeValidationCachesEXT is not loaded");
        let src_cache_count = p_src_caches.len() as u32;
        let err = (fp)(self.handle, dst_cache, src_cache_count, p_src_caches.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_descriptor_set_layout_support(
        &self,
        p_create_info: &vk::DescriptorSetLayoutCreateInfo,
        p_support: &mut vk::DescriptorSetLayoutSupport,
    ) {
        let fp = self
            .fp_get_descriptor_set_layout_support
            .expect("vkGetDescriptorSetLayoutSupport is not loaded");
        (fp)(self.handle, p_create_info, p_support)
    }
    pub unsafe fn get_descriptor_set_layout_support_khr(
        &self,
        p_create_info: &vk::DescriptorSetLayoutCreateInfo,
        p_support: &mut vk::DescriptorSetLayoutSupport,
    ) {
        let fp = self
            .fp_get_descriptor_set_layout_support
            .expect("vkGetDescriptorSetLayoutSupportKHR is not loaded");
        (fp)(self.handle, p_create_info, p_support)
    }
    pub unsafe fn get_shader_info_amd(
        &self,
        pipeline: vk::Pipeline,
        shader_stage: vk::ShaderStageFlags,
        info_type: vk::ShaderInfoTypeAMD,
        p_info_size: &mut usize,
        p_info: *mut c_void,
    ) -> Result<vk::Result> {
        let fp = self.fp_get_shader_info_amd.expect("vkGetShaderInfoAMD is not loaded");
        let err = (fp)(self.handle, pipeline, shader_stage, info_type, p_info_size, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn set_local_dimming_amd(&self, swap_chain: vk::SwapchainKHR, local_dimming_enable: bool) {
        let fp = self
            .fp_set_local_dimming_amd
            .expect("vkSetLocalDimmingAMD is not loaded");
        (fp)(
            self.handle,
            swap_chain,
            if local_dimming_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn get_physical_device_calibrateable_time_domains_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_time_domain_count: &mut u32,
        p_time_domains: *mut vk::TimeDomainKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_calibrateable_time_domains_khr
            .expect("vkGetPhysicalDeviceCalibrateableTimeDomainsKHR is not loaded");
        let err = (fp)(physical_device, p_time_domain_count, p_time_domains);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_calibrateable_time_domains_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::TimeDomainKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_calibrateable_time_domains_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_physical_device_calibrateable_time_domains_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        p_time_domain_count: &mut u32,
        p_time_domains: *mut vk::TimeDomainKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_calibrateable_time_domains_khr
            .expect("vkGetPhysicalDeviceCalibrateableTimeDomainsEXT is not loaded");
        let err = (fp)(physical_device, p_time_domain_count, p_time_domains);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_calibrateable_time_domains_ext_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::TimeDomainKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_calibrateable_time_domains_ext(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_calibrated_timestamps_khr(
        &self,
        p_timestamp_infos: &[vk::CalibratedTimestampInfoKHR],
        p_timestamps: &mut [u64],
    ) -> Result<u64> {
        let fp = self
            .fp_get_calibrated_timestamps_khr
            .expect("vkGetCalibratedTimestampsKHR is not loaded");
        let timestamp_count = p_timestamp_infos.len() as u32;
        assert_eq!(timestamp_count, p_timestamps.len() as u32);
        let mut p_max_deviation = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            timestamp_count,
            p_timestamp_infos.as_ptr(),
            p_timestamps.as_mut_ptr(),
            p_max_deviation.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_max_deviation.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_calibrated_timestamps_ext(
        &self,
        p_timestamp_infos: &[vk::CalibratedTimestampInfoKHR],
        p_timestamps: &mut [u64],
    ) -> Result<u64> {
        let fp = self
            .fp_get_calibrated_timestamps_khr
            .expect("vkGetCalibratedTimestampsEXT is not loaded");
        let timestamp_count = p_timestamp_infos.len() as u32;
        assert_eq!(timestamp_count, p_timestamps.len() as u32);
        let mut p_max_deviation = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            timestamp_count,
            p_timestamp_infos.as_ptr(),
            p_timestamps.as_mut_ptr(),
            p_max_deviation.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_max_deviation.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_host_pointer_properties_ext(
        &self,
        handle_type: vk::ExternalMemoryHandleTypeFlags,
        p_host_pointer: *const c_void,
        p_memory_host_pointer_properties: &mut vk::MemoryHostPointerPropertiesEXT,
    ) -> Result<()> {
        let fp = self
            .fp_get_memory_host_pointer_properties_ext
            .expect("vkGetMemoryHostPointerPropertiesEXT is not loaded");
        let err = (fp)(
            self.handle,
            handle_type,
            p_host_pointer,
            p_memory_host_pointer_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_write_buffer_marker_amd(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_stage: vk::PipelineStageFlags,
        dst_buffer: vk::Buffer,
        dst_offset: vk::DeviceSize,
        marker: u32,
    ) {
        let fp = self
            .fp_cmd_write_buffer_marker_amd
            .expect("vkCmdWriteBufferMarkerAMD is not loaded");
        (fp)(command_buffer, pipeline_stage, dst_buffer, dst_offset, marker)
    }
    pub unsafe fn create_render_pass2(
        &self,
        p_create_info: &vk::RenderPassCreateInfo2,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::RenderPass> {
        let fp = self.fp_create_render_pass2.expect("vkCreateRenderPass2 is not loaded");
        let mut p_render_pass = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_render_pass.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_render_pass.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_render_pass2_khr(
        &self,
        p_create_info: &vk::RenderPassCreateInfo2,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::RenderPass> {
        let fp = self
            .fp_create_render_pass2
            .expect("vkCreateRenderPass2KHR is not loaded");
        let mut p_render_pass = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_render_pass.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_render_pass.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_begin_render_pass2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_render_pass_begin: &vk::RenderPassBeginInfo,
        p_subpass_begin_info: &vk::SubpassBeginInfo,
    ) {
        let fp = self
            .fp_cmd_begin_render_pass2
            .expect("vkCmdBeginRenderPass2 is not loaded");
        (fp)(command_buffer, p_render_pass_begin, p_subpass_begin_info)
    }
    pub unsafe fn cmd_begin_render_pass2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_render_pass_begin: &vk::RenderPassBeginInfo,
        p_subpass_begin_info: &vk::SubpassBeginInfo,
    ) {
        let fp = self
            .fp_cmd_begin_render_pass2
            .expect("vkCmdBeginRenderPass2KHR is not loaded");
        (fp)(command_buffer, p_render_pass_begin, p_subpass_begin_info)
    }
    pub unsafe fn cmd_next_subpass2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_subpass_begin_info: &vk::SubpassBeginInfo,
        p_subpass_end_info: &vk::SubpassEndInfo,
    ) {
        let fp = self.fp_cmd_next_subpass2.expect("vkCmdNextSubpass2 is not loaded");
        (fp)(command_buffer, p_subpass_begin_info, p_subpass_end_info)
    }
    pub unsafe fn cmd_next_subpass2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_subpass_begin_info: &vk::SubpassBeginInfo,
        p_subpass_end_info: &vk::SubpassEndInfo,
    ) {
        let fp = self.fp_cmd_next_subpass2.expect("vkCmdNextSubpass2KHR is not loaded");
        (fp)(command_buffer, p_subpass_begin_info, p_subpass_end_info)
    }
    pub unsafe fn cmd_end_render_pass2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_subpass_end_info: &vk::SubpassEndInfo,
    ) {
        let fp = self.fp_cmd_end_render_pass2.expect("vkCmdEndRenderPass2 is not loaded");
        (fp)(command_buffer, p_subpass_end_info)
    }
    pub unsafe fn cmd_end_render_pass2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_subpass_end_info: &vk::SubpassEndInfo,
    ) {
        let fp = self
            .fp_cmd_end_render_pass2
            .expect("vkCmdEndRenderPass2KHR is not loaded");
        (fp)(command_buffer, p_subpass_end_info)
    }
    pub unsafe fn get_semaphore_counter_value(&self, semaphore: vk::Semaphore) -> Result<u64> {
        let fp = self
            .fp_get_semaphore_counter_value
            .expect("vkGetSemaphoreCounterValue is not loaded");
        let mut p_value = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, semaphore, p_value.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_value.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_semaphore_counter_value_khr(&self, semaphore: vk::Semaphore) -> Result<u64> {
        let fp = self
            .fp_get_semaphore_counter_value
            .expect("vkGetSemaphoreCounterValueKHR is not loaded");
        let mut p_value = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, semaphore, p_value.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_value.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn wait_semaphores(&self, p_wait_info: &vk::SemaphoreWaitInfo, timeout: u64) -> Result<vk::Result> {
        let fp = self.fp_wait_semaphores.expect("vkWaitSemaphores is not loaded");
        let err = (fp)(self.handle, p_wait_info, timeout);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn wait_semaphores_khr(&self, p_wait_info: &vk::SemaphoreWaitInfo, timeout: u64) -> Result<vk::Result> {
        let fp = self.fp_wait_semaphores.expect("vkWaitSemaphoresKHR is not loaded");
        let err = (fp)(self.handle, p_wait_info, timeout);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn signal_semaphore(&self, p_signal_info: &vk::SemaphoreSignalInfo) -> Result<()> {
        let fp = self.fp_signal_semaphore.expect("vkSignalSemaphore is not loaded");
        let err = (fp)(self.handle, p_signal_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn signal_semaphore_khr(&self, p_signal_info: &vk::SemaphoreSignalInfo) -> Result<()> {
        let fp = self.fp_signal_semaphore.expect("vkSignalSemaphoreKHR is not loaded");
        let err = (fp)(self.handle, p_signal_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_android_hardware_buffer_properties_android(
        &self,
        buffer: &vk::AHardwareBuffer,
        p_properties: &mut vk::AndroidHardwareBufferPropertiesANDROID,
    ) -> Result<()> {
        let fp = self
            .fp_get_android_hardware_buffer_properties_android
            .expect("vkGetAndroidHardwareBufferPropertiesANDROID is not loaded");
        let err = (fp)(self.handle, buffer, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_android_hardware_buffer_android(
        &self,
        p_info: &vk::MemoryGetAndroidHardwareBufferInfoANDROID,
    ) -> Result<*mut vk::AHardwareBuffer> {
        let fp = self
            .fp_get_memory_android_hardware_buffer_android
            .expect("vkGetMemoryAndroidHardwareBufferANDROID is not loaded");
        let mut p_buffer = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_info, p_buffer.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_buffer.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_draw_indirect_count(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indirect_count
            .expect("vkCmdDrawIndirectCount is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_indirect_count_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indirect_count
            .expect("vkCmdDrawIndirectCountKHR is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_indirect_count_amd(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indirect_count
            .expect("vkCmdDrawIndirectCountAMD is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_indexed_indirect_count(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indexed_indirect_count
            .expect("vkCmdDrawIndexedIndirectCount is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_indexed_indirect_count_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indexed_indirect_count
            .expect("vkCmdDrawIndexedIndirectCountKHR is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_indexed_indirect_count_amd(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indexed_indirect_count
            .expect("vkCmdDrawIndexedIndirectCountAMD is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_set_checkpoint_nv(&self, command_buffer: vk::CommandBuffer, p_checkpoint_marker: *const c_void) {
        let fp = self
            .fp_cmd_set_checkpoint_nv
            .expect("vkCmdSetCheckpointNV is not loaded");
        (fp)(command_buffer, p_checkpoint_marker)
    }
    pub unsafe fn get_queue_checkpoint_data_nv(
        &self,
        queue: vk::Queue,
        p_checkpoint_data_count: &mut u32,
        p_checkpoint_data: *mut vk::CheckpointDataNV,
    ) {
        let fp = self
            .fp_get_queue_checkpoint_data_nv
            .expect("vkGetQueueCheckpointDataNV is not loaded");
        (fp)(queue, p_checkpoint_data_count, p_checkpoint_data);
    }
    pub unsafe fn get_queue_checkpoint_data_nv_to_vec(&self, queue: vk::Queue) -> Vec<vk::CheckpointDataNV> {
        enumerate_generic_unchecked_to_vec(|len, ptr| self.get_queue_checkpoint_data_nv(queue, len, ptr))
    }
    pub unsafe fn cmd_bind_transform_feedback_buffers_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_binding: u32,
        p_buffers: &[vk::Buffer],
        p_offsets: &[vk::DeviceSize],
        p_sizes: Option<&[vk::DeviceSize]>,
    ) {
        let fp = self
            .fp_cmd_bind_transform_feedback_buffers_ext
            .expect("vkCmdBindTransformFeedbackBuffersEXT is not loaded");
        let binding_count = p_buffers.len() as u32;
        assert_eq!(binding_count, p_offsets.len() as u32);
        if let Some(s) = p_sizes {
            assert_eq!(binding_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            first_binding,
            binding_count,
            p_buffers.as_ptr(),
            p_offsets.as_ptr(),
            p_sizes.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn cmd_begin_transform_feedback_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_counter_buffer: u32,
        p_counter_buffers: &[vk::Buffer],
        p_counter_buffer_offsets: Option<&[vk::DeviceSize]>,
    ) {
        let fp = self
            .fp_cmd_begin_transform_feedback_ext
            .expect("vkCmdBeginTransformFeedbackEXT is not loaded");
        let counter_buffer_count = p_counter_buffers.len() as u32;
        if let Some(s) = p_counter_buffer_offsets {
            assert_eq!(counter_buffer_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            first_counter_buffer,
            counter_buffer_count,
            p_counter_buffers.as_ptr(),
            p_counter_buffer_offsets.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn cmd_end_transform_feedback_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_counter_buffer: u32,
        p_counter_buffers: &[vk::Buffer],
        p_counter_buffer_offsets: Option<&[vk::DeviceSize]>,
    ) {
        let fp = self
            .fp_cmd_end_transform_feedback_ext
            .expect("vkCmdEndTransformFeedbackEXT is not loaded");
        let counter_buffer_count = p_counter_buffers.len() as u32;
        if let Some(s) = p_counter_buffer_offsets {
            assert_eq!(counter_buffer_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            first_counter_buffer,
            counter_buffer_count,
            p_counter_buffers.as_ptr(),
            p_counter_buffer_offsets.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn cmd_begin_query_indexed_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        query_pool: vk::QueryPool,
        query: u32,
        flags: vk::QueryControlFlags,
        index: u32,
    ) {
        let fp = self
            .fp_cmd_begin_query_indexed_ext
            .expect("vkCmdBeginQueryIndexedEXT is not loaded");
        (fp)(command_buffer, query_pool, query, flags, index)
    }
    pub unsafe fn cmd_end_query_indexed_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        query_pool: vk::QueryPool,
        query: u32,
        index: u32,
    ) {
        let fp = self
            .fp_cmd_end_query_indexed_ext
            .expect("vkCmdEndQueryIndexedEXT is not loaded");
        (fp)(command_buffer, query_pool, query, index)
    }
    pub unsafe fn cmd_draw_indirect_byte_count_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        instance_count: u32,
        first_instance: u32,
        counter_buffer: vk::Buffer,
        counter_buffer_offset: vk::DeviceSize,
        counter_offset: u32,
        vertex_stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_indirect_byte_count_ext
            .expect("vkCmdDrawIndirectByteCountEXT is not loaded");
        (fp)(
            command_buffer,
            instance_count,
            first_instance,
            counter_buffer,
            counter_buffer_offset,
            counter_offset,
            vertex_stride,
        )
    }
    pub unsafe fn cmd_set_exclusive_scissor_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        first_exclusive_scissor: u32,
        p_exclusive_scissors: &[vk::Rect2D],
    ) {
        let fp = self
            .fp_cmd_set_exclusive_scissor_nv
            .expect("vkCmdSetExclusiveScissorNV is not loaded");
        let exclusive_scissor_count = p_exclusive_scissors.len() as u32;
        (fp)(
            command_buffer,
            first_exclusive_scissor,
            exclusive_scissor_count,
            p_exclusive_scissors.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_exclusive_scissor_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        first_exclusive_scissor: u32,
        p_exclusive_scissor_enables: &[vk::Bool32],
    ) {
        let fp = self
            .fp_cmd_set_exclusive_scissor_enable_nv
            .expect("vkCmdSetExclusiveScissorEnableNV is not loaded");
        let exclusive_scissor_count = p_exclusive_scissor_enables.len() as u32;
        (fp)(
            command_buffer,
            first_exclusive_scissor,
            exclusive_scissor_count,
            p_exclusive_scissor_enables.as_ptr(),
        )
    }
    pub unsafe fn cmd_bind_shading_rate_image_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        image_view: vk::ImageView,
        image_layout: vk::ImageLayout,
    ) {
        let fp = self
            .fp_cmd_bind_shading_rate_image_nv
            .expect("vkCmdBindShadingRateImageNV is not loaded");
        (fp)(command_buffer, image_view, image_layout)
    }
    pub unsafe fn cmd_set_viewport_shading_rate_palette_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        first_viewport: u32,
        p_shading_rate_palettes: &[vk::ShadingRatePaletteNV],
    ) {
        let fp = self
            .fp_cmd_set_viewport_shading_rate_palette_nv
            .expect("vkCmdSetViewportShadingRatePaletteNV is not loaded");
        let viewport_count = p_shading_rate_palettes.len() as u32;
        (fp)(
            command_buffer,
            first_viewport,
            viewport_count,
            p_shading_rate_palettes.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_coarse_sample_order_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        sample_order_type: vk::CoarseSampleOrderTypeNV,
        p_custom_sample_orders: &[vk::CoarseSampleOrderCustomNV],
    ) {
        let fp = self
            .fp_cmd_set_coarse_sample_order_nv
            .expect("vkCmdSetCoarseSampleOrderNV is not loaded");
        let custom_sample_order_count = p_custom_sample_orders.len() as u32;
        (fp)(
            command_buffer,
            sample_order_type,
            custom_sample_order_count,
            p_custom_sample_orders.as_ptr(),
        )
    }
    pub unsafe fn cmd_draw_mesh_tasks_nv(&self, command_buffer: vk::CommandBuffer, task_count: u32, first_task: u32) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_nv
            .expect("vkCmdDrawMeshTasksNV is not loaded");
        (fp)(command_buffer, task_count, first_task)
    }
    pub unsafe fn cmd_draw_mesh_tasks_indirect_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_indirect_nv
            .expect("vkCmdDrawMeshTasksIndirectNV is not loaded");
        (fp)(command_buffer, buffer, offset, draw_count, stride)
    }
    pub unsafe fn cmd_draw_mesh_tasks_indirect_count_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_indirect_count_nv
            .expect("vkCmdDrawMeshTasksIndirectCountNV is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn cmd_draw_mesh_tasks_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        group_count_x: u32,
        group_count_y: u32,
        group_count_z: u32,
    ) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_ext
            .expect("vkCmdDrawMeshTasksEXT is not loaded");
        (fp)(command_buffer, group_count_x, group_count_y, group_count_z)
    }
    pub unsafe fn cmd_draw_mesh_tasks_indirect_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_indirect_ext
            .expect("vkCmdDrawMeshTasksIndirectEXT is not loaded");
        (fp)(command_buffer, buffer, offset, draw_count, stride)
    }
    pub unsafe fn cmd_draw_mesh_tasks_indirect_count_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        count_buffer: vk::Buffer,
        count_buffer_offset: vk::DeviceSize,
        max_draw_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_draw_mesh_tasks_indirect_count_ext
            .expect("vkCmdDrawMeshTasksIndirectCountEXT is not loaded");
        (fp)(
            command_buffer,
            buffer,
            offset,
            count_buffer,
            count_buffer_offset,
            max_draw_count,
            stride,
        )
    }
    pub unsafe fn compile_deferred_nv(&self, pipeline: vk::Pipeline, shader: u32) -> Result<()> {
        let fp = self.fp_compile_deferred_nv.expect("vkCompileDeferredNV is not loaded");
        let err = (fp)(self.handle, pipeline, shader);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_acceleration_structure_nv(
        &self,
        p_create_info: &vk::AccelerationStructureCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::AccelerationStructureNV> {
        let fp = self
            .fp_create_acceleration_structure_nv
            .expect("vkCreateAccelerationStructureNV is not loaded");
        let mut p_acceleration_structure = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_acceleration_structure.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_acceleration_structure.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_bind_invocation_mask_huawei(
        &self,
        command_buffer: vk::CommandBuffer,
        image_view: vk::ImageView,
        image_layout: vk::ImageLayout,
    ) {
        let fp = self
            .fp_cmd_bind_invocation_mask_huawei
            .expect("vkCmdBindInvocationMaskHUAWEI is not loaded");
        (fp)(command_buffer, image_view, image_layout)
    }
    pub unsafe fn destroy_acceleration_structure_khr(
        &self,
        acceleration_structure: vk::AccelerationStructureKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_acceleration_structure_khr
            .expect("vkDestroyAccelerationStructureKHR is not loaded");
        (fp)(
            self.handle,
            acceleration_structure,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn destroy_acceleration_structure_nv(
        &self,
        acceleration_structure: vk::AccelerationStructureNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_acceleration_structure_nv
            .expect("vkDestroyAccelerationStructureNV is not loaded");
        (fp)(
            self.handle,
            acceleration_structure,
            p_allocator.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn get_acceleration_structure_memory_requirements_nv(
        &self,
        p_info: &vk::AccelerationStructureMemoryRequirementsInfoNV,
    ) -> vk::MemoryRequirements2KHR {
        let fp = self
            .fp_get_acceleration_structure_memory_requirements_nv
            .expect("vkGetAccelerationStructureMemoryRequirementsNV is not loaded");
        let mut p_memory_requirements = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_info, p_memory_requirements.as_mut_ptr());
        p_memory_requirements.assume_init()
    }
    pub unsafe fn bind_acceleration_structure_memory_nv(
        &self,
        p_bind_infos: &[vk::BindAccelerationStructureMemoryInfoNV],
    ) -> Result<()> {
        let fp = self
            .fp_bind_acceleration_structure_memory_nv
            .expect("vkBindAccelerationStructureMemoryNV is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_copy_acceleration_structure_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        dst: vk::AccelerationStructureNV,
        src: vk::AccelerationStructureNV,
        mode: vk::CopyAccelerationStructureModeKHR,
    ) {
        let fp = self
            .fp_cmd_copy_acceleration_structure_nv
            .expect("vkCmdCopyAccelerationStructureNV is not loaded");
        (fp)(command_buffer, dst, src, mode)
    }
    pub unsafe fn cmd_copy_acceleration_structure_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::CopyAccelerationStructureInfoKHR,
    ) {
        let fp = self
            .fp_cmd_copy_acceleration_structure_khr
            .expect("vkCmdCopyAccelerationStructureKHR is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_acceleration_structure_khr(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyAccelerationStructureInfoKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_copy_acceleration_structure_khr
            .expect("vkCopyAccelerationStructureKHR is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_copy_acceleration_structure_to_memory_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::CopyAccelerationStructureToMemoryInfoKHR,
    ) {
        let fp = self
            .fp_cmd_copy_acceleration_structure_to_memory_khr
            .expect("vkCmdCopyAccelerationStructureToMemoryKHR is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_acceleration_structure_to_memory_khr(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyAccelerationStructureToMemoryInfoKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_copy_acceleration_structure_to_memory_khr
            .expect("vkCopyAccelerationStructureToMemoryKHR is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_copy_memory_to_acceleration_structure_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::CopyMemoryToAccelerationStructureInfoKHR,
    ) {
        let fp = self
            .fp_cmd_copy_memory_to_acceleration_structure_khr
            .expect("vkCmdCopyMemoryToAccelerationStructureKHR is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_memory_to_acceleration_structure_khr(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyMemoryToAccelerationStructureInfoKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_copy_memory_to_acceleration_structure_khr
            .expect("vkCopyMemoryToAccelerationStructureKHR is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_write_acceleration_structures_properties_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_acceleration_structures: &[vk::AccelerationStructureKHR],
        query_type: vk::QueryType,
        query_pool: vk::QueryPool,
        first_query: u32,
    ) {
        let fp = self
            .fp_cmd_write_acceleration_structures_properties_khr
            .expect("vkCmdWriteAccelerationStructuresPropertiesKHR is not loaded");
        let acceleration_structure_count = p_acceleration_structures.len() as u32;
        (fp)(
            command_buffer,
            acceleration_structure_count,
            p_acceleration_structures.as_ptr(),
            query_type,
            query_pool,
            first_query,
        )
    }
    pub unsafe fn cmd_write_acceleration_structures_properties_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_acceleration_structures: &[vk::AccelerationStructureNV],
        query_type: vk::QueryType,
        query_pool: vk::QueryPool,
        first_query: u32,
    ) {
        let fp = self
            .fp_cmd_write_acceleration_structures_properties_nv
            .expect("vkCmdWriteAccelerationStructuresPropertiesNV is not loaded");
        let acceleration_structure_count = p_acceleration_structures.len() as u32;
        (fp)(
            command_buffer,
            acceleration_structure_count,
            p_acceleration_structures.as_ptr(),
            query_type,
            query_pool,
            first_query,
        )
    }
    pub unsafe fn cmd_build_acceleration_structure_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::AccelerationStructureInfoNV,
        instance_data: vk::Buffer,
        instance_offset: vk::DeviceSize,
        update: bool,
        dst: vk::AccelerationStructureNV,
        src: vk::AccelerationStructureNV,
        scratch: vk::Buffer,
        scratch_offset: vk::DeviceSize,
    ) {
        let fp = self
            .fp_cmd_build_acceleration_structure_nv
            .expect("vkCmdBuildAccelerationStructureNV is not loaded");
        (fp)(
            command_buffer,
            p_info,
            instance_data,
            instance_offset,
            if update { vk::TRUE } else { vk::FALSE },
            dst,
            src,
            scratch,
            scratch_offset,
        )
    }
    pub unsafe fn write_acceleration_structures_properties_khr(
        &self,
        p_acceleration_structures: &[vk::AccelerationStructureKHR],
        query_type: vk::QueryType,
        p_data: &mut [u8],
        stride: usize,
    ) -> Result<()> {
        let fp = self
            .fp_write_acceleration_structures_properties_khr
            .expect("vkWriteAccelerationStructuresPropertiesKHR is not loaded");
        let acceleration_structure_count = p_acceleration_structures.len() as u32;
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            acceleration_structure_count,
            p_acceleration_structures.as_ptr(),
            query_type,
            data_size,
            p_data.as_mut_ptr() as *mut _,
            stride,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_trace_rays_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_raygen_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_miss_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_hit_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_callable_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        width: u32,
        height: u32,
        depth: u32,
    ) {
        let fp = self.fp_cmd_trace_rays_khr.expect("vkCmdTraceRaysKHR is not loaded");
        (fp)(
            command_buffer,
            p_raygen_shader_binding_table,
            p_miss_shader_binding_table,
            p_hit_shader_binding_table,
            p_callable_shader_binding_table,
            width,
            height,
            depth,
        )
    }
    pub unsafe fn cmd_trace_rays_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        raygen_shader_binding_table_buffer: vk::Buffer,
        raygen_shader_binding_offset: vk::DeviceSize,
        miss_shader_binding_table_buffer: vk::Buffer,
        miss_shader_binding_offset: vk::DeviceSize,
        miss_shader_binding_stride: vk::DeviceSize,
        hit_shader_binding_table_buffer: vk::Buffer,
        hit_shader_binding_offset: vk::DeviceSize,
        hit_shader_binding_stride: vk::DeviceSize,
        callable_shader_binding_table_buffer: vk::Buffer,
        callable_shader_binding_offset: vk::DeviceSize,
        callable_shader_binding_stride: vk::DeviceSize,
        width: u32,
        height: u32,
        depth: u32,
    ) {
        let fp = self.fp_cmd_trace_rays_nv.expect("vkCmdTraceRaysNV is not loaded");
        (fp)(
            command_buffer,
            raygen_shader_binding_table_buffer,
            raygen_shader_binding_offset,
            miss_shader_binding_table_buffer,
            miss_shader_binding_offset,
            miss_shader_binding_stride,
            hit_shader_binding_table_buffer,
            hit_shader_binding_offset,
            hit_shader_binding_stride,
            callable_shader_binding_table_buffer,
            callable_shader_binding_offset,
            callable_shader_binding_stride,
            width,
            height,
            depth,
        )
    }
    pub unsafe fn get_ray_tracing_shader_group_handles_khr(
        &self,
        pipeline: vk::Pipeline,
        first_group: u32,
        group_count: u32,
        p_data: &mut [u8],
    ) -> Result<()> {
        let fp = self
            .fp_get_ray_tracing_shader_group_handles_khr
            .expect("vkGetRayTracingShaderGroupHandlesKHR is not loaded");
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            pipeline,
            first_group,
            group_count,
            data_size,
            p_data.as_mut_ptr() as *mut _,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_ray_tracing_shader_group_handles_nv(
        &self,
        pipeline: vk::Pipeline,
        first_group: u32,
        group_count: u32,
        p_data: &mut [u8],
    ) -> Result<()> {
        let fp = self
            .fp_get_ray_tracing_shader_group_handles_khr
            .expect("vkGetRayTracingShaderGroupHandlesNV is not loaded");
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            pipeline,
            first_group,
            group_count,
            data_size,
            p_data.as_mut_ptr() as *mut _,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_ray_tracing_capture_replay_shader_group_handles_khr(
        &self,
        pipeline: vk::Pipeline,
        first_group: u32,
        group_count: u32,
        p_data: &mut [u8],
    ) -> Result<()> {
        let fp = self
            .fp_get_ray_tracing_capture_replay_shader_group_handles_khr
            .expect("vkGetRayTracingCaptureReplayShaderGroupHandlesKHR is not loaded");
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            pipeline,
            first_group,
            group_count,
            data_size,
            p_data.as_mut_ptr() as *mut _,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_acceleration_structure_handle_nv(
        &self,
        acceleration_structure: vk::AccelerationStructureNV,
        p_data: &mut [u8],
    ) -> Result<()> {
        let fp = self
            .fp_get_acceleration_structure_handle_nv
            .expect("vkGetAccelerationStructureHandleNV is not loaded");
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            acceleration_structure,
            data_size,
            p_data.as_mut_ptr() as *mut _,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_ray_tracing_pipelines_nv(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::RayTracingPipelineCreateInfoNV],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_create_ray_tracing_pipelines_nv
            .expect("vkCreateRayTracingPipelinesNV is not loaded");
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_ray_tracing_pipelines_nv_single(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::RayTracingPipelineCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_ray_tracing_pipelines_nv(
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn create_ray_tracing_pipelines_khr(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::RayTracingPipelineCreateInfoKHR],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_create_ray_tracing_pipelines_khr
            .expect("vkCreateRayTracingPipelinesKHR is not loaded");
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            deferred_operation,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS
            | vk::Result::OPERATION_DEFERRED_KHR
            | vk::Result::OPERATION_NOT_DEFERRED_KHR
            | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_ray_tracing_pipelines_khr_single(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::RayTracingPipelineCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_ray_tracing_pipelines_khr(
            deferred_operation,
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn get_physical_device_cooperative_matrix_properties_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::CooperativeMatrixPropertiesNV,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_cooperative_matrix_properties_nv
            .expect("vkGetPhysicalDeviceCooperativeMatrixPropertiesNV is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_cooperative_matrix_properties_nv_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::CooperativeMatrixPropertiesNV>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_cooperative_matrix_properties_nv(physical_device, len, ptr)
        })
    }
    pub unsafe fn cmd_trace_rays_indirect_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_raygen_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_miss_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_hit_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        p_callable_shader_binding_table: &vk::StridedDeviceAddressRegionKHR,
        indirect_device_address: vk::DeviceAddress,
    ) {
        let fp = self
            .fp_cmd_trace_rays_indirect_khr
            .expect("vkCmdTraceRaysIndirectKHR is not loaded");
        (fp)(
            command_buffer,
            p_raygen_shader_binding_table,
            p_miss_shader_binding_table,
            p_hit_shader_binding_table,
            p_callable_shader_binding_table,
            indirect_device_address,
        )
    }
    pub unsafe fn cmd_trace_rays_indirect2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        indirect_device_address: vk::DeviceAddress,
    ) {
        let fp = self
            .fp_cmd_trace_rays_indirect2_khr
            .expect("vkCmdTraceRaysIndirect2KHR is not loaded");
        (fp)(command_buffer, indirect_device_address)
    }
    pub unsafe fn get_cluster_acceleration_structure_build_sizes_nv(
        &self,
        p_info: &vk::ClusterAccelerationStructureInputInfoNV,
        p_size_info: &mut vk::AccelerationStructureBuildSizesInfoKHR,
    ) {
        let fp = self
            .fp_get_cluster_acceleration_structure_build_sizes_nv
            .expect("vkGetClusterAccelerationStructureBuildSizesNV is not loaded");
        (fp)(self.handle, p_info, p_size_info)
    }
    pub unsafe fn cmd_build_cluster_acceleration_structure_indirect_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_command_infos: &vk::ClusterAccelerationStructureCommandsInfoNV,
    ) {
        let fp = self
            .fp_cmd_build_cluster_acceleration_structure_indirect_nv
            .expect("vkCmdBuildClusterAccelerationStructureIndirectNV is not loaded");
        (fp)(command_buffer, p_command_infos)
    }
    pub unsafe fn get_device_acceleration_structure_compatibility_khr(
        &self,
        p_version_info: &vk::AccelerationStructureVersionInfoKHR,
    ) -> vk::AccelerationStructureCompatibilityKHR {
        let fp = self
            .fp_get_device_acceleration_structure_compatibility_khr
            .expect("vkGetDeviceAccelerationStructureCompatibilityKHR is not loaded");
        let mut p_compatibility = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_version_info, p_compatibility.as_mut_ptr());
        p_compatibility.assume_init()
    }
    pub unsafe fn get_ray_tracing_shader_group_stack_size_khr(
        &self,
        pipeline: vk::Pipeline,
        group: u32,
        group_shader: vk::ShaderGroupShaderKHR,
    ) -> vk::DeviceSize {
        let fp = self
            .fp_get_ray_tracing_shader_group_stack_size_khr
            .expect("vkGetRayTracingShaderGroupStackSizeKHR is not loaded");
        (fp)(self.handle, pipeline, group, group_shader)
    }
    pub unsafe fn cmd_set_ray_tracing_pipeline_stack_size_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_stack_size: u32,
    ) {
        let fp = self
            .fp_cmd_set_ray_tracing_pipeline_stack_size_khr
            .expect("vkCmdSetRayTracingPipelineStackSizeKHR is not loaded");
        (fp)(command_buffer, pipeline_stack_size)
    }
    pub unsafe fn get_image_view_handle_nvx(&self, p_info: &vk::ImageViewHandleInfoNVX) -> u32 {
        let fp = self
            .fp_get_image_view_handle_nvx
            .expect("vkGetImageViewHandleNVX is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_image_view_handle64_nvx(&self, p_info: &vk::ImageViewHandleInfoNVX) -> u64 {
        let fp = self
            .fp_get_image_view_handle64_nvx
            .expect("vkGetImageViewHandle64NVX is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_image_view_address_nvx(
        &self,
        image_view: vk::ImageView,
        p_properties: &mut vk::ImageViewAddressPropertiesNVX,
    ) -> Result<()> {
        let fp = self
            .fp_get_image_view_address_nvx
            .expect("vkGetImageViewAddressNVX is not loaded");
        let err = (fp)(self.handle, image_view, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_combined_image_sampler_index_nvx(&self, image_view_index: u64, sampler_index: u64) -> u64 {
        let fp = self
            .fp_get_device_combined_image_sampler_index_nvx
            .expect("vkGetDeviceCombinedImageSamplerIndexNVX is not loaded");
        (fp)(self.handle, image_view_index, sampler_index)
    }
    pub unsafe fn get_physical_device_surface_present_modes2_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
        p_present_mode_count: &mut u32,
        p_present_modes: *mut vk::PresentModeKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_surface_present_modes2_ext
            .expect("vkGetPhysicalDeviceSurfacePresentModes2EXT is not loaded");
        let err = (fp)(physical_device, p_surface_info, p_present_mode_count, p_present_modes);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_surface_present_modes2_ext_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
    ) -> Result<Vec<vk::PresentModeKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_surface_present_modes2_ext(physical_device, p_surface_info, len, ptr)
        })
    }
    pub unsafe fn get_device_group_surface_present_modes2_ext(
        &self,
        p_surface_info: &vk::PhysicalDeviceSurfaceInfo2KHR,
    ) -> Result<vk::DeviceGroupPresentModeFlagsKHR> {
        let fp = self
            .fp_get_device_group_surface_present_modes2_ext
            .expect("vkGetDeviceGroupSurfacePresentModes2EXT is not loaded");
        let mut p_modes = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_surface_info, p_modes.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_modes.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_full_screen_exclusive_mode_ext(&self, swapchain: vk::SwapchainKHR) -> Result<()> {
        let fp = self
            .fp_acquire_full_screen_exclusive_mode_ext
            .expect("vkAcquireFullScreenExclusiveModeEXT is not loaded");
        let err = (fp)(self.handle, swapchain);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn release_full_screen_exclusive_mode_ext(&self, swapchain: vk::SwapchainKHR) -> Result<()> {
        let fp = self
            .fp_release_full_screen_exclusive_mode_ext
            .expect("vkReleaseFullScreenExclusiveModeEXT is not loaded");
        let err = (fp)(self.handle, swapchain);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_device_queue_family_performance_query_counters_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        p_counter_count: &mut u32,
        p_counters: *mut vk::PerformanceCounterKHR,
        p_counter_descriptions: *mut vk::PerformanceCounterDescriptionKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_enumerate_physical_device_queue_family_performance_query_counters_khr
            .expect("vkEnumeratePhysicalDeviceQueueFamilyPerformanceQueryCountersKHR is not loaded");
        let err = (fp)(
            physical_device,
            queue_family_index,
            p_counter_count,
            p_counters,
            p_counter_descriptions,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_queue_family_performance_query_passes_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_performance_query_create_info: &vk::QueryPoolPerformanceCreateInfoKHR,
    ) -> u32 {
        let fp = self
            .fp_get_physical_device_queue_family_performance_query_passes_khr
            .expect("vkGetPhysicalDeviceQueueFamilyPerformanceQueryPassesKHR is not loaded");
        let mut p_num_passes = MaybeUninit::<_>::uninit();
        (fp)(
            physical_device,
            p_performance_query_create_info,
            p_num_passes.as_mut_ptr(),
        );
        p_num_passes.assume_init()
    }
    pub unsafe fn acquire_profiling_lock_khr(&self, p_info: &vk::AcquireProfilingLockInfoKHR) -> Result<()> {
        let fp = self
            .fp_acquire_profiling_lock_khr
            .expect("vkAcquireProfilingLockKHR is not loaded");
        let err = (fp)(self.handle, p_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn release_profiling_lock_khr(&self) {
        let fp = self
            .fp_release_profiling_lock_khr
            .expect("vkReleaseProfilingLockKHR is not loaded");
        (fp)(self.handle)
    }
    pub unsafe fn get_image_drm_format_modifier_properties_ext(
        &self,
        image: vk::Image,
        p_properties: &mut vk::ImageDrmFormatModifierPropertiesEXT,
    ) -> Result<()> {
        let fp = self
            .fp_get_image_drm_format_modifier_properties_ext
            .expect("vkGetImageDrmFormatModifierPropertiesEXT is not loaded");
        let err = (fp)(self.handle, image, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_buffer_opaque_capture_address(&self, p_info: &vk::BufferDeviceAddressInfo) -> u64 {
        let fp = self
            .fp_get_buffer_opaque_capture_address
            .expect("vkGetBufferOpaqueCaptureAddress is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_buffer_opaque_capture_address_khr(&self, p_info: &vk::BufferDeviceAddressInfo) -> u64 {
        let fp = self
            .fp_get_buffer_opaque_capture_address
            .expect("vkGetBufferOpaqueCaptureAddressKHR is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_buffer_device_address(&self, p_info: &vk::BufferDeviceAddressInfo) -> vk::DeviceAddress {
        let fp = self
            .fp_get_buffer_device_address
            .expect("vkGetBufferDeviceAddress is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_buffer_device_address_khr(&self, p_info: &vk::BufferDeviceAddressInfo) -> vk::DeviceAddress {
        let fp = self
            .fp_get_buffer_device_address
            .expect("vkGetBufferDeviceAddressKHR is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_buffer_device_address_ext(&self, p_info: &vk::BufferDeviceAddressInfo) -> vk::DeviceAddress {
        let fp = self
            .fp_get_buffer_device_address
            .expect("vkGetBufferDeviceAddressEXT is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_physical_device_supported_framebuffer_mixed_samples_combinations_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        p_combination_count: &mut u32,
        p_combinations: *mut vk::FramebufferMixedSamplesCombinationNV,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_supported_framebuffer_mixed_samples_combinations_nv
            .expect("vkGetPhysicalDeviceSupportedFramebufferMixedSamplesCombinationsNV is not loaded");
        let err = (fp)(physical_device, p_combination_count, p_combinations);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_supported_framebuffer_mixed_samples_combinations_nv_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::FramebufferMixedSamplesCombinationNV>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_supported_framebuffer_mixed_samples_combinations_nv(physical_device, len, ptr)
        })
    }
    pub unsafe fn initialize_performance_api_intel(
        &self,
        p_initialize_info: &vk::InitializePerformanceApiInfoINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_initialize_performance_api_intel
            .expect("vkInitializePerformanceApiINTEL is not loaded");
        let err = (fp)(self.handle, p_initialize_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn uninitialize_performance_api_intel(&self) {
        let fp = self
            .fp_uninitialize_performance_api_intel
            .expect("vkUninitializePerformanceApiINTEL is not loaded");
        (fp)(self.handle)
    }
    pub unsafe fn cmd_set_performance_marker_intel(
        &self,
        command_buffer: vk::CommandBuffer,
        p_marker_info: &vk::PerformanceMarkerInfoINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_cmd_set_performance_marker_intel
            .expect("vkCmdSetPerformanceMarkerINTEL is not loaded");
        let err = (fp)(command_buffer, p_marker_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_set_performance_stream_marker_intel(
        &self,
        command_buffer: vk::CommandBuffer,
        p_marker_info: &vk::PerformanceStreamMarkerInfoINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_cmd_set_performance_stream_marker_intel
            .expect("vkCmdSetPerformanceStreamMarkerINTEL is not loaded");
        let err = (fp)(command_buffer, p_marker_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_set_performance_override_intel(
        &self,
        command_buffer: vk::CommandBuffer,
        p_override_info: &vk::PerformanceOverrideInfoINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_cmd_set_performance_override_intel
            .expect("vkCmdSetPerformanceOverrideINTEL is not loaded");
        let err = (fp)(command_buffer, p_override_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn acquire_performance_configuration_intel(
        &self,
        p_acquire_info: &vk::PerformanceConfigurationAcquireInfoINTEL,
    ) -> Result<vk::PerformanceConfigurationINTEL> {
        let fp = self
            .fp_acquire_performance_configuration_intel
            .expect("vkAcquirePerformanceConfigurationINTEL is not loaded");
        let mut p_configuration = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_acquire_info, p_configuration.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_configuration.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn release_performance_configuration_intel(
        &self,
        configuration: vk::PerformanceConfigurationINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_release_performance_configuration_intel
            .expect("vkReleasePerformanceConfigurationINTEL is not loaded");
        let err = (fp)(self.handle, configuration);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn queue_set_performance_configuration_intel(
        &self,
        queue: vk::Queue,
        configuration: vk::PerformanceConfigurationINTEL,
    ) -> Result<()> {
        let fp = self
            .fp_queue_set_performance_configuration_intel
            .expect("vkQueueSetPerformanceConfigurationINTEL is not loaded");
        let err = (fp)(queue, configuration);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_performance_parameter_intel(
        &self,
        parameter: vk::PerformanceParameterTypeINTEL,
    ) -> Result<vk::PerformanceValueINTEL> {
        let fp = self
            .fp_get_performance_parameter_intel
            .expect("vkGetPerformanceParameterINTEL is not loaded");
        let mut p_value = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, parameter, p_value.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_value.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_memory_opaque_capture_address(
        &self,
        p_info: &vk::DeviceMemoryOpaqueCaptureAddressInfo,
    ) -> u64 {
        let fp = self
            .fp_get_device_memory_opaque_capture_address
            .expect("vkGetDeviceMemoryOpaqueCaptureAddress is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_device_memory_opaque_capture_address_khr(
        &self,
        p_info: &vk::DeviceMemoryOpaqueCaptureAddressInfo,
    ) -> u64 {
        let fp = self
            .fp_get_device_memory_opaque_capture_address
            .expect("vkGetDeviceMemoryOpaqueCaptureAddressKHR is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn get_pipeline_executable_properties_khr(
        &self,
        p_pipeline_info: &vk::PipelineInfoKHR,
        p_executable_count: &mut u32,
        p_properties: *mut vk::PipelineExecutablePropertiesKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_pipeline_executable_properties_khr
            .expect("vkGetPipelineExecutablePropertiesKHR is not loaded");
        let err = (fp)(self.handle, p_pipeline_info, p_executable_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_pipeline_executable_properties_khr_to_vec(
        &self,
        p_pipeline_info: &vk::PipelineInfoKHR,
    ) -> Result<Vec<vk::PipelineExecutablePropertiesKHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_pipeline_executable_properties_khr(p_pipeline_info, len, ptr))
    }
    pub unsafe fn get_pipeline_executable_statistics_khr(
        &self,
        p_executable_info: &vk::PipelineExecutableInfoKHR,
        p_statistic_count: &mut u32,
        p_statistics: *mut vk::PipelineExecutableStatisticKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_pipeline_executable_statistics_khr
            .expect("vkGetPipelineExecutableStatisticsKHR is not loaded");
        let err = (fp)(self.handle, p_executable_info, p_statistic_count, p_statistics);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_pipeline_executable_statistics_khr_to_vec(
        &self,
        p_executable_info: &vk::PipelineExecutableInfoKHR,
    ) -> Result<Vec<vk::PipelineExecutableStatisticKHR>> {
        enumerate_generic_to_vec(|len, ptr| self.get_pipeline_executable_statistics_khr(p_executable_info, len, ptr))
    }
    pub unsafe fn get_pipeline_executable_internal_representations_khr(
        &self,
        p_executable_info: &vk::PipelineExecutableInfoKHR,
        p_internal_representation_count: &mut u32,
        p_internal_representations: *mut vk::PipelineExecutableInternalRepresentationKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_pipeline_executable_internal_representations_khr
            .expect("vkGetPipelineExecutableInternalRepresentationsKHR is not loaded");
        let err = (fp)(
            self.handle,
            p_executable_info,
            p_internal_representation_count,
            p_internal_representations,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_pipeline_executable_internal_representations_khr_to_vec(
        &self,
        p_executable_info: &vk::PipelineExecutableInfoKHR,
    ) -> Result<Vec<vk::PipelineExecutableInternalRepresentationKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_pipeline_executable_internal_representations_khr(p_executable_info, len, ptr)
        })
    }
    pub unsafe fn cmd_set_line_stipple(
        &self,
        command_buffer: vk::CommandBuffer,
        line_stipple_factor: u32,
        line_stipple_pattern: u16,
    ) {
        let fp = self.fp_cmd_set_line_stipple.expect("vkCmdSetLineStipple is not loaded");
        (fp)(command_buffer, line_stipple_factor, line_stipple_pattern)
    }
    pub unsafe fn cmd_set_line_stipple_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        line_stipple_factor: u32,
        line_stipple_pattern: u16,
    ) {
        let fp = self
            .fp_cmd_set_line_stipple
            .expect("vkCmdSetLineStippleKHR is not loaded");
        (fp)(command_buffer, line_stipple_factor, line_stipple_pattern)
    }
    pub unsafe fn cmd_set_line_stipple_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        line_stipple_factor: u32,
        line_stipple_pattern: u16,
    ) {
        let fp = self
            .fp_cmd_set_line_stipple
            .expect("vkCmdSetLineStippleEXT is not loaded");
        (fp)(command_buffer, line_stipple_factor, line_stipple_pattern)
    }
    pub unsafe fn get_physical_device_tool_properties(
        &self,
        physical_device: vk::PhysicalDevice,
        p_tool_count: &mut u32,
        p_tool_properties: *mut vk::PhysicalDeviceToolProperties,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_tool_properties
            .expect("vkGetPhysicalDeviceToolProperties is not loaded");
        let err = (fp)(physical_device, p_tool_count, p_tool_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_tool_properties_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::PhysicalDeviceToolProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.get_physical_device_tool_properties(physical_device, len, ptr))
    }
    pub unsafe fn get_physical_device_tool_properties_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        p_tool_count: &mut u32,
        p_tool_properties: *mut vk::PhysicalDeviceToolProperties,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_tool_properties
            .expect("vkGetPhysicalDeviceToolPropertiesEXT is not loaded");
        let err = (fp)(physical_device, p_tool_count, p_tool_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_tool_properties_ext_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::PhysicalDeviceToolProperties>> {
        enumerate_generic_to_vec(|len, ptr| self.get_physical_device_tool_properties_ext(physical_device, len, ptr))
    }
    pub unsafe fn create_acceleration_structure_khr(
        &self,
        p_create_info: &vk::AccelerationStructureCreateInfoKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::AccelerationStructureKHR> {
        let fp = self
            .fp_create_acceleration_structure_khr
            .expect("vkCreateAccelerationStructureKHR is not loaded");
        let mut p_acceleration_structure = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_acceleration_structure.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_acceleration_structure.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_build_acceleration_structures_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_infos: &[vk::AccelerationStructureBuildGeometryInfoKHR],
        pp_build_range_infos: &[*const vk::AccelerationStructureBuildRangeInfoKHR],
    ) {
        let fp = self
            .fp_cmd_build_acceleration_structures_khr
            .expect("vkCmdBuildAccelerationStructuresKHR is not loaded");
        let info_count = p_infos.len() as u32;
        assert_eq!(info_count, pp_build_range_infos.len() as u32);
        (fp)(
            command_buffer,
            info_count,
            p_infos.as_ptr(),
            pp_build_range_infos.as_ptr(),
        )
    }
    pub unsafe fn cmd_build_acceleration_structures_indirect_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_infos: &[vk::AccelerationStructureBuildGeometryInfoKHR],
        p_indirect_device_addresses: &[vk::DeviceAddress],
        p_indirect_strides: &[u32],
        pp_max_primitive_counts: &[*const u32],
    ) {
        let fp = self
            .fp_cmd_build_acceleration_structures_indirect_khr
            .expect("vkCmdBuildAccelerationStructuresIndirectKHR is not loaded");
        let info_count = p_infos.len() as u32;
        assert_eq!(info_count, p_indirect_device_addresses.len() as u32);
        assert_eq!(info_count, p_indirect_strides.len() as u32);
        assert_eq!(info_count, pp_max_primitive_counts.len() as u32);
        (fp)(
            command_buffer,
            info_count,
            p_infos.as_ptr(),
            p_indirect_device_addresses.as_ptr(),
            p_indirect_strides.as_ptr(),
            pp_max_primitive_counts.as_ptr(),
        )
    }
    pub unsafe fn build_acceleration_structures_khr(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_infos: &[vk::AccelerationStructureBuildGeometryInfoKHR],
        pp_build_range_infos: &[*const vk::AccelerationStructureBuildRangeInfoKHR],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_build_acceleration_structures_khr
            .expect("vkBuildAccelerationStructuresKHR is not loaded");
        let info_count = p_infos.len() as u32;
        assert_eq!(info_count, pp_build_range_infos.len() as u32);
        let err = (fp)(
            self.handle,
            deferred_operation,
            info_count,
            p_infos.as_ptr(),
            pp_build_range_infos.as_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn get_acceleration_structure_device_address_khr(
        &self,
        p_info: &vk::AccelerationStructureDeviceAddressInfoKHR,
    ) -> vk::DeviceAddress {
        let fp = self
            .fp_get_acceleration_structure_device_address_khr
            .expect("vkGetAccelerationStructureDeviceAddressKHR is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn create_deferred_operation_khr(
        &self,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DeferredOperationKHR> {
        let fp = self
            .fp_create_deferred_operation_khr
            .expect("vkCreateDeferredOperationKHR is not loaded");
        let mut p_deferred_operation = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_allocator.map_or(ptr::null(), |r| r),
            p_deferred_operation.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_deferred_operation.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_deferred_operation_khr(
        &self,
        operation: vk::DeferredOperationKHR,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_deferred_operation_khr
            .expect("vkDestroyDeferredOperationKHR is not loaded");
        (fp)(self.handle, operation, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_deferred_operation_max_concurrency_khr(&self, operation: vk::DeferredOperationKHR) -> u32 {
        let fp = self
            .fp_get_deferred_operation_max_concurrency_khr
            .expect("vkGetDeferredOperationMaxConcurrencyKHR is not loaded");
        (fp)(self.handle, operation)
    }
    pub unsafe fn get_deferred_operation_result_khr(&self, operation: vk::DeferredOperationKHR) -> Result<vk::Result> {
        let fp = self
            .fp_get_deferred_operation_result_khr
            .expect("vkGetDeferredOperationResultKHR is not loaded");
        let err = (fp)(self.handle, operation);
        match err {
            vk::Result::SUCCESS | vk::Result::NOT_READY => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn deferred_operation_join_khr(&self, operation: vk::DeferredOperationKHR) -> Result<vk::Result> {
        let fp = self
            .fp_deferred_operation_join_khr
            .expect("vkDeferredOperationJoinKHR is not loaded");
        let err = (fp)(self.handle, operation);
        match err {
            vk::Result::SUCCESS | vk::Result::THREAD_DONE_KHR | vk::Result::THREAD_IDLE_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_pipeline_indirect_memory_requirements_nv(
        &self,
        p_create_info: &vk::ComputePipelineCreateInfo,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_pipeline_indirect_memory_requirements_nv
            .expect("vkGetPipelineIndirectMemoryRequirementsNV is not loaded");
        (fp)(self.handle, p_create_info, p_memory_requirements)
    }
    pub unsafe fn get_pipeline_indirect_device_address_nv(
        &self,
        p_info: &vk::PipelineIndirectDeviceAddressInfoNV,
    ) -> vk::DeviceAddress {
        let fp = self
            .fp_get_pipeline_indirect_device_address_nv
            .expect("vkGetPipelineIndirectDeviceAddressNV is not loaded");
        (fp)(self.handle, p_info)
    }
    pub unsafe fn anti_lag_update_amd(&self, p_data: &vk::AntiLagDataAMD) {
        let fp = self.fp_anti_lag_update_amd.expect("vkAntiLagUpdateAMD is not loaded");
        (fp)(self.handle, p_data)
    }
    pub unsafe fn cmd_set_cull_mode(&self, command_buffer: vk::CommandBuffer, cull_mode: vk::CullModeFlags) {
        let fp = self.fp_cmd_set_cull_mode.expect("vkCmdSetCullMode is not loaded");
        (fp)(command_buffer, cull_mode)
    }
    pub unsafe fn cmd_set_cull_mode_ext(&self, command_buffer: vk::CommandBuffer, cull_mode: vk::CullModeFlags) {
        let fp = self.fp_cmd_set_cull_mode.expect("vkCmdSetCullModeEXT is not loaded");
        (fp)(command_buffer, cull_mode)
    }
    pub unsafe fn cmd_set_front_face(&self, command_buffer: vk::CommandBuffer, front_face: vk::FrontFace) {
        let fp = self.fp_cmd_set_front_face.expect("vkCmdSetFrontFace is not loaded");
        (fp)(command_buffer, front_face)
    }
    pub unsafe fn cmd_set_front_face_ext(&self, command_buffer: vk::CommandBuffer, front_face: vk::FrontFace) {
        let fp = self.fp_cmd_set_front_face.expect("vkCmdSetFrontFaceEXT is not loaded");
        (fp)(command_buffer, front_face)
    }
    pub unsafe fn cmd_set_primitive_topology(
        &self,
        command_buffer: vk::CommandBuffer,
        primitive_topology: vk::PrimitiveTopology,
    ) {
        let fp = self
            .fp_cmd_set_primitive_topology
            .expect("vkCmdSetPrimitiveTopology is not loaded");
        (fp)(command_buffer, primitive_topology)
    }
    pub unsafe fn cmd_set_primitive_topology_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        primitive_topology: vk::PrimitiveTopology,
    ) {
        let fp = self
            .fp_cmd_set_primitive_topology
            .expect("vkCmdSetPrimitiveTopologyEXT is not loaded");
        (fp)(command_buffer, primitive_topology)
    }
    pub unsafe fn cmd_set_viewport_with_count(&self, command_buffer: vk::CommandBuffer, p_viewports: &[vk::Viewport]) {
        let fp = self
            .fp_cmd_set_viewport_with_count
            .expect("vkCmdSetViewportWithCount is not loaded");
        let viewport_count = p_viewports.len() as u32;
        (fp)(command_buffer, viewport_count, p_viewports.as_ptr())
    }
    pub unsafe fn cmd_set_viewport_with_count_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_viewports: &[vk::Viewport],
    ) {
        let fp = self
            .fp_cmd_set_viewport_with_count
            .expect("vkCmdSetViewportWithCountEXT is not loaded");
        let viewport_count = p_viewports.len() as u32;
        (fp)(command_buffer, viewport_count, p_viewports.as_ptr())
    }
    pub unsafe fn cmd_set_scissor_with_count(&self, command_buffer: vk::CommandBuffer, p_scissors: &[vk::Rect2D]) {
        let fp = self
            .fp_cmd_set_scissor_with_count
            .expect("vkCmdSetScissorWithCount is not loaded");
        let scissor_count = p_scissors.len() as u32;
        (fp)(command_buffer, scissor_count, p_scissors.as_ptr())
    }
    pub unsafe fn cmd_set_scissor_with_count_ext(&self, command_buffer: vk::CommandBuffer, p_scissors: &[vk::Rect2D]) {
        let fp = self
            .fp_cmd_set_scissor_with_count
            .expect("vkCmdSetScissorWithCountEXT is not loaded");
        let scissor_count = p_scissors.len() as u32;
        (fp)(command_buffer, scissor_count, p_scissors.as_ptr())
    }
    pub unsafe fn cmd_bind_index_buffer2(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        size: vk::DeviceSize,
        index_type: vk::IndexType,
    ) {
        let fp = self
            .fp_cmd_bind_index_buffer2
            .expect("vkCmdBindIndexBuffer2 is not loaded");
        (fp)(command_buffer, buffer, offset, size, index_type)
    }
    pub unsafe fn cmd_bind_index_buffer2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        buffer: vk::Buffer,
        offset: vk::DeviceSize,
        size: vk::DeviceSize,
        index_type: vk::IndexType,
    ) {
        let fp = self
            .fp_cmd_bind_index_buffer2
            .expect("vkCmdBindIndexBuffer2KHR is not loaded");
        (fp)(command_buffer, buffer, offset, size, index_type)
    }
    pub unsafe fn cmd_bind_vertex_buffers2(
        &self,
        command_buffer: vk::CommandBuffer,
        first_binding: u32,
        p_buffers: &[vk::Buffer],
        p_offsets: &[vk::DeviceSize],
        p_sizes: Option<&[vk::DeviceSize]>,
        p_strides: Option<&[vk::DeviceSize]>,
    ) {
        let fp = self
            .fp_cmd_bind_vertex_buffers2
            .expect("vkCmdBindVertexBuffers2 is not loaded");
        let binding_count = p_buffers.len() as u32;
        assert_eq!(binding_count, p_offsets.len() as u32);
        if let Some(s) = p_sizes {
            assert_eq!(binding_count, s.len() as u32);
        }
        if let Some(s) = p_strides {
            assert_eq!(binding_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            first_binding,
            binding_count,
            p_buffers.as_ptr(),
            p_offsets.as_ptr(),
            p_sizes.map_or(ptr::null(), |r| r.as_ptr()),
            p_strides.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn cmd_bind_vertex_buffers2_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_binding: u32,
        p_buffers: &[vk::Buffer],
        p_offsets: &[vk::DeviceSize],
        p_sizes: Option<&[vk::DeviceSize]>,
        p_strides: Option<&[vk::DeviceSize]>,
    ) {
        let fp = self
            .fp_cmd_bind_vertex_buffers2
            .expect("vkCmdBindVertexBuffers2EXT is not loaded");
        let binding_count = p_buffers.len() as u32;
        assert_eq!(binding_count, p_offsets.len() as u32);
        if let Some(s) = p_sizes {
            assert_eq!(binding_count, s.len() as u32);
        }
        if let Some(s) = p_strides {
            assert_eq!(binding_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            first_binding,
            binding_count,
            p_buffers.as_ptr(),
            p_offsets.as_ptr(),
            p_sizes.map_or(ptr::null(), |r| r.as_ptr()),
            p_strides.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn cmd_set_depth_test_enable(&self, command_buffer: vk::CommandBuffer, depth_test_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_test_enable
            .expect("vkCmdSetDepthTestEnable is not loaded");
        (fp)(command_buffer, if depth_test_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_test_enable_ext(&self, command_buffer: vk::CommandBuffer, depth_test_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_test_enable
            .expect("vkCmdSetDepthTestEnableEXT is not loaded");
        (fp)(command_buffer, if depth_test_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_write_enable(&self, command_buffer: vk::CommandBuffer, depth_write_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_write_enable
            .expect("vkCmdSetDepthWriteEnable is not loaded");
        (fp)(command_buffer, if depth_write_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_write_enable_ext(&self, command_buffer: vk::CommandBuffer, depth_write_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_write_enable
            .expect("vkCmdSetDepthWriteEnableEXT is not loaded");
        (fp)(command_buffer, if depth_write_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_compare_op(&self, command_buffer: vk::CommandBuffer, depth_compare_op: vk::CompareOp) {
        let fp = self
            .fp_cmd_set_depth_compare_op
            .expect("vkCmdSetDepthCompareOp is not loaded");
        (fp)(command_buffer, depth_compare_op)
    }
    pub unsafe fn cmd_set_depth_compare_op_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        depth_compare_op: vk::CompareOp,
    ) {
        let fp = self
            .fp_cmd_set_depth_compare_op
            .expect("vkCmdSetDepthCompareOpEXT is not loaded");
        (fp)(command_buffer, depth_compare_op)
    }
    pub unsafe fn cmd_set_depth_bounds_test_enable(
        &self,
        command_buffer: vk::CommandBuffer,
        depth_bounds_test_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_depth_bounds_test_enable
            .expect("vkCmdSetDepthBoundsTestEnable is not loaded");
        (fp)(
            command_buffer,
            if depth_bounds_test_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_depth_bounds_test_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        depth_bounds_test_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_depth_bounds_test_enable
            .expect("vkCmdSetDepthBoundsTestEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if depth_bounds_test_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_stencil_test_enable(&self, command_buffer: vk::CommandBuffer, stencil_test_enable: bool) {
        let fp = self
            .fp_cmd_set_stencil_test_enable
            .expect("vkCmdSetStencilTestEnable is not loaded");
        (fp)(command_buffer, if stencil_test_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_stencil_test_enable_ext(&self, command_buffer: vk::CommandBuffer, stencil_test_enable: bool) {
        let fp = self
            .fp_cmd_set_stencil_test_enable
            .expect("vkCmdSetStencilTestEnableEXT is not loaded");
        (fp)(command_buffer, if stencil_test_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_stencil_op(
        &self,
        command_buffer: vk::CommandBuffer,
        face_mask: vk::StencilFaceFlags,
        fail_op: vk::StencilOp,
        pass_op: vk::StencilOp,
        depth_fail_op: vk::StencilOp,
        compare_op: vk::CompareOp,
    ) {
        let fp = self.fp_cmd_set_stencil_op.expect("vkCmdSetStencilOp is not loaded");
        (fp)(command_buffer, face_mask, fail_op, pass_op, depth_fail_op, compare_op)
    }
    pub unsafe fn cmd_set_stencil_op_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        face_mask: vk::StencilFaceFlags,
        fail_op: vk::StencilOp,
        pass_op: vk::StencilOp,
        depth_fail_op: vk::StencilOp,
        compare_op: vk::CompareOp,
    ) {
        let fp = self.fp_cmd_set_stencil_op.expect("vkCmdSetStencilOpEXT is not loaded");
        (fp)(command_buffer, face_mask, fail_op, pass_op, depth_fail_op, compare_op)
    }
    pub unsafe fn cmd_set_patch_control_points_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        patch_control_points: u32,
    ) {
        let fp = self
            .fp_cmd_set_patch_control_points_ext
            .expect("vkCmdSetPatchControlPointsEXT is not loaded");
        (fp)(command_buffer, patch_control_points)
    }
    pub unsafe fn cmd_set_rasterizer_discard_enable(
        &self,
        command_buffer: vk::CommandBuffer,
        rasterizer_discard_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_rasterizer_discard_enable
            .expect("vkCmdSetRasterizerDiscardEnable is not loaded");
        (fp)(
            command_buffer,
            if rasterizer_discard_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_rasterizer_discard_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        rasterizer_discard_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_rasterizer_discard_enable
            .expect("vkCmdSetRasterizerDiscardEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if rasterizer_discard_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_depth_bias_enable(&self, command_buffer: vk::CommandBuffer, depth_bias_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_bias_enable
            .expect("vkCmdSetDepthBiasEnable is not loaded");
        (fp)(command_buffer, if depth_bias_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_bias_enable_ext(&self, command_buffer: vk::CommandBuffer, depth_bias_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_bias_enable
            .expect("vkCmdSetDepthBiasEnableEXT is not loaded");
        (fp)(command_buffer, if depth_bias_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_logic_op_ext(&self, command_buffer: vk::CommandBuffer, logic_op: vk::LogicOp) {
        let fp = self.fp_cmd_set_logic_op_ext.expect("vkCmdSetLogicOpEXT is not loaded");
        (fp)(command_buffer, logic_op)
    }
    pub unsafe fn cmd_set_primitive_restart_enable(
        &self,
        command_buffer: vk::CommandBuffer,
        primitive_restart_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_primitive_restart_enable
            .expect("vkCmdSetPrimitiveRestartEnable is not loaded");
        (fp)(
            command_buffer,
            if primitive_restart_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_primitive_restart_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        primitive_restart_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_primitive_restart_enable
            .expect("vkCmdSetPrimitiveRestartEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if primitive_restart_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_tessellation_domain_origin_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        domain_origin: vk::TessellationDomainOrigin,
    ) {
        let fp = self
            .fp_cmd_set_tessellation_domain_origin_ext
            .expect("vkCmdSetTessellationDomainOriginEXT is not loaded");
        (fp)(command_buffer, domain_origin)
    }
    pub unsafe fn cmd_set_depth_clamp_enable_ext(&self, command_buffer: vk::CommandBuffer, depth_clamp_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_clamp_enable_ext
            .expect("vkCmdSetDepthClampEnableEXT is not loaded");
        (fp)(command_buffer, if depth_clamp_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_polygon_mode_ext(&self, command_buffer: vk::CommandBuffer, polygon_mode: vk::PolygonMode) {
        let fp = self
            .fp_cmd_set_polygon_mode_ext
            .expect("vkCmdSetPolygonModeEXT is not loaded");
        (fp)(command_buffer, polygon_mode)
    }
    pub unsafe fn cmd_set_rasterization_samples_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        rasterization_samples: vk::SampleCountFlags,
    ) {
        let fp = self
            .fp_cmd_set_rasterization_samples_ext
            .expect("vkCmdSetRasterizationSamplesEXT is not loaded");
        (fp)(command_buffer, rasterization_samples)
    }
    pub unsafe fn cmd_set_sample_mask_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        samples: vk::SampleCountFlags,
        p_sample_mask: *const vk::SampleMask,
    ) {
        let fp = self
            .fp_cmd_set_sample_mask_ext
            .expect("vkCmdSetSampleMaskEXT is not loaded");
        (fp)(command_buffer, samples, p_sample_mask)
    }
    pub unsafe fn cmd_set_alpha_to_coverage_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        alpha_to_coverage_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_alpha_to_coverage_enable_ext
            .expect("vkCmdSetAlphaToCoverageEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if alpha_to_coverage_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_alpha_to_one_enable_ext(&self, command_buffer: vk::CommandBuffer, alpha_to_one_enable: bool) {
        let fp = self
            .fp_cmd_set_alpha_to_one_enable_ext
            .expect("vkCmdSetAlphaToOneEnableEXT is not loaded");
        (fp)(command_buffer, if alpha_to_one_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_logic_op_enable_ext(&self, command_buffer: vk::CommandBuffer, logic_op_enable: bool) {
        let fp = self
            .fp_cmd_set_logic_op_enable_ext
            .expect("vkCmdSetLogicOpEnableEXT is not loaded");
        (fp)(command_buffer, if logic_op_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_color_blend_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_attachment: u32,
        p_color_blend_enables: &[vk::Bool32],
    ) {
        let fp = self
            .fp_cmd_set_color_blend_enable_ext
            .expect("vkCmdSetColorBlendEnableEXT is not loaded");
        let attachment_count = p_color_blend_enables.len() as u32;
        (fp)(
            command_buffer,
            first_attachment,
            attachment_count,
            p_color_blend_enables.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_color_blend_equation_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_attachment: u32,
        p_color_blend_equations: &[vk::ColorBlendEquationEXT],
    ) {
        let fp = self
            .fp_cmd_set_color_blend_equation_ext
            .expect("vkCmdSetColorBlendEquationEXT is not loaded");
        let attachment_count = p_color_blend_equations.len() as u32;
        (fp)(
            command_buffer,
            first_attachment,
            attachment_count,
            p_color_blend_equations.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_color_write_mask_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_attachment: u32,
        p_color_write_masks: &[vk::ColorComponentFlags],
    ) {
        let fp = self
            .fp_cmd_set_color_write_mask_ext
            .expect("vkCmdSetColorWriteMaskEXT is not loaded");
        let attachment_count = p_color_write_masks.len() as u32;
        (fp)(
            command_buffer,
            first_attachment,
            attachment_count,
            p_color_write_masks.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_rasterization_stream_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        rasterization_stream: u32,
    ) {
        let fp = self
            .fp_cmd_set_rasterization_stream_ext
            .expect("vkCmdSetRasterizationStreamEXT is not loaded");
        (fp)(command_buffer, rasterization_stream)
    }
    pub unsafe fn cmd_set_conservative_rasterization_mode_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        conservative_rasterization_mode: vk::ConservativeRasterizationModeEXT,
    ) {
        let fp = self
            .fp_cmd_set_conservative_rasterization_mode_ext
            .expect("vkCmdSetConservativeRasterizationModeEXT is not loaded");
        (fp)(command_buffer, conservative_rasterization_mode)
    }
    pub unsafe fn cmd_set_extra_primitive_overestimation_size_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        extra_primitive_overestimation_size: f32,
    ) {
        let fp = self
            .fp_cmd_set_extra_primitive_overestimation_size_ext
            .expect("vkCmdSetExtraPrimitiveOverestimationSizeEXT is not loaded");
        (fp)(command_buffer, extra_primitive_overestimation_size)
    }
    pub unsafe fn cmd_set_depth_clip_enable_ext(&self, command_buffer: vk::CommandBuffer, depth_clip_enable: bool) {
        let fp = self
            .fp_cmd_set_depth_clip_enable_ext
            .expect("vkCmdSetDepthClipEnableEXT is not loaded");
        (fp)(command_buffer, if depth_clip_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_sample_locations_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        sample_locations_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_sample_locations_enable_ext
            .expect("vkCmdSetSampleLocationsEnableEXT is not loaded");
        (fp)(
            command_buffer,
            if sample_locations_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_color_blend_advanced_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        first_attachment: u32,
        p_color_blend_advanced: &[vk::ColorBlendAdvancedEXT],
    ) {
        let fp = self
            .fp_cmd_set_color_blend_advanced_ext
            .expect("vkCmdSetColorBlendAdvancedEXT is not loaded");
        let attachment_count = p_color_blend_advanced.len() as u32;
        (fp)(
            command_buffer,
            first_attachment,
            attachment_count,
            p_color_blend_advanced.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_provoking_vertex_mode_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        provoking_vertex_mode: vk::ProvokingVertexModeEXT,
    ) {
        let fp = self
            .fp_cmd_set_provoking_vertex_mode_ext
            .expect("vkCmdSetProvokingVertexModeEXT is not loaded");
        (fp)(command_buffer, provoking_vertex_mode)
    }
    pub unsafe fn cmd_set_line_rasterization_mode_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        line_rasterization_mode: vk::LineRasterizationModeEXT,
    ) {
        let fp = self
            .fp_cmd_set_line_rasterization_mode_ext
            .expect("vkCmdSetLineRasterizationModeEXT is not loaded");
        (fp)(command_buffer, line_rasterization_mode)
    }
    pub unsafe fn cmd_set_line_stipple_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        stippled_line_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_line_stipple_enable_ext
            .expect("vkCmdSetLineStippleEnableEXT is not loaded");
        (fp)(command_buffer, if stippled_line_enable { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_depth_clip_negative_one_to_one_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        negative_one_to_one: bool,
    ) {
        let fp = self
            .fp_cmd_set_depth_clip_negative_one_to_one_ext
            .expect("vkCmdSetDepthClipNegativeOneToOneEXT is not loaded");
        (fp)(command_buffer, if negative_one_to_one { vk::TRUE } else { vk::FALSE })
    }
    pub unsafe fn cmd_set_viewport_w_scaling_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        viewport_w_scaling_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_viewport_w_scaling_enable_nv
            .expect("vkCmdSetViewportWScalingEnableNV is not loaded");
        (fp)(
            command_buffer,
            if viewport_w_scaling_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_viewport_swizzle_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        first_viewport: u32,
        p_viewport_swizzles: &[vk::ViewportSwizzleNV],
    ) {
        let fp = self
            .fp_cmd_set_viewport_swizzle_nv
            .expect("vkCmdSetViewportSwizzleNV is not loaded");
        let viewport_count = p_viewport_swizzles.len() as u32;
        (fp)(
            command_buffer,
            first_viewport,
            viewport_count,
            p_viewport_swizzles.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_coverage_to_color_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        coverage_to_color_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_coverage_to_color_enable_nv
            .expect("vkCmdSetCoverageToColorEnableNV is not loaded");
        (fp)(
            command_buffer,
            if coverage_to_color_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_coverage_to_color_location_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        coverage_to_color_location: u32,
    ) {
        let fp = self
            .fp_cmd_set_coverage_to_color_location_nv
            .expect("vkCmdSetCoverageToColorLocationNV is not loaded");
        (fp)(command_buffer, coverage_to_color_location)
    }
    pub unsafe fn cmd_set_coverage_modulation_mode_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        coverage_modulation_mode: vk::CoverageModulationModeNV,
    ) {
        let fp = self
            .fp_cmd_set_coverage_modulation_mode_nv
            .expect("vkCmdSetCoverageModulationModeNV is not loaded");
        (fp)(command_buffer, coverage_modulation_mode)
    }
    pub unsafe fn cmd_set_coverage_modulation_table_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        coverage_modulation_table_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_coverage_modulation_table_enable_nv
            .expect("vkCmdSetCoverageModulationTableEnableNV is not loaded");
        (fp)(
            command_buffer,
            if coverage_modulation_table_enable {
                vk::TRUE
            } else {
                vk::FALSE
            },
        )
    }
    pub unsafe fn cmd_set_coverage_modulation_table_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_coverage_modulation_table: &[f32],
    ) {
        let fp = self
            .fp_cmd_set_coverage_modulation_table_nv
            .expect("vkCmdSetCoverageModulationTableNV is not loaded");
        let coverage_modulation_table_count = p_coverage_modulation_table.len() as u32;
        (fp)(
            command_buffer,
            coverage_modulation_table_count,
            p_coverage_modulation_table.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_shading_rate_image_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        shading_rate_image_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_shading_rate_image_enable_nv
            .expect("vkCmdSetShadingRateImageEnableNV is not loaded");
        (fp)(
            command_buffer,
            if shading_rate_image_enable { vk::TRUE } else { vk::FALSE },
        )
    }
    pub unsafe fn cmd_set_coverage_reduction_mode_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        coverage_reduction_mode: vk::CoverageReductionModeNV,
    ) {
        let fp = self
            .fp_cmd_set_coverage_reduction_mode_nv
            .expect("vkCmdSetCoverageReductionModeNV is not loaded");
        (fp)(command_buffer, coverage_reduction_mode)
    }
    pub unsafe fn cmd_set_representative_fragment_test_enable_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        representative_fragment_test_enable: bool,
    ) {
        let fp = self
            .fp_cmd_set_representative_fragment_test_enable_nv
            .expect("vkCmdSetRepresentativeFragmentTestEnableNV is not loaded");
        (fp)(
            command_buffer,
            if representative_fragment_test_enable {
                vk::TRUE
            } else {
                vk::FALSE
            },
        )
    }
    pub unsafe fn create_private_data_slot(
        &self,
        p_create_info: &vk::PrivateDataSlotCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::PrivateDataSlot> {
        let fp = self
            .fp_create_private_data_slot
            .expect("vkCreatePrivateDataSlot is not loaded");
        let mut p_private_data_slot = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_private_data_slot.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_private_data_slot.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_private_data_slot_ext(
        &self,
        p_create_info: &vk::PrivateDataSlotCreateInfo,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::PrivateDataSlot> {
        let fp = self
            .fp_create_private_data_slot
            .expect("vkCreatePrivateDataSlotEXT is not loaded");
        let mut p_private_data_slot = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_private_data_slot.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_private_data_slot.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_private_data_slot(
        &self,
        private_data_slot: vk::PrivateDataSlot,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_private_data_slot
            .expect("vkDestroyPrivateDataSlot is not loaded");
        (fp)(self.handle, private_data_slot, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn destroy_private_data_slot_ext(
        &self,
        private_data_slot: vk::PrivateDataSlot,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_private_data_slot
            .expect("vkDestroyPrivateDataSlotEXT is not loaded");
        (fp)(self.handle, private_data_slot, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn set_private_data(
        &self,
        object_type: vk::ObjectType,
        object_handle: u64,
        private_data_slot: vk::PrivateDataSlot,
        data: u64,
    ) -> Result<()> {
        let fp = self.fp_set_private_data.expect("vkSetPrivateData is not loaded");
        let err = (fp)(self.handle, object_type, object_handle, private_data_slot, data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_private_data_ext(
        &self,
        object_type: vk::ObjectType,
        object_handle: u64,
        private_data_slot: vk::PrivateDataSlot,
        data: u64,
    ) -> Result<()> {
        let fp = self.fp_set_private_data.expect("vkSetPrivateDataEXT is not loaded");
        let err = (fp)(self.handle, object_type, object_handle, private_data_slot, data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_private_data(
        &self,
        object_type: vk::ObjectType,
        object_handle: u64,
        private_data_slot: vk::PrivateDataSlot,
    ) -> u64 {
        let fp = self.fp_get_private_data.expect("vkGetPrivateData is not loaded");
        let mut p_data = MaybeUninit::<_>::uninit();
        (fp)(
            self.handle,
            object_type,
            object_handle,
            private_data_slot,
            p_data.as_mut_ptr(),
        );
        p_data.assume_init()
    }
    pub unsafe fn get_private_data_ext(
        &self,
        object_type: vk::ObjectType,
        object_handle: u64,
        private_data_slot: vk::PrivateDataSlot,
    ) -> u64 {
        let fp = self.fp_get_private_data.expect("vkGetPrivateDataEXT is not loaded");
        let mut p_data = MaybeUninit::<_>::uninit();
        (fp)(
            self.handle,
            object_type,
            object_handle,
            private_data_slot,
            p_data.as_mut_ptr(),
        );
        p_data.assume_init()
    }
    pub unsafe fn cmd_copy_buffer2(&self, command_buffer: vk::CommandBuffer, p_copy_buffer_info: &vk::CopyBufferInfo2) {
        let fp = self.fp_cmd_copy_buffer2.expect("vkCmdCopyBuffer2 is not loaded");
        (fp)(command_buffer, p_copy_buffer_info)
    }
    pub unsafe fn cmd_copy_buffer2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_buffer_info: &vk::CopyBufferInfo2,
    ) {
        let fp = self.fp_cmd_copy_buffer2.expect("vkCmdCopyBuffer2KHR is not loaded");
        (fp)(command_buffer, p_copy_buffer_info)
    }
    pub unsafe fn cmd_copy_image2(&self, command_buffer: vk::CommandBuffer, p_copy_image_info: &vk::CopyImageInfo2) {
        let fp = self.fp_cmd_copy_image2.expect("vkCmdCopyImage2 is not loaded");
        (fp)(command_buffer, p_copy_image_info)
    }
    pub unsafe fn cmd_copy_image2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_image_info: &vk::CopyImageInfo2,
    ) {
        let fp = self.fp_cmd_copy_image2.expect("vkCmdCopyImage2KHR is not loaded");
        (fp)(command_buffer, p_copy_image_info)
    }
    pub unsafe fn cmd_blit_image2(&self, command_buffer: vk::CommandBuffer, p_blit_image_info: &vk::BlitImageInfo2) {
        let fp = self.fp_cmd_blit_image2.expect("vkCmdBlitImage2 is not loaded");
        (fp)(command_buffer, p_blit_image_info)
    }
    pub unsafe fn cmd_blit_image2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_blit_image_info: &vk::BlitImageInfo2,
    ) {
        let fp = self.fp_cmd_blit_image2.expect("vkCmdBlitImage2KHR is not loaded");
        (fp)(command_buffer, p_blit_image_info)
    }
    pub unsafe fn cmd_copy_buffer_to_image2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_buffer_to_image_info: &vk::CopyBufferToImageInfo2,
    ) {
        let fp = self
            .fp_cmd_copy_buffer_to_image2
            .expect("vkCmdCopyBufferToImage2 is not loaded");
        (fp)(command_buffer, p_copy_buffer_to_image_info)
    }
    pub unsafe fn cmd_copy_buffer_to_image2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_buffer_to_image_info: &vk::CopyBufferToImageInfo2,
    ) {
        let fp = self
            .fp_cmd_copy_buffer_to_image2
            .expect("vkCmdCopyBufferToImage2KHR is not loaded");
        (fp)(command_buffer, p_copy_buffer_to_image_info)
    }
    pub unsafe fn cmd_copy_image_to_buffer2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_image_to_buffer_info: &vk::CopyImageToBufferInfo2,
    ) {
        let fp = self
            .fp_cmd_copy_image_to_buffer2
            .expect("vkCmdCopyImageToBuffer2 is not loaded");
        (fp)(command_buffer, p_copy_image_to_buffer_info)
    }
    pub unsafe fn cmd_copy_image_to_buffer2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_image_to_buffer_info: &vk::CopyImageToBufferInfo2,
    ) {
        let fp = self
            .fp_cmd_copy_image_to_buffer2
            .expect("vkCmdCopyImageToBuffer2KHR is not loaded");
        (fp)(command_buffer, p_copy_image_to_buffer_info)
    }
    pub unsafe fn cmd_resolve_image2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_resolve_image_info: &vk::ResolveImageInfo2,
    ) {
        let fp = self.fp_cmd_resolve_image2.expect("vkCmdResolveImage2 is not loaded");
        (fp)(command_buffer, p_resolve_image_info)
    }
    pub unsafe fn cmd_resolve_image2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_resolve_image_info: &vk::ResolveImageInfo2,
    ) {
        let fp = self.fp_cmd_resolve_image2.expect("vkCmdResolveImage2KHR is not loaded");
        (fp)(command_buffer, p_resolve_image_info)
    }
    pub unsafe fn cmd_set_fragment_shading_rate_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_fragment_size: &vk::Extent2D,
        combiner_ops: *const vk::FragmentShadingRateCombinerOpKHR,
    ) {
        let fp = self
            .fp_cmd_set_fragment_shading_rate_khr
            .expect("vkCmdSetFragmentShadingRateKHR is not loaded");
        (fp)(command_buffer, p_fragment_size, combiner_ops)
    }
    pub unsafe fn get_physical_device_fragment_shading_rates_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_fragment_shading_rate_count: &mut u32,
        p_fragment_shading_rates: *mut vk::PhysicalDeviceFragmentShadingRateKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_fragment_shading_rates_khr
            .expect("vkGetPhysicalDeviceFragmentShadingRatesKHR is not loaded");
        let err = (fp)(physical_device, p_fragment_shading_rate_count, p_fragment_shading_rates);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_fragment_shading_rates_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::PhysicalDeviceFragmentShadingRateKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_fragment_shading_rates_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn cmd_set_fragment_shading_rate_enum_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        shading_rate: vk::FragmentShadingRateNV,
        combiner_ops: *const vk::FragmentShadingRateCombinerOpKHR,
    ) {
        let fp = self
            .fp_cmd_set_fragment_shading_rate_enum_nv
            .expect("vkCmdSetFragmentShadingRateEnumNV is not loaded");
        (fp)(command_buffer, shading_rate, combiner_ops)
    }
    pub unsafe fn get_acceleration_structure_build_sizes_khr(
        &self,
        build_type: vk::AccelerationStructureBuildTypeKHR,
        p_build_info: &vk::AccelerationStructureBuildGeometryInfoKHR,
        p_max_primitive_counts: Option<&[u32]>,
        p_size_info: &mut vk::AccelerationStructureBuildSizesInfoKHR,
    ) {
        let fp = self
            .fp_get_acceleration_structure_build_sizes_khr
            .expect("vkGetAccelerationStructureBuildSizesKHR is not loaded");
        if let Some(s) = p_max_primitive_counts {
            assert_eq!(p_build_info.geometry_count as usize, s.len());
        }
        (fp)(
            self.handle,
            build_type,
            p_build_info,
            p_max_primitive_counts.map_or(ptr::null(), |r| r.as_ptr()),
            p_size_info,
        )
    }
    pub unsafe fn cmd_set_vertex_input_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_vertex_binding_descriptions: &[vk::VertexInputBindingDescription2EXT],
        p_vertex_attribute_descriptions: &[vk::VertexInputAttributeDescription2EXT],
    ) {
        let fp = self
            .fp_cmd_set_vertex_input_ext
            .expect("vkCmdSetVertexInputEXT is not loaded");
        let vertex_binding_description_count = p_vertex_binding_descriptions.len() as u32;
        let vertex_attribute_description_count = p_vertex_attribute_descriptions.len() as u32;
        (fp)(
            command_buffer,
            vertex_binding_description_count,
            p_vertex_binding_descriptions.as_ptr(),
            vertex_attribute_description_count,
            p_vertex_attribute_descriptions.as_ptr(),
        )
    }
    pub unsafe fn cmd_set_color_write_enable_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_color_write_enables: &[vk::Bool32],
    ) {
        let fp = self
            .fp_cmd_set_color_write_enable_ext
            .expect("vkCmdSetColorWriteEnableEXT is not loaded");
        let attachment_count = p_color_write_enables.len() as u32;
        (fp)(command_buffer, attachment_count, p_color_write_enables.as_ptr())
    }
    pub unsafe fn cmd_set_event2(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        p_dependency_info: &vk::DependencyInfo,
    ) {
        let fp = self.fp_cmd_set_event2.expect("vkCmdSetEvent2 is not loaded");
        (fp)(command_buffer, event, p_dependency_info)
    }
    pub unsafe fn cmd_set_event2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        p_dependency_info: &vk::DependencyInfo,
    ) {
        let fp = self.fp_cmd_set_event2.expect("vkCmdSetEvent2KHR is not loaded");
        (fp)(command_buffer, event, p_dependency_info)
    }
    pub unsafe fn cmd_reset_event2(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        stage_mask: vk::PipelineStageFlags2,
    ) {
        let fp = self.fp_cmd_reset_event2.expect("vkCmdResetEvent2 is not loaded");
        (fp)(command_buffer, event, stage_mask)
    }
    pub unsafe fn cmd_reset_event2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        event: vk::Event,
        stage_mask: vk::PipelineStageFlags2,
    ) {
        let fp = self.fp_cmd_reset_event2.expect("vkCmdResetEvent2KHR is not loaded");
        (fp)(command_buffer, event, stage_mask)
    }
    pub unsafe fn cmd_wait_events2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_events: &[vk::Event],
        p_dependency_infos: &[vk::DependencyInfo],
    ) {
        let fp = self.fp_cmd_wait_events2.expect("vkCmdWaitEvents2 is not loaded");
        let event_count = p_events.len() as u32;
        assert_eq!(event_count, p_dependency_infos.len() as u32);
        (fp)(
            command_buffer,
            event_count,
            p_events.as_ptr(),
            p_dependency_infos.as_ptr(),
        )
    }
    pub unsafe fn cmd_wait_events2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_events: &[vk::Event],
        p_dependency_infos: &[vk::DependencyInfo],
    ) {
        let fp = self.fp_cmd_wait_events2.expect("vkCmdWaitEvents2KHR is not loaded");
        let event_count = p_events.len() as u32;
        assert_eq!(event_count, p_dependency_infos.len() as u32);
        (fp)(
            command_buffer,
            event_count,
            p_events.as_ptr(),
            p_dependency_infos.as_ptr(),
        )
    }
    pub unsafe fn cmd_pipeline_barrier2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_dependency_info: &vk::DependencyInfo,
    ) {
        let fp = self
            .fp_cmd_pipeline_barrier2
            .expect("vkCmdPipelineBarrier2 is not loaded");
        (fp)(command_buffer, p_dependency_info)
    }
    pub unsafe fn cmd_pipeline_barrier2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_dependency_info: &vk::DependencyInfo,
    ) {
        let fp = self
            .fp_cmd_pipeline_barrier2
            .expect("vkCmdPipelineBarrier2KHR is not loaded");
        (fp)(command_buffer, p_dependency_info)
    }
    pub unsafe fn queue_submit2(
        &self,
        queue: vk::Queue,
        p_submits: &[vk::SubmitInfo2],
        fence: vk::Fence,
    ) -> Result<()> {
        let fp = self.fp_queue_submit2.expect("vkQueueSubmit2 is not loaded");
        let submit_count = p_submits.len() as u32;
        let err = (fp)(queue, submit_count, p_submits.as_ptr(), fence);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn queue_submit2_khr(
        &self,
        queue: vk::Queue,
        p_submits: &[vk::SubmitInfo2],
        fence: vk::Fence,
    ) -> Result<()> {
        let fp = self.fp_queue_submit2.expect("vkQueueSubmit2KHR is not loaded");
        let submit_count = p_submits.len() as u32;
        let err = (fp)(queue, submit_count, p_submits.as_ptr(), fence);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_write_timestamp2(
        &self,
        command_buffer: vk::CommandBuffer,
        stage: vk::PipelineStageFlags2,
        query_pool: vk::QueryPool,
        query: u32,
    ) {
        let fp = self
            .fp_cmd_write_timestamp2
            .expect("vkCmdWriteTimestamp2 is not loaded");
        (fp)(command_buffer, stage, query_pool, query)
    }
    pub unsafe fn cmd_write_timestamp2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        stage: vk::PipelineStageFlags2,
        query_pool: vk::QueryPool,
        query: u32,
    ) {
        let fp = self
            .fp_cmd_write_timestamp2
            .expect("vkCmdWriteTimestamp2KHR is not loaded");
        (fp)(command_buffer, stage, query_pool, query)
    }
    pub unsafe fn cmd_write_buffer_marker2_amd(
        &self,
        command_buffer: vk::CommandBuffer,
        stage: vk::PipelineStageFlags2,
        dst_buffer: vk::Buffer,
        dst_offset: vk::DeviceSize,
        marker: u32,
    ) {
        let fp = self
            .fp_cmd_write_buffer_marker2_amd
            .expect("vkCmdWriteBufferMarker2AMD is not loaded");
        (fp)(command_buffer, stage, dst_buffer, dst_offset, marker)
    }
    pub unsafe fn get_queue_checkpoint_data2_nv(
        &self,
        queue: vk::Queue,
        p_checkpoint_data_count: &mut u32,
        p_checkpoint_data: *mut vk::CheckpointData2NV,
    ) {
        let fp = self
            .fp_get_queue_checkpoint_data2_nv
            .expect("vkGetQueueCheckpointData2NV is not loaded");
        (fp)(queue, p_checkpoint_data_count, p_checkpoint_data);
    }
    pub unsafe fn get_queue_checkpoint_data2_nv_to_vec(&self, queue: vk::Queue) -> Vec<vk::CheckpointData2NV> {
        enumerate_generic_unchecked_to_vec(|len, ptr| self.get_queue_checkpoint_data2_nv(queue, len, ptr))
    }
    pub unsafe fn copy_memory_to_image(&self, p_copy_memory_to_image_info: &vk::CopyMemoryToImageInfo) -> Result<()> {
        let fp = self.fp_copy_memory_to_image.expect("vkCopyMemoryToImage is not loaded");
        let err = (fp)(self.handle, p_copy_memory_to_image_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn copy_memory_to_image_ext(
        &self,
        p_copy_memory_to_image_info: &vk::CopyMemoryToImageInfo,
    ) -> Result<()> {
        let fp = self
            .fp_copy_memory_to_image
            .expect("vkCopyMemoryToImageEXT is not loaded");
        let err = (fp)(self.handle, p_copy_memory_to_image_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn copy_image_to_memory(&self, p_copy_image_to_memory_info: &vk::CopyImageToMemoryInfo) -> Result<()> {
        let fp = self.fp_copy_image_to_memory.expect("vkCopyImageToMemory is not loaded");
        let err = (fp)(self.handle, p_copy_image_to_memory_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn copy_image_to_memory_ext(
        &self,
        p_copy_image_to_memory_info: &vk::CopyImageToMemoryInfo,
    ) -> Result<()> {
        let fp = self
            .fp_copy_image_to_memory
            .expect("vkCopyImageToMemoryEXT is not loaded");
        let err = (fp)(self.handle, p_copy_image_to_memory_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn copy_image_to_image(&self, p_copy_image_to_image_info: &vk::CopyImageToImageInfo) -> Result<()> {
        let fp = self.fp_copy_image_to_image.expect("vkCopyImageToImage is not loaded");
        let err = (fp)(self.handle, p_copy_image_to_image_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn copy_image_to_image_ext(&self, p_copy_image_to_image_info: &vk::CopyImageToImageInfo) -> Result<()> {
        let fp = self
            .fp_copy_image_to_image
            .expect("vkCopyImageToImageEXT is not loaded");
        let err = (fp)(self.handle, p_copy_image_to_image_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn transition_image_layout(&self, p_transitions: &[vk::HostImageLayoutTransitionInfo]) -> Result<()> {
        let fp = self
            .fp_transition_image_layout
            .expect("vkTransitionImageLayout is not loaded");
        let transition_count = p_transitions.len() as u32;
        let err = (fp)(self.handle, transition_count, p_transitions.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn transition_image_layout_ext(
        &self,
        p_transitions: &[vk::HostImageLayoutTransitionInfo],
    ) -> Result<()> {
        let fp = self
            .fp_transition_image_layout
            .expect("vkTransitionImageLayoutEXT is not loaded");
        let transition_count = p_transitions.len() as u32;
        let err = (fp)(self.handle, transition_count, p_transitions.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_decompress_memory_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_decompress_memory_regions: &[vk::DecompressMemoryRegionNV],
    ) {
        let fp = self
            .fp_cmd_decompress_memory_nv
            .expect("vkCmdDecompressMemoryNV is not loaded");
        let decompress_region_count = p_decompress_memory_regions.len() as u32;
        (fp)(
            command_buffer,
            decompress_region_count,
            p_decompress_memory_regions.as_ptr(),
        )
    }
    pub unsafe fn cmd_decompress_memory_indirect_count_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        indirect_commands_address: vk::DeviceAddress,
        indirect_commands_count_address: vk::DeviceAddress,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_decompress_memory_indirect_count_nv
            .expect("vkCmdDecompressMemoryIndirectCountNV is not loaded");
        (fp)(
            command_buffer,
            indirect_commands_address,
            indirect_commands_count_address,
            stride,
        )
    }
    pub unsafe fn get_partitioned_acceleration_structures_build_sizes_nv(
        &self,
        p_info: &vk::PartitionedAccelerationStructureInstancesInputNV,
        p_size_info: &mut vk::AccelerationStructureBuildSizesInfoKHR,
    ) {
        let fp = self
            .fp_get_partitioned_acceleration_structures_build_sizes_nv
            .expect("vkGetPartitionedAccelerationStructuresBuildSizesNV is not loaded");
        (fp)(self.handle, p_info, p_size_info)
    }
    pub unsafe fn cmd_build_partitioned_acceleration_structures_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_build_info: &vk::BuildPartitionedAccelerationStructureInfoNV,
    ) {
        let fp = self
            .fp_cmd_build_partitioned_acceleration_structures_nv
            .expect("vkCmdBuildPartitionedAccelerationStructuresNV is not loaded");
        (fp)(command_buffer, p_build_info)
    }
    pub unsafe fn cmd_decompress_memory_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_decompress_memory_info_ext: &vk::DecompressMemoryInfoEXT,
    ) {
        let fp = self
            .fp_cmd_decompress_memory_ext
            .expect("vkCmdDecompressMemoryEXT is not loaded");
        (fp)(command_buffer, p_decompress_memory_info_ext)
    }
    pub unsafe fn cmd_decompress_memory_indirect_count_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        decompression_method: vk::MemoryDecompressionMethodFlagsEXT,
        indirect_commands_address: vk::DeviceAddress,
        indirect_commands_count_address: vk::DeviceAddress,
        max_decompression_count: u32,
        stride: u32,
    ) {
        let fp = self
            .fp_cmd_decompress_memory_indirect_count_ext
            .expect("vkCmdDecompressMemoryIndirectCountEXT is not loaded");
        (fp)(
            command_buffer,
            decompression_method,
            indirect_commands_address,
            indirect_commands_count_address,
            max_decompression_count,
            stride,
        )
    }
    pub unsafe fn create_cu_module_nvx(
        &self,
        p_create_info: &vk::CuModuleCreateInfoNVX,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::CuModuleNVX> {
        let fp = self.fp_create_cu_module_nvx.expect("vkCreateCuModuleNVX is not loaded");
        let mut p_module = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_module.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_module.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_cu_function_nvx(
        &self,
        p_create_info: &vk::CuFunctionCreateInfoNVX,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::CuFunctionNVX> {
        let fp = self
            .fp_create_cu_function_nvx
            .expect("vkCreateCuFunctionNVX is not loaded");
        let mut p_function = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_function.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_function.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_cu_module_nvx(&self, module: vk::CuModuleNVX, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self
            .fp_destroy_cu_module_nvx
            .expect("vkDestroyCuModuleNVX is not loaded");
        (fp)(self.handle, module, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn destroy_cu_function_nvx(
        &self,
        function: vk::CuFunctionNVX,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_cu_function_nvx
            .expect("vkDestroyCuFunctionNVX is not loaded");
        (fp)(self.handle, function, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_cu_launch_kernel_nvx(
        &self,
        command_buffer: vk::CommandBuffer,
        p_launch_info: &vk::CuLaunchInfoNVX,
    ) {
        let fp = self
            .fp_cmd_cu_launch_kernel_nvx
            .expect("vkCmdCuLaunchKernelNVX is not loaded");
        (fp)(command_buffer, p_launch_info)
    }
    pub unsafe fn get_descriptor_set_layout_size_ext(&self, layout: vk::DescriptorSetLayout) -> vk::DeviceSize {
        let fp = self
            .fp_get_descriptor_set_layout_size_ext
            .expect("vkGetDescriptorSetLayoutSizeEXT is not loaded");
        let mut p_layout_size_in_bytes = MaybeUninit::<_>::uninit();
        (fp)(self.handle, layout, p_layout_size_in_bytes.as_mut_ptr());
        p_layout_size_in_bytes.assume_init()
    }
    pub unsafe fn get_descriptor_set_layout_binding_offset_ext(
        &self,
        layout: vk::DescriptorSetLayout,
        binding: u32,
    ) -> vk::DeviceSize {
        let fp = self
            .fp_get_descriptor_set_layout_binding_offset_ext
            .expect("vkGetDescriptorSetLayoutBindingOffsetEXT is not loaded");
        let mut p_offset = MaybeUninit::<_>::uninit();
        (fp)(self.handle, layout, binding, p_offset.as_mut_ptr());
        p_offset.assume_init()
    }
    pub unsafe fn get_descriptor_ext(&self, p_descriptor_info: &vk::DescriptorGetInfoEXT, p_descriptor: &mut [u8]) {
        let fp = self.fp_get_descriptor_ext.expect("vkGetDescriptorEXT is not loaded");
        let data_size = p_descriptor.len();
        (fp)(
            self.handle,
            p_descriptor_info,
            data_size,
            p_descriptor.as_mut_ptr() as *mut _,
        )
    }
    pub unsafe fn cmd_bind_descriptor_buffers_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_binding_infos: &[vk::DescriptorBufferBindingInfoEXT],
    ) {
        let fp = self
            .fp_cmd_bind_descriptor_buffers_ext
            .expect("vkCmdBindDescriptorBuffersEXT is not loaded");
        let buffer_count = p_binding_infos.len() as u32;
        (fp)(command_buffer, buffer_count, p_binding_infos.as_ptr())
    }
    pub unsafe fn cmd_set_descriptor_buffer_offsets_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        layout: vk::PipelineLayout,
        first_set: u32,
        p_buffer_indices: &[u32],
        p_offsets: &[vk::DeviceSize],
    ) {
        let fp = self
            .fp_cmd_set_descriptor_buffer_offsets_ext
            .expect("vkCmdSetDescriptorBufferOffsetsEXT is not loaded");
        let set_count = p_buffer_indices.len() as u32;
        assert_eq!(set_count, p_offsets.len() as u32);
        (fp)(
            command_buffer,
            pipeline_bind_point,
            layout,
            first_set,
            set_count,
            p_buffer_indices.as_ptr(),
            p_offsets.as_ptr(),
        )
    }
    pub unsafe fn cmd_bind_descriptor_buffer_embedded_samplers_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        pipeline_bind_point: vk::PipelineBindPoint,
        layout: vk::PipelineLayout,
        set: u32,
    ) {
        let fp = self
            .fp_cmd_bind_descriptor_buffer_embedded_samplers_ext
            .expect("vkCmdBindDescriptorBufferEmbeddedSamplersEXT is not loaded");
        (fp)(command_buffer, pipeline_bind_point, layout, set)
    }
    pub unsafe fn get_buffer_opaque_capture_descriptor_data_ext(
        &self,
        p_info: &vk::BufferCaptureDescriptorDataInfoEXT,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_buffer_opaque_capture_descriptor_data_ext
            .expect("vkGetBufferOpaqueCaptureDescriptorDataEXT is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_image_opaque_capture_descriptor_data_ext(
        &self,
        p_info: &vk::ImageCaptureDescriptorDataInfoEXT,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_image_opaque_capture_descriptor_data_ext
            .expect("vkGetImageOpaqueCaptureDescriptorDataEXT is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_image_view_opaque_capture_descriptor_data_ext(
        &self,
        p_info: &vk::ImageViewCaptureDescriptorDataInfoEXT,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_image_view_opaque_capture_descriptor_data_ext
            .expect("vkGetImageViewOpaqueCaptureDescriptorDataEXT is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_sampler_opaque_capture_descriptor_data_ext(
        &self,
        p_info: &vk::SamplerCaptureDescriptorDataInfoEXT,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_sampler_opaque_capture_descriptor_data_ext
            .expect("vkGetSamplerOpaqueCaptureDescriptorDataEXT is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_acceleration_structure_opaque_capture_descriptor_data_ext(
        &self,
        p_info: &vk::AccelerationStructureCaptureDescriptorDataInfoEXT,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_acceleration_structure_opaque_capture_descriptor_data_ext
            .expect("vkGetAccelerationStructureOpaqueCaptureDescriptorDataEXT is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_device_memory_priority_ext(&self, memory: vk::DeviceMemory, priority: f32) {
        let fp = self
            .fp_set_device_memory_priority_ext
            .expect("vkSetDeviceMemoryPriorityEXT is not loaded");
        (fp)(self.handle, memory, priority)
    }
    pub unsafe fn wait_for_present2_khr(
        &self,
        swapchain: vk::SwapchainKHR,
        p_present_wait2_info: &vk::PresentWait2InfoKHR,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_wait_for_present2_khr
            .expect("vkWaitForPresent2KHR is not loaded");
        let err = (fp)(self.handle, swapchain, p_present_wait2_info);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn wait_for_present_khr(
        &self,
        swapchain: vk::SwapchainKHR,
        present_id: u64,
        timeout: u64,
    ) -> Result<vk::Result> {
        let fp = self.fp_wait_for_present_khr.expect("vkWaitForPresentKHR is not loaded");
        let err = (fp)(self.handle, swapchain, present_id, timeout);
        match err {
            vk::Result::SUCCESS | vk::Result::TIMEOUT | vk::Result::SUBOPTIMAL_KHR => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_buffer_collection_fuchsia(
        &self,
        p_create_info: &vk::BufferCollectionCreateInfoFUCHSIA,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::BufferCollectionFUCHSIA> {
        let fp = self
            .fp_create_buffer_collection_fuchsia
            .expect("vkCreateBufferCollectionFUCHSIA is not loaded");
        let mut p_collection = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_collection.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_collection.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_buffer_collection_buffer_constraints_fuchsia(
        &self,
        collection: vk::BufferCollectionFUCHSIA,
        p_buffer_constraints_info: &vk::BufferConstraintsInfoFUCHSIA,
    ) -> Result<()> {
        let fp = self
            .fp_set_buffer_collection_buffer_constraints_fuchsia
            .expect("vkSetBufferCollectionBufferConstraintsFUCHSIA is not loaded");
        let err = (fp)(self.handle, collection, p_buffer_constraints_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_buffer_collection_image_constraints_fuchsia(
        &self,
        collection: vk::BufferCollectionFUCHSIA,
        p_image_constraints_info: &vk::ImageConstraintsInfoFUCHSIA,
    ) -> Result<()> {
        let fp = self
            .fp_set_buffer_collection_image_constraints_fuchsia
            .expect("vkSetBufferCollectionImageConstraintsFUCHSIA is not loaded");
        let err = (fp)(self.handle, collection, p_image_constraints_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_buffer_collection_fuchsia(
        &self,
        collection: vk::BufferCollectionFUCHSIA,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_buffer_collection_fuchsia
            .expect("vkDestroyBufferCollectionFUCHSIA is not loaded");
        (fp)(self.handle, collection, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_buffer_collection_properties_fuchsia(
        &self,
        collection: vk::BufferCollectionFUCHSIA,
        p_properties: &mut vk::BufferCollectionPropertiesFUCHSIA,
    ) -> Result<()> {
        let fp = self
            .fp_get_buffer_collection_properties_fuchsia
            .expect("vkGetBufferCollectionPropertiesFUCHSIA is not loaded");
        let err = (fp)(self.handle, collection, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_cuda_module_nv(
        &self,
        p_create_info: &vk::CudaModuleCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::CudaModuleNV> {
        let fp = self
            .fp_create_cuda_module_nv
            .expect("vkCreateCudaModuleNV is not loaded");
        let mut p_module = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_module.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_module.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_cuda_module_cache_nv(
        &self,
        module: vk::CudaModuleNV,
        p_cache_size: &mut usize,
        p_cache_data: *mut c_void,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_cuda_module_cache_nv
            .expect("vkGetCudaModuleCacheNV is not loaded");
        let err = (fp)(self.handle, module, p_cache_size, p_cache_data);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_cuda_function_nv(
        &self,
        p_create_info: &vk::CudaFunctionCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::CudaFunctionNV> {
        let fp = self
            .fp_create_cuda_function_nv
            .expect("vkCreateCudaFunctionNV is not loaded");
        let mut p_function = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_function.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_function.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_cuda_module_nv(
        &self,
        module: vk::CudaModuleNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_cuda_module_nv
            .expect("vkDestroyCudaModuleNV is not loaded");
        (fp)(self.handle, module, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn destroy_cuda_function_nv(
        &self,
        function: vk::CudaFunctionNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_cuda_function_nv
            .expect("vkDestroyCudaFunctionNV is not loaded");
        (fp)(self.handle, function, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_cuda_launch_kernel_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_launch_info: &vk::CudaLaunchInfoNV,
    ) {
        let fp = self
            .fp_cmd_cuda_launch_kernel_nv
            .expect("vkCmdCudaLaunchKernelNV is not loaded");
        (fp)(command_buffer, p_launch_info)
    }
    pub unsafe fn cmd_begin_rendering(&self, command_buffer: vk::CommandBuffer, p_rendering_info: &vk::RenderingInfo) {
        let fp = self.fp_cmd_begin_rendering.expect("vkCmdBeginRendering is not loaded");
        (fp)(command_buffer, p_rendering_info)
    }
    pub unsafe fn cmd_begin_rendering_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_rendering_info: &vk::RenderingInfo,
    ) {
        let fp = self
            .fp_cmd_begin_rendering
            .expect("vkCmdBeginRenderingKHR is not loaded");
        (fp)(command_buffer, p_rendering_info)
    }
    pub unsafe fn cmd_end_rendering(&self, command_buffer: vk::CommandBuffer) {
        let fp = self.fp_cmd_end_rendering.expect("vkCmdEndRendering is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn cmd_end_rendering2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_rendering_end_info: Option<&vk::RenderingEndInfoKHR>,
    ) {
        let fp = self
            .fp_cmd_end_rendering2_khr
            .expect("vkCmdEndRendering2KHR is not loaded");
        (fp)(command_buffer, p_rendering_end_info.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_end_rendering2_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_rendering_end_info: Option<&vk::RenderingEndInfoKHR>,
    ) {
        let fp = self
            .fp_cmd_end_rendering2_khr
            .expect("vkCmdEndRendering2EXT is not loaded");
        (fp)(command_buffer, p_rendering_end_info.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_end_rendering_khr(&self, command_buffer: vk::CommandBuffer) {
        let fp = self.fp_cmd_end_rendering.expect("vkCmdEndRenderingKHR is not loaded");
        (fp)(command_buffer)
    }
    pub unsafe fn get_descriptor_set_layout_host_mapping_info_valve(
        &self,
        p_binding_reference: &vk::DescriptorSetBindingReferenceVALVE,
        p_host_mapping: &mut vk::DescriptorSetLayoutHostMappingInfoVALVE,
    ) {
        let fp = self
            .fp_get_descriptor_set_layout_host_mapping_info_valve
            .expect("vkGetDescriptorSetLayoutHostMappingInfoVALVE is not loaded");
        (fp)(self.handle, p_binding_reference, p_host_mapping)
    }
    pub unsafe fn get_descriptor_set_host_mapping_valve(&self, descriptor_set: vk::DescriptorSet) -> *mut c_void {
        let fp = self
            .fp_get_descriptor_set_host_mapping_valve
            .expect("vkGetDescriptorSetHostMappingVALVE is not loaded");
        let mut pp_data = MaybeUninit::<_>::uninit();
        (fp)(self.handle, descriptor_set, pp_data.as_mut_ptr());
        pp_data.assume_init()
    }
    pub unsafe fn create_micromap_ext(
        &self,
        p_create_info: &vk::MicromapCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::MicromapEXT> {
        let fp = self.fp_create_micromap_ext.expect("vkCreateMicromapEXT is not loaded");
        let mut p_micromap = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_micromap.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_micromap.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_build_micromaps_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_infos: &[vk::MicromapBuildInfoEXT],
    ) {
        let fp = self
            .fp_cmd_build_micromaps_ext
            .expect("vkCmdBuildMicromapsEXT is not loaded");
        let info_count = p_infos.len() as u32;
        (fp)(command_buffer, info_count, p_infos.as_ptr())
    }
    pub unsafe fn build_micromaps_ext(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_infos: &[vk::MicromapBuildInfoEXT],
    ) -> Result<vk::Result> {
        let fp = self.fp_build_micromaps_ext.expect("vkBuildMicromapsEXT is not loaded");
        let info_count = p_infos.len() as u32;
        let err = (fp)(self.handle, deferred_operation, info_count, p_infos.as_ptr());
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_micromap_ext(
        &self,
        micromap: vk::MicromapEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_micromap_ext
            .expect("vkDestroyMicromapEXT is not loaded");
        (fp)(self.handle, micromap, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_copy_micromap_ext(&self, command_buffer: vk::CommandBuffer, p_info: &vk::CopyMicromapInfoEXT) {
        let fp = self
            .fp_cmd_copy_micromap_ext
            .expect("vkCmdCopyMicromapEXT is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_micromap_ext(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyMicromapInfoEXT,
    ) -> Result<vk::Result> {
        let fp = self.fp_copy_micromap_ext.expect("vkCopyMicromapEXT is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_copy_micromap_to_memory_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::CopyMicromapToMemoryInfoEXT,
    ) {
        let fp = self
            .fp_cmd_copy_micromap_to_memory_ext
            .expect("vkCmdCopyMicromapToMemoryEXT is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_micromap_to_memory_ext(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyMicromapToMemoryInfoEXT,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_copy_micromap_to_memory_ext
            .expect("vkCopyMicromapToMemoryEXT is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_copy_memory_to_micromap_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_info: &vk::CopyMemoryToMicromapInfoEXT,
    ) {
        let fp = self
            .fp_cmd_copy_memory_to_micromap_ext
            .expect("vkCmdCopyMemoryToMicromapEXT is not loaded");
        (fp)(command_buffer, p_info)
    }
    pub unsafe fn copy_memory_to_micromap_ext(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        p_info: &vk::CopyMemoryToMicromapInfoEXT,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_copy_memory_to_micromap_ext
            .expect("vkCopyMemoryToMicromapEXT is not loaded");
        let err = (fp)(self.handle, deferred_operation, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::OPERATION_DEFERRED_KHR | vk::Result::OPERATION_NOT_DEFERRED_KHR => {
                Ok(err)
            }
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_write_micromaps_properties_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_micromaps: &[vk::MicromapEXT],
        query_type: vk::QueryType,
        query_pool: vk::QueryPool,
        first_query: u32,
    ) {
        let fp = self
            .fp_cmd_write_micromaps_properties_ext
            .expect("vkCmdWriteMicromapsPropertiesEXT is not loaded");
        let micromap_count = p_micromaps.len() as u32;
        (fp)(
            command_buffer,
            micromap_count,
            p_micromaps.as_ptr(),
            query_type,
            query_pool,
            first_query,
        )
    }
    pub unsafe fn write_micromaps_properties_ext(
        &self,
        p_micromaps: &[vk::MicromapEXT],
        query_type: vk::QueryType,
        p_data: &mut [u8],
        stride: usize,
    ) -> Result<()> {
        let fp = self
            .fp_write_micromaps_properties_ext
            .expect("vkWriteMicromapsPropertiesEXT is not loaded");
        let micromap_count = p_micromaps.len() as u32;
        let data_size = p_data.len();
        let err = (fp)(
            self.handle,
            micromap_count,
            p_micromaps.as_ptr(),
            query_type,
            data_size,
            p_data.as_mut_ptr() as *mut _,
            stride,
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_micromap_compatibility_ext(
        &self,
        p_version_info: &vk::MicromapVersionInfoEXT,
    ) -> vk::AccelerationStructureCompatibilityKHR {
        let fp = self
            .fp_get_device_micromap_compatibility_ext
            .expect("vkGetDeviceMicromapCompatibilityEXT is not loaded");
        let mut p_compatibility = MaybeUninit::<_>::uninit();
        (fp)(self.handle, p_version_info, p_compatibility.as_mut_ptr());
        p_compatibility.assume_init()
    }
    pub unsafe fn get_micromap_build_sizes_ext(
        &self,
        build_type: vk::AccelerationStructureBuildTypeKHR,
        p_build_info: &vk::MicromapBuildInfoEXT,
        p_size_info: &mut vk::MicromapBuildSizesInfoEXT,
    ) {
        let fp = self
            .fp_get_micromap_build_sizes_ext
            .expect("vkGetMicromapBuildSizesEXT is not loaded");
        (fp)(self.handle, build_type, p_build_info, p_size_info)
    }
    pub unsafe fn get_shader_module_identifier_ext(
        &self,
        shader_module: vk::ShaderModule,
        p_identifier: &mut vk::ShaderModuleIdentifierEXT,
    ) {
        let fp = self
            .fp_get_shader_module_identifier_ext
            .expect("vkGetShaderModuleIdentifierEXT is not loaded");
        (fp)(self.handle, shader_module, p_identifier)
    }
    pub unsafe fn get_shader_module_create_info_identifier_ext(
        &self,
        p_create_info: &vk::ShaderModuleCreateInfo,
        p_identifier: &mut vk::ShaderModuleIdentifierEXT,
    ) {
        let fp = self
            .fp_get_shader_module_create_info_identifier_ext
            .expect("vkGetShaderModuleCreateInfoIdentifierEXT is not loaded");
        (fp)(self.handle, p_create_info, p_identifier)
    }
    pub unsafe fn get_image_subresource_layout2(
        &self,
        image: vk::Image,
        p_subresource: &vk::ImageSubresource2,
        p_layout: &mut vk::SubresourceLayout2,
    ) {
        let fp = self
            .fp_get_image_subresource_layout2
            .expect("vkGetImageSubresourceLayout2 is not loaded");
        (fp)(self.handle, image, p_subresource, p_layout)
    }
    pub unsafe fn get_image_subresource_layout2_khr(
        &self,
        image: vk::Image,
        p_subresource: &vk::ImageSubresource2,
        p_layout: &mut vk::SubresourceLayout2,
    ) {
        let fp = self
            .fp_get_image_subresource_layout2
            .expect("vkGetImageSubresourceLayout2KHR is not loaded");
        (fp)(self.handle, image, p_subresource, p_layout)
    }
    pub unsafe fn get_image_subresource_layout2_ext(
        &self,
        image: vk::Image,
        p_subresource: &vk::ImageSubresource2,
        p_layout: &mut vk::SubresourceLayout2,
    ) {
        let fp = self
            .fp_get_image_subresource_layout2
            .expect("vkGetImageSubresourceLayout2EXT is not loaded");
        (fp)(self.handle, image, p_subresource, p_layout)
    }
    pub unsafe fn get_pipeline_properties_ext(
        &self,
        p_pipeline_info: &vk::PipelineInfoEXT,
    ) -> Result<vk::BaseOutStructure> {
        let fp = self
            .fp_get_pipeline_properties_ext
            .expect("vkGetPipelinePropertiesEXT is not loaded");
        let mut p_pipeline_properties = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_pipeline_info, p_pipeline_properties.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_pipeline_properties.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn export_metal_objects_ext(&self, p_metal_objects_info: &mut vk::ExportMetalObjectsInfoEXT) {
        let fp = self
            .fp_export_metal_objects_ext
            .expect("vkExportMetalObjectsEXT is not loaded");
        (fp)(self.handle, p_metal_objects_info)
    }
    pub unsafe fn cmd_bind_tile_memory_qcom(
        &self,
        command_buffer: vk::CommandBuffer,
        p_tile_memory_bind_info: Option<&vk::TileMemoryBindInfoQCOM>,
    ) {
        let fp = self
            .fp_cmd_bind_tile_memory_qcom
            .expect("vkCmdBindTileMemoryQCOM is not loaded");
        (fp)(command_buffer, p_tile_memory_bind_info.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_framebuffer_tile_properties_qcom(
        &self,
        framebuffer: vk::Framebuffer,
        p_properties_count: &mut u32,
        p_properties: *mut vk::TilePropertiesQCOM,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_framebuffer_tile_properties_qcom
            .expect("vkGetFramebufferTilePropertiesQCOM is not loaded");
        let err = (fp)(self.handle, framebuffer, p_properties_count, p_properties);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_dynamic_rendering_tile_properties_qcom(
        &self,
        p_rendering_info: &vk::RenderingInfo,
        p_properties: &mut vk::TilePropertiesQCOM,
    ) -> Result<()> {
        let fp = self
            .fp_get_dynamic_rendering_tile_properties_qcom
            .expect("vkGetDynamicRenderingTilePropertiesQCOM is not loaded");
        let err = (fp)(self.handle, p_rendering_info, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_optical_flow_image_formats_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        p_optical_flow_image_format_info: &vk::OpticalFlowImageFormatInfoNV,
        p_format_count: &mut u32,
        p_image_format_properties: *mut vk::OpticalFlowImageFormatPropertiesNV,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_optical_flow_image_formats_nv
            .expect("vkGetPhysicalDeviceOpticalFlowImageFormatsNV is not loaded");
        let err = (fp)(
            physical_device,
            p_optical_flow_image_format_info,
            p_format_count,
            p_image_format_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_optical_flow_image_formats_nv_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        p_optical_flow_image_format_info: &vk::OpticalFlowImageFormatInfoNV,
    ) -> Result<Vec<vk::OpticalFlowImageFormatPropertiesNV>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_optical_flow_image_formats_nv(
                physical_device,
                p_optical_flow_image_format_info,
                len,
                ptr,
            )
        })
    }
    pub unsafe fn create_optical_flow_session_nv(
        &self,
        p_create_info: &vk::OpticalFlowSessionCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::OpticalFlowSessionNV> {
        let fp = self
            .fp_create_optical_flow_session_nv
            .expect("vkCreateOpticalFlowSessionNV is not loaded");
        let mut p_session = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_session.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_session.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_optical_flow_session_nv(
        &self,
        session: vk::OpticalFlowSessionNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_optical_flow_session_nv
            .expect("vkDestroyOpticalFlowSessionNV is not loaded");
        (fp)(self.handle, session, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn bind_optical_flow_session_image_nv(
        &self,
        session: vk::OpticalFlowSessionNV,
        binding_point: vk::OpticalFlowSessionBindingPointNV,
        view: vk::ImageView,
        layout: vk::ImageLayout,
    ) -> Result<()> {
        let fp = self
            .fp_bind_optical_flow_session_image_nv
            .expect("vkBindOpticalFlowSessionImageNV is not loaded");
        let err = (fp)(self.handle, session, binding_point, view, layout);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_optical_flow_execute_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        session: vk::OpticalFlowSessionNV,
        p_execute_info: &vk::OpticalFlowExecuteInfoNV,
    ) {
        let fp = self
            .fp_cmd_optical_flow_execute_nv
            .expect("vkCmdOpticalFlowExecuteNV is not loaded");
        (fp)(command_buffer, session, p_execute_info)
    }
    pub unsafe fn get_device_fault_info_ext(
        &self,
        p_fault_counts: &mut vk::DeviceFaultCountsEXT,
        p_fault_info: Option<&mut vk::DeviceFaultInfoEXT>,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_device_fault_info_ext
            .expect("vkGetDeviceFaultInfoEXT is not loaded");
        let err = (fp)(self.handle, p_fault_counts, p_fault_info.map_or(ptr::null_mut(), |r| r));
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_set_depth_bias2_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_depth_bias_info: &vk::DepthBiasInfoEXT,
    ) {
        let fp = self
            .fp_cmd_set_depth_bias2_ext
            .expect("vkCmdSetDepthBias2EXT is not loaded");
        (fp)(command_buffer, p_depth_bias_info)
    }
    pub unsafe fn release_swapchain_images_khr(
        &self,
        p_release_info: &vk::ReleaseSwapchainImagesInfoKHR,
    ) -> Result<()> {
        let fp = self
            .fp_release_swapchain_images_khr
            .expect("vkReleaseSwapchainImagesKHR is not loaded");
        let err = (fp)(self.handle, p_release_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn release_swapchain_images_ext(
        &self,
        p_release_info: &vk::ReleaseSwapchainImagesInfoKHR,
    ) -> Result<()> {
        let fp = self
            .fp_release_swapchain_images_khr
            .expect("vkReleaseSwapchainImagesEXT is not loaded");
        let err = (fp)(self.handle, p_release_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_image_subresource_layout(
        &self,
        p_info: &vk::DeviceImageSubresourceInfo,
        p_layout: &mut vk::SubresourceLayout2,
    ) {
        let fp = self
            .fp_get_device_image_subresource_layout
            .expect("vkGetDeviceImageSubresourceLayout is not loaded");
        (fp)(self.handle, p_info, p_layout)
    }
    pub unsafe fn get_device_image_subresource_layout_khr(
        &self,
        p_info: &vk::DeviceImageSubresourceInfo,
        p_layout: &mut vk::SubresourceLayout2,
    ) {
        let fp = self
            .fp_get_device_image_subresource_layout
            .expect("vkGetDeviceImageSubresourceLayoutKHR is not loaded");
        (fp)(self.handle, p_info, p_layout)
    }
    pub unsafe fn map_memory2(&self, p_memory_map_info: &vk::MemoryMapInfo) -> Result<*mut c_void> {
        let fp = self.fp_map_memory2.expect("vkMapMemory2 is not loaded");
        let mut pp_data = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_memory_map_info, pp_data.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(pp_data.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn map_memory2_khr(&self, p_memory_map_info: &vk::MemoryMapInfo) -> Result<*mut c_void> {
        let fp = self.fp_map_memory2.expect("vkMapMemory2KHR is not loaded");
        let mut pp_data = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_memory_map_info, pp_data.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(pp_data.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn unmap_memory2(&self, p_memory_unmap_info: &vk::MemoryUnmapInfo) -> Result<()> {
        let fp = self.fp_unmap_memory2.expect("vkUnmapMemory2 is not loaded");
        let err = (fp)(self.handle, p_memory_unmap_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn unmap_memory2_khr(&self, p_memory_unmap_info: &vk::MemoryUnmapInfo) -> Result<()> {
        let fp = self.fp_unmap_memory2.expect("vkUnmapMemory2KHR is not loaded");
        let err = (fp)(self.handle, p_memory_unmap_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_shaders_ext(
        &self,
        p_create_infos: &[vk::ShaderCreateInfoEXT],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_shaders: &mut [vk::ShaderEXT],
    ) -> Result<vk::Result> {
        let fp = self.fp_create_shaders_ext.expect("vkCreateShadersEXT is not loaded");
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_shaders.len() as u32);
        let err = (fp)(
            self.handle,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_shaders.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPATIBLE_SHADER_BINARY_EXT => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_shaders_ext_single(
        &self,
        p_create_infos: &vk::ShaderCreateInfoEXT,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::ShaderEXT)> {
        let mut p_shaders = Default::default();
        self.create_shaders_ext(
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_shaders),
        )
        .map(|res| (res, p_shaders))
    }
    pub unsafe fn destroy_shader_ext(&self, shader: vk::ShaderEXT, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_shader_ext.expect("vkDestroyShaderEXT is not loaded");
        (fp)(self.handle, shader, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_shader_binary_data_ext(
        &self,
        shader: vk::ShaderEXT,
        p_data_size: &mut usize,
        p_data: *mut c_void,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_shader_binary_data_ext
            .expect("vkGetShaderBinaryDataEXT is not loaded");
        let err = (fp)(self.handle, shader, p_data_size, p_data);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_bind_shaders_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_stages: &[vk::ShaderStageFlags],
        p_shaders: Option<&[vk::ShaderEXT]>,
    ) {
        let fp = self.fp_cmd_bind_shaders_ext.expect("vkCmdBindShadersEXT is not loaded");
        let stage_count = p_stages.len() as u32;
        if let Some(s) = p_shaders {
            assert_eq!(stage_count, s.len() as u32);
        }
        (fp)(
            command_buffer,
            stage_count,
            p_stages.as_ptr(),
            p_shaders.map_or(ptr::null(), |r| r.as_ptr()),
        )
    }
    pub unsafe fn set_swapchain_present_timing_queue_size_ext(
        &self,
        swapchain: vk::SwapchainKHR,
        size: u32,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_set_swapchain_present_timing_queue_size_ext
            .expect("vkSetSwapchainPresentTimingQueueSizeEXT is not loaded");
        let err = (fp)(self.handle, swapchain, size);
        match err {
            vk::Result::SUCCESS | vk::Result::NOT_READY => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_swapchain_timing_properties_ext(
        &self,
        swapchain: vk::SwapchainKHR,
        p_swapchain_timing_properties: &mut vk::SwapchainTimingPropertiesEXT,
        p_swapchain_timing_properties_counter: Option<&mut u64>,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_swapchain_timing_properties_ext
            .expect("vkGetSwapchainTimingPropertiesEXT is not loaded");
        let err = (fp)(
            self.handle,
            swapchain,
            p_swapchain_timing_properties,
            p_swapchain_timing_properties_counter.map_or(ptr::null_mut(), |r| r),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::NOT_READY => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_swapchain_time_domain_properties_ext(
        &self,
        swapchain: vk::SwapchainKHR,
        p_swapchain_time_domain_properties: &mut vk::SwapchainTimeDomainPropertiesEXT,
        p_time_domains_counter: Option<&mut u64>,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_swapchain_time_domain_properties_ext
            .expect("vkGetSwapchainTimeDomainPropertiesEXT is not loaded");
        let err = (fp)(
            self.handle,
            swapchain,
            p_swapchain_time_domain_properties,
            p_time_domains_counter.map_or(ptr::null_mut(), |r| r),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_past_presentation_timing_ext(
        &self,
        p_past_presentation_timing_info: &vk::PastPresentationTimingInfoEXT,
        p_past_presentation_timing_properties: &mut vk::PastPresentationTimingPropertiesEXT,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_past_presentation_timing_ext
            .expect("vkGetPastPresentationTimingEXT is not loaded");
        let err = (fp)(
            self.handle,
            p_past_presentation_timing_info,
            p_past_presentation_timing_properties,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_cooperative_matrix_properties_khr(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::CooperativeMatrixPropertiesKHR,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_cooperative_matrix_properties_khr
            .expect("vkGetPhysicalDeviceCooperativeMatrixPropertiesKHR is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_cooperative_matrix_properties_khr_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::CooperativeMatrixPropertiesKHR>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_cooperative_matrix_properties_khr(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_execution_graph_pipeline_scratch_size_amdx(
        &self,
        execution_graph: vk::Pipeline,
        p_size_info: &mut vk::ExecutionGraphPipelineScratchSizeAMDX,
    ) -> Result<()> {
        let fp = self
            .fp_get_execution_graph_pipeline_scratch_size_amdx
            .expect("vkGetExecutionGraphPipelineScratchSizeAMDX is not loaded");
        let err = (fp)(self.handle, execution_graph, p_size_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_execution_graph_pipeline_node_index_amdx(
        &self,
        execution_graph: vk::Pipeline,
        p_node_info: &vk::PipelineShaderStageNodeCreateInfoAMDX,
    ) -> Result<u32> {
        let fp = self
            .fp_get_execution_graph_pipeline_node_index_amdx
            .expect("vkGetExecutionGraphPipelineNodeIndexAMDX is not loaded");
        let mut p_node_index = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, execution_graph, p_node_info, p_node_index.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_node_index.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn create_execution_graph_pipelines_amdx(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::ExecutionGraphPipelineCreateInfoAMDX],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_create_execution_graph_pipelines_amdx
            .expect("vkCreateExecutionGraphPipelinesAMDX is not loaded");
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_execution_graph_pipelines_amdx_single(
        &self,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::ExecutionGraphPipelineCreateInfoAMDX,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_execution_graph_pipelines_amdx(
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn cmd_initialize_graph_scratch_memory_amdx(
        &self,
        command_buffer: vk::CommandBuffer,
        execution_graph: vk::Pipeline,
        scratch: vk::DeviceAddress,
        scratch_size: vk::DeviceSize,
    ) {
        let fp = self
            .fp_cmd_initialize_graph_scratch_memory_amdx
            .expect("vkCmdInitializeGraphScratchMemoryAMDX is not loaded");
        (fp)(command_buffer, execution_graph, scratch, scratch_size)
    }
    pub unsafe fn cmd_dispatch_graph_amdx(
        &self,
        command_buffer: vk::CommandBuffer,
        scratch: vk::DeviceAddress,
        scratch_size: vk::DeviceSize,
        p_count_info: &vk::DispatchGraphCountInfoAMDX,
    ) {
        let fp = self
            .fp_cmd_dispatch_graph_amdx
            .expect("vkCmdDispatchGraphAMDX is not loaded");
        (fp)(command_buffer, scratch, scratch_size, p_count_info)
    }
    pub unsafe fn cmd_dispatch_graph_indirect_amdx(
        &self,
        command_buffer: vk::CommandBuffer,
        scratch: vk::DeviceAddress,
        scratch_size: vk::DeviceSize,
        p_count_info: &vk::DispatchGraphCountInfoAMDX,
    ) {
        let fp = self
            .fp_cmd_dispatch_graph_indirect_amdx
            .expect("vkCmdDispatchGraphIndirectAMDX is not loaded");
        (fp)(command_buffer, scratch, scratch_size, p_count_info)
    }
    pub unsafe fn cmd_dispatch_graph_indirect_count_amdx(
        &self,
        command_buffer: vk::CommandBuffer,
        scratch: vk::DeviceAddress,
        scratch_size: vk::DeviceSize,
        count_info: vk::DeviceAddress,
    ) {
        let fp = self
            .fp_cmd_dispatch_graph_indirect_count_amdx
            .expect("vkCmdDispatchGraphIndirectCountAMDX is not loaded");
        (fp)(command_buffer, scratch, scratch_size, count_info)
    }
    pub unsafe fn cmd_bind_descriptor_sets2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_bind_descriptor_sets_info: &vk::BindDescriptorSetsInfo,
    ) {
        let fp = self
            .fp_cmd_bind_descriptor_sets2
            .expect("vkCmdBindDescriptorSets2 is not loaded");
        (fp)(command_buffer, p_bind_descriptor_sets_info)
    }
    pub unsafe fn cmd_bind_descriptor_sets2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_bind_descriptor_sets_info: &vk::BindDescriptorSetsInfo,
    ) {
        let fp = self
            .fp_cmd_bind_descriptor_sets2
            .expect("vkCmdBindDescriptorSets2KHR is not loaded");
        (fp)(command_buffer, p_bind_descriptor_sets_info)
    }
    pub unsafe fn cmd_push_constants2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_constants_info: &vk::PushConstantsInfo,
    ) {
        let fp = self.fp_cmd_push_constants2.expect("vkCmdPushConstants2 is not loaded");
        (fp)(command_buffer, p_push_constants_info)
    }
    pub unsafe fn cmd_push_constants2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_constants_info: &vk::PushConstantsInfo,
    ) {
        let fp = self
            .fp_cmd_push_constants2
            .expect("vkCmdPushConstants2KHR is not loaded");
        (fp)(command_buffer, p_push_constants_info)
    }
    pub unsafe fn cmd_push_descriptor_set2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_descriptor_set_info: &vk::PushDescriptorSetInfo,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set2
            .expect("vkCmdPushDescriptorSet2 is not loaded");
        (fp)(command_buffer, p_push_descriptor_set_info)
    }
    pub unsafe fn cmd_push_descriptor_set2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_descriptor_set_info: &vk::PushDescriptorSetInfo,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set2
            .expect("vkCmdPushDescriptorSet2KHR is not loaded");
        (fp)(command_buffer, p_push_descriptor_set_info)
    }
    pub unsafe fn cmd_push_descriptor_set_with_template2(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_descriptor_set_with_template_info: &vk::PushDescriptorSetWithTemplateInfo,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set_with_template2
            .expect("vkCmdPushDescriptorSetWithTemplate2 is not loaded");
        (fp)(command_buffer, p_push_descriptor_set_with_template_info)
    }
    pub unsafe fn cmd_push_descriptor_set_with_template2_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_push_descriptor_set_with_template_info: &vk::PushDescriptorSetWithTemplateInfo,
    ) {
        let fp = self
            .fp_cmd_push_descriptor_set_with_template2
            .expect("vkCmdPushDescriptorSetWithTemplate2KHR is not loaded");
        (fp)(command_buffer, p_push_descriptor_set_with_template_info)
    }
    pub unsafe fn cmd_set_descriptor_buffer_offsets2_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_set_descriptor_buffer_offsets_info: &vk::SetDescriptorBufferOffsetsInfoEXT,
    ) {
        let fp = self
            .fp_cmd_set_descriptor_buffer_offsets2_ext
            .expect("vkCmdSetDescriptorBufferOffsets2EXT is not loaded");
        (fp)(command_buffer, p_set_descriptor_buffer_offsets_info)
    }
    pub unsafe fn cmd_bind_descriptor_buffer_embedded_samplers2_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_bind_descriptor_buffer_embedded_samplers_info: &vk::BindDescriptorBufferEmbeddedSamplersInfoEXT,
    ) {
        let fp = self
            .fp_cmd_bind_descriptor_buffer_embedded_samplers2_ext
            .expect("vkCmdBindDescriptorBufferEmbeddedSamplers2EXT is not loaded");
        (fp)(command_buffer, p_bind_descriptor_buffer_embedded_samplers_info)
    }
    pub unsafe fn set_latency_sleep_mode_nv(
        &self,
        swapchain: vk::SwapchainKHR,
        p_sleep_mode_info: &vk::LatencySleepModeInfoNV,
    ) -> Result<()> {
        let fp = self
            .fp_set_latency_sleep_mode_nv
            .expect("vkSetLatencySleepModeNV is not loaded");
        let err = (fp)(self.handle, swapchain, p_sleep_mode_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn latency_sleep_nv(
        &self,
        swapchain: vk::SwapchainKHR,
        p_sleep_info: &vk::LatencySleepInfoNV,
    ) -> Result<()> {
        let fp = self.fp_latency_sleep_nv.expect("vkLatencySleepNV is not loaded");
        let err = (fp)(self.handle, swapchain, p_sleep_info);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn set_latency_marker_nv(
        &self,
        swapchain: vk::SwapchainKHR,
        p_latency_marker_info: &vk::SetLatencyMarkerInfoNV,
    ) {
        let fp = self
            .fp_set_latency_marker_nv
            .expect("vkSetLatencyMarkerNV is not loaded");
        (fp)(self.handle, swapchain, p_latency_marker_info)
    }
    pub unsafe fn get_latency_timings_nv(
        &self,
        swapchain: vk::SwapchainKHR,
        p_latency_marker_info: &mut vk::GetLatencyMarkerInfoNV,
    ) {
        let fp = self
            .fp_get_latency_timings_nv
            .expect("vkGetLatencyTimingsNV is not loaded");
        (fp)(self.handle, swapchain, p_latency_marker_info)
    }
    pub unsafe fn queue_notify_out_of_band_nv(
        &self,
        queue: vk::Queue,
        p_queue_type_info: &vk::OutOfBandQueueTypeInfoNV,
    ) {
        let fp = self
            .fp_queue_notify_out_of_band_nv
            .expect("vkQueueNotifyOutOfBandNV is not loaded");
        (fp)(queue, p_queue_type_info)
    }
    pub unsafe fn cmd_set_rendering_attachment_locations(
        &self,
        command_buffer: vk::CommandBuffer,
        p_location_info: &vk::RenderingAttachmentLocationInfo,
    ) {
        let fp = self
            .fp_cmd_set_rendering_attachment_locations
            .expect("vkCmdSetRenderingAttachmentLocations is not loaded");
        (fp)(command_buffer, p_location_info)
    }
    pub unsafe fn cmd_set_rendering_attachment_locations_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_location_info: &vk::RenderingAttachmentLocationInfo,
    ) {
        let fp = self
            .fp_cmd_set_rendering_attachment_locations
            .expect("vkCmdSetRenderingAttachmentLocationsKHR is not loaded");
        (fp)(command_buffer, p_location_info)
    }
    pub unsafe fn cmd_set_rendering_input_attachment_indices(
        &self,
        command_buffer: vk::CommandBuffer,
        p_input_attachment_index_info: &vk::RenderingInputAttachmentIndexInfo,
    ) {
        let fp = self
            .fp_cmd_set_rendering_input_attachment_indices
            .expect("vkCmdSetRenderingInputAttachmentIndices is not loaded");
        (fp)(command_buffer, p_input_attachment_index_info)
    }
    pub unsafe fn cmd_set_rendering_input_attachment_indices_khr(
        &self,
        command_buffer: vk::CommandBuffer,
        p_input_attachment_index_info: &vk::RenderingInputAttachmentIndexInfo,
    ) {
        let fp = self
            .fp_cmd_set_rendering_input_attachment_indices
            .expect("vkCmdSetRenderingInputAttachmentIndicesKHR is not loaded");
        (fp)(command_buffer, p_input_attachment_index_info)
    }
    pub unsafe fn cmd_set_depth_clamp_range_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        depth_clamp_mode: vk::DepthClampModeEXT,
        p_depth_clamp_range: Option<&vk::DepthClampRangeEXT>,
    ) {
        let fp = self
            .fp_cmd_set_depth_clamp_range_ext
            .expect("vkCmdSetDepthClampRangeEXT is not loaded");
        (fp)(
            command_buffer,
            depth_clamp_mode,
            p_depth_clamp_range.map_or(ptr::null(), |r| r),
        )
    }
    pub unsafe fn get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::CooperativeMatrixFlexibleDimensionsPropertiesNV,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv
            .expect("vkGetPhysicalDeviceCooperativeMatrixFlexibleDimensionsPropertiesNV is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
    ) -> Result<Vec<vk::CooperativeMatrixFlexibleDimensionsPropertiesNV>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_cooperative_matrix_flexible_dimensions_properties_nv(physical_device, len, ptr)
        })
    }
    pub unsafe fn get_memory_metal_handle_ext(
        &self,
        p_get_metal_handle_info: &vk::MemoryGetMetalHandleInfoEXT,
    ) -> Result<*mut c_void> {
        let fp = self
            .fp_get_memory_metal_handle_ext
            .expect("vkGetMemoryMetalHandleEXT is not loaded");
        let mut p_handle = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_get_metal_handle_info, p_handle.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_handle.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_metal_handle_properties_ext(
        &self,
        handle_type: vk::ExternalMemoryHandleTypeFlags,
        p_handle: *const c_void,
        p_memory_metal_handle_properties: &mut vk::MemoryMetalHandlePropertiesEXT,
    ) -> Result<()> {
        let fp = self
            .fp_get_memory_metal_handle_properties_ext
            .expect("vkGetMemoryMetalHandlePropertiesEXT is not loaded");
        let err = (fp)(self.handle, handle_type, p_handle, p_memory_metal_handle_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_cooperative_vector_properties_nv(
        &self,
        physical_device: vk::PhysicalDevice,
        p_property_count: &mut u32,
        p_properties: *mut vk::CooperativeVectorPropertiesNV,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_physical_device_cooperative_vector_properties_nv
            .expect("vkGetPhysicalDeviceCooperativeVectorPropertiesNV is not loaded");
        let err = (fp)(physical_device, p_property_count, p_properties);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn convert_cooperative_vector_matrix_nv(
        &self,
        p_info: &vk::ConvertCooperativeVectorMatrixInfoNV,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_convert_cooperative_vector_matrix_nv
            .expect("vkConvertCooperativeVectorMatrixNV is not loaded");
        let err = (fp)(self.handle, p_info);
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_convert_cooperative_vector_matrix_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_infos: &[vk::ConvertCooperativeVectorMatrixInfoNV],
    ) {
        let fp = self
            .fp_cmd_convert_cooperative_vector_matrix_nv
            .expect("vkCmdConvertCooperativeVectorMatrixNV is not loaded");
        let info_count = p_infos.len() as u32;
        (fp)(command_buffer, info_count, p_infos.as_ptr())
    }
    pub unsafe fn cmd_dispatch_tile_qcom(
        &self,
        command_buffer: vk::CommandBuffer,
        p_dispatch_tile_info: &vk::DispatchTileInfoQCOM,
    ) {
        let fp = self
            .fp_cmd_dispatch_tile_qcom
            .expect("vkCmdDispatchTileQCOM is not loaded");
        (fp)(command_buffer, p_dispatch_tile_info)
    }
    pub unsafe fn cmd_begin_per_tile_execution_qcom(
        &self,
        command_buffer: vk::CommandBuffer,
        p_per_tile_begin_info: &vk::PerTileBeginInfoQCOM,
    ) {
        let fp = self
            .fp_cmd_begin_per_tile_execution_qcom
            .expect("vkCmdBeginPerTileExecutionQCOM is not loaded");
        (fp)(command_buffer, p_per_tile_begin_info)
    }
    pub unsafe fn cmd_end_per_tile_execution_qcom(
        &self,
        command_buffer: vk::CommandBuffer,
        p_per_tile_end_info: &vk::PerTileEndInfoQCOM,
    ) {
        let fp = self
            .fp_cmd_end_per_tile_execution_qcom
            .expect("vkCmdEndPerTileExecutionQCOM is not loaded");
        (fp)(command_buffer, p_per_tile_end_info)
    }
    pub unsafe fn create_external_compute_queue_nv(
        &self,
        p_create_info: &vk::ExternalComputeQueueCreateInfoNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::ExternalComputeQueueNV> {
        let fp = self
            .fp_create_external_compute_queue_nv
            .expect("vkCreateExternalComputeQueueNV is not loaded");
        let mut p_external_queue = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_external_queue.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_external_queue.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_external_compute_queue_nv(
        &self,
        external_queue: vk::ExternalComputeQueueNV,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_external_compute_queue_nv
            .expect("vkDestroyExternalComputeQueueNV is not loaded");
        (fp)(self.handle, external_queue, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_external_compute_queue_data_nv(
        &self,
        external_queue: vk::ExternalComputeQueueNV,
        params: &mut vk::ExternalComputeQueueDataParamsNV,
        p_data: *mut c_void,
    ) {
        let fp = self
            .fp_get_external_compute_queue_data_nv
            .expect("vkGetExternalComputeQueueDataNV is not loaded");
        (fp)(external_queue, params, p_data)
    }
    pub unsafe fn create_tensor_arm(
        &self,
        p_create_info: &vk::TensorCreateInfoARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::TensorARM> {
        let fp = self.fp_create_tensor_arm.expect("vkCreateTensorARM is not loaded");
        let mut p_tensor = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_tensor.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_tensor.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_tensor_arm(&self, tensor: vk::TensorARM, p_allocator: Option<&vk::AllocationCallbacks>) {
        let fp = self.fp_destroy_tensor_arm.expect("vkDestroyTensorARM is not loaded");
        (fp)(self.handle, tensor, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn create_tensor_view_arm(
        &self,
        p_create_info: &vk::TensorViewCreateInfoARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::TensorViewARM> {
        let fp = self
            .fp_create_tensor_view_arm
            .expect("vkCreateTensorViewARM is not loaded");
        let mut p_view = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_view.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_view.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_tensor_view_arm(
        &self,
        tensor_view: vk::TensorViewARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_tensor_view_arm
            .expect("vkDestroyTensorViewARM is not loaded");
        (fp)(self.handle, tensor_view, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_tensor_memory_requirements_arm(
        &self,
        p_info: &vk::TensorMemoryRequirementsInfoARM,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_tensor_memory_requirements_arm
            .expect("vkGetTensorMemoryRequirementsARM is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn bind_tensor_memory_arm(&self, p_bind_infos: &[vk::BindTensorMemoryInfoARM]) -> Result<()> {
        let fp = self
            .fp_bind_tensor_memory_arm
            .expect("vkBindTensorMemoryARM is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_device_tensor_memory_requirements_arm(
        &self,
        p_info: &vk::DeviceTensorMemoryRequirementsARM,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_device_tensor_memory_requirements_arm
            .expect("vkGetDeviceTensorMemoryRequirementsARM is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn cmd_copy_tensor_arm(
        &self,
        command_buffer: vk::CommandBuffer,
        p_copy_tensor_info: &vk::CopyTensorInfoARM,
    ) {
        let fp = self.fp_cmd_copy_tensor_arm.expect("vkCmdCopyTensorARM is not loaded");
        (fp)(command_buffer, p_copy_tensor_info)
    }
    pub unsafe fn get_tensor_opaque_capture_descriptor_data_arm(
        &self,
        p_info: &vk::TensorCaptureDescriptorDataInfoARM,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_tensor_opaque_capture_descriptor_data_arm
            .expect("vkGetTensorOpaqueCaptureDescriptorDataARM is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_tensor_view_opaque_capture_descriptor_data_arm(
        &self,
        p_info: &vk::TensorViewCaptureDescriptorDataInfoARM,
        p_data: *mut c_void,
    ) -> Result<()> {
        let fp = self
            .fp_get_tensor_view_opaque_capture_descriptor_data_arm
            .expect("vkGetTensorViewOpaqueCaptureDescriptorDataARM is not loaded");
        let err = (fp)(self.handle, p_info, p_data);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_external_tensor_properties_arm(
        &self,
        physical_device: vk::PhysicalDevice,
        p_external_tensor_info: &vk::PhysicalDeviceExternalTensorInfoARM,
        p_external_tensor_properties: &mut vk::ExternalTensorPropertiesARM,
    ) {
        let fp = self
            .fp_get_physical_device_external_tensor_properties_arm
            .expect("vkGetPhysicalDeviceExternalTensorPropertiesARM is not loaded");
        (fp)(physical_device, p_external_tensor_info, p_external_tensor_properties)
    }
    pub unsafe fn create_data_graph_pipelines_arm(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &[vk::DataGraphPipelineCreateInfoARM],
        p_allocator: Option<&vk::AllocationCallbacks>,
        p_pipelines: &mut [vk::Pipeline],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_create_data_graph_pipelines_arm
            .expect("vkCreateDataGraphPipelinesARM is not loaded");
        let create_info_count = p_create_infos.len() as u32;
        assert_eq!(create_info_count, p_pipelines.len() as u32);
        let err = (fp)(
            self.handle,
            deferred_operation,
            pipeline_cache,
            create_info_count,
            p_create_infos.as_ptr(),
            p_allocator.map_or(ptr::null(), |r| r),
            p_pipelines.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::PIPELINE_COMPILE_REQUIRED => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn create_data_graph_pipelines_arm_single(
        &self,
        deferred_operation: vk::DeferredOperationKHR,
        pipeline_cache: vk::PipelineCache,
        p_create_infos: &vk::DataGraphPipelineCreateInfoARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<(vk::Result, vk::Pipeline)> {
        let mut p_pipelines = Default::default();
        self.create_data_graph_pipelines_arm(
            deferred_operation,
            pipeline_cache,
            slice::from_ref(p_create_infos),
            p_allocator,
            slice::from_mut(&mut p_pipelines),
        )
        .map(|res| (res, p_pipelines))
    }
    pub unsafe fn create_data_graph_pipeline_session_arm(
        &self,
        p_create_info: &vk::DataGraphPipelineSessionCreateInfoARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) -> Result<vk::DataGraphPipelineSessionARM> {
        let fp = self
            .fp_create_data_graph_pipeline_session_arm
            .expect("vkCreateDataGraphPipelineSessionARM is not loaded");
        let mut p_session = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_create_info,
            p_allocator.map_or(ptr::null(), |r| r),
            p_session.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_session.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_data_graph_pipeline_session_bind_point_requirements_arm(
        &self,
        p_info: &vk::DataGraphPipelineSessionBindPointRequirementsInfoARM,
        p_bind_point_requirement_count: &mut u32,
        p_bind_point_requirements: *mut vk::DataGraphPipelineSessionBindPointRequirementARM,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_data_graph_pipeline_session_bind_point_requirements_arm
            .expect("vkGetDataGraphPipelineSessionBindPointRequirementsARM is not loaded");
        let err = (fp)(
            self.handle,
            p_info,
            p_bind_point_requirement_count,
            p_bind_point_requirements,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_data_graph_pipeline_session_bind_point_requirements_arm_to_vec(
        &self,
        p_info: &vk::DataGraphPipelineSessionBindPointRequirementsInfoARM,
    ) -> Result<Vec<vk::DataGraphPipelineSessionBindPointRequirementARM>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_data_graph_pipeline_session_bind_point_requirements_arm(p_info, len, ptr)
        })
    }
    pub unsafe fn get_data_graph_pipeline_session_memory_requirements_arm(
        &self,
        p_info: &vk::DataGraphPipelineSessionMemoryRequirementsInfoARM,
        p_memory_requirements: &mut vk::MemoryRequirements2,
    ) {
        let fp = self
            .fp_get_data_graph_pipeline_session_memory_requirements_arm
            .expect("vkGetDataGraphPipelineSessionMemoryRequirementsARM is not loaded");
        (fp)(self.handle, p_info, p_memory_requirements)
    }
    pub unsafe fn bind_data_graph_pipeline_session_memory_arm(
        &self,
        p_bind_infos: &[vk::BindDataGraphPipelineSessionMemoryInfoARM],
    ) -> Result<()> {
        let fp = self
            .fp_bind_data_graph_pipeline_session_memory_arm
            .expect("vkBindDataGraphPipelineSessionMemoryARM is not loaded");
        let bind_info_count = p_bind_infos.len() as u32;
        let err = (fp)(self.handle, bind_info_count, p_bind_infos.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn destroy_data_graph_pipeline_session_arm(
        &self,
        session: vk::DataGraphPipelineSessionARM,
        p_allocator: Option<&vk::AllocationCallbacks>,
    ) {
        let fp = self
            .fp_destroy_data_graph_pipeline_session_arm
            .expect("vkDestroyDataGraphPipelineSessionARM is not loaded");
        (fp)(self.handle, session, p_allocator.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn cmd_dispatch_data_graph_arm(
        &self,
        command_buffer: vk::CommandBuffer,
        session: vk::DataGraphPipelineSessionARM,
        p_info: Option<&vk::DataGraphPipelineDispatchInfoARM>,
    ) {
        let fp = self
            .fp_cmd_dispatch_data_graph_arm
            .expect("vkCmdDispatchDataGraphARM is not loaded");
        (fp)(command_buffer, session, p_info.map_or(ptr::null(), |r| r))
    }
    pub unsafe fn get_data_graph_pipeline_available_properties_arm(
        &self,
        p_pipeline_info: &vk::DataGraphPipelineInfoARM,
        p_properties_count: &mut u32,
        p_properties: *mut vk::DataGraphPipelinePropertyARM,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_data_graph_pipeline_available_properties_arm
            .expect("vkGetDataGraphPipelineAvailablePropertiesARM is not loaded");
        let err = (fp)(self.handle, p_pipeline_info, p_properties_count, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_data_graph_pipeline_available_properties_arm_to_vec(
        &self,
        p_pipeline_info: &vk::DataGraphPipelineInfoARM,
    ) -> Result<Vec<vk::DataGraphPipelinePropertyARM>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_data_graph_pipeline_available_properties_arm(p_pipeline_info, len, ptr)
        })
    }
    pub unsafe fn get_data_graph_pipeline_properties_arm(
        &self,
        p_pipeline_info: &vk::DataGraphPipelineInfoARM,
        p_properties: &mut [vk::DataGraphPipelinePropertyQueryResultARM],
    ) -> Result<vk::Result> {
        let fp = self
            .fp_get_data_graph_pipeline_properties_arm
            .expect("vkGetDataGraphPipelinePropertiesARM is not loaded");
        let properties_count = p_properties.len() as u32;
        let err = (fp)(
            self.handle,
            p_pipeline_info,
            properties_count,
            p_properties.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn get_data_graph_pipeline_properties_arm_single(
        &self,
        p_pipeline_info: &vk::DataGraphPipelineInfoARM,
    ) -> Result<(vk::Result, vk::DataGraphPipelinePropertyQueryResultARM)> {
        let mut p_properties = Default::default();
        self.get_data_graph_pipeline_properties_arm(p_pipeline_info, slice::from_mut(&mut p_properties))
            .map(|res| (res, p_properties))
    }
    pub unsafe fn get_physical_device_queue_family_data_graph_properties_arm(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        p_queue_family_data_graph_property_count: &mut u32,
        p_queue_family_data_graph_properties: *mut vk::QueueFamilyDataGraphPropertiesARM,
    ) -> Result<EnumerateResult> {
        let fp = self
            .fp_get_physical_device_queue_family_data_graph_properties_arm
            .expect("vkGetPhysicalDeviceQueueFamilyDataGraphPropertiesARM is not loaded");
        let err = (fp)(
            physical_device,
            queue_family_index,
            p_queue_family_data_graph_property_count,
            p_queue_family_data_graph_properties,
        );
        match err {
            vk::Result::SUCCESS => Ok(EnumerateResult::Success),
            vk::Result::INCOMPLETE => Ok(EnumerateResult::Incomplete),
            _ => Err(err),
        }
    }
    pub unsafe fn get_physical_device_queue_family_data_graph_properties_arm_to_vec(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
    ) -> Result<Vec<vk::QueueFamilyDataGraphPropertiesARM>> {
        enumerate_generic_to_vec(|len, ptr| {
            self.get_physical_device_queue_family_data_graph_properties_arm(
                physical_device,
                queue_family_index,
                len,
                ptr,
            )
        })
    }
    pub unsafe fn get_physical_device_queue_family_data_graph_processing_engine_properties_arm(
        &self,
        physical_device: vk::PhysicalDevice,
        p_queue_family_data_graph_processing_engine_info: &vk::PhysicalDeviceQueueFamilyDataGraphProcessingEngineInfoARM,
        p_queue_family_data_graph_processing_engine_properties: &mut vk::QueueFamilyDataGraphProcessingEnginePropertiesARM,
    ) {
        let fp = self
            .fp_get_physical_device_queue_family_data_graph_processing_engine_properties_arm
            .expect("vkGetPhysicalDeviceQueueFamilyDataGraphProcessingEnginePropertiesARM is not loaded");
        (fp)(
            physical_device,
            p_queue_family_data_graph_processing_engine_info,
            p_queue_family_data_graph_processing_engine_properties,
        )
    }
    pub unsafe fn get_native_buffer_properties_ohos(
        &self,
        buffer: &vk::OH_NativeBuffer,
        p_properties: &mut vk::NativeBufferPropertiesOHOS,
    ) -> Result<()> {
        let fp = self
            .fp_get_native_buffer_properties_ohos
            .expect("vkGetNativeBufferPropertiesOHOS is not loaded");
        let err = (fp)(self.handle, buffer, p_properties);
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_memory_native_buffer_ohos(
        &self,
        p_info: &vk::MemoryGetNativeBufferInfoOHOS,
    ) -> Result<*mut vk::OH_NativeBuffer> {
        let fp = self
            .fp_get_memory_native_buffer_ohos
            .expect("vkGetMemoryNativeBufferOHOS is not loaded");
        let mut p_buffer = MaybeUninit::<_>::uninit();
        let err = (fp)(self.handle, p_info, p_buffer.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(p_buffer.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn enumerate_physical_device_queue_family_performance_counters_by_region_arm(
        &self,
        physical_device: vk::PhysicalDevice,
        queue_family_index: u32,
        p_counter_count: &mut u32,
        p_counters: *mut vk::PerformanceCounterARM,
        p_counter_descriptions: *mut vk::PerformanceCounterDescriptionARM,
    ) -> Result<vk::Result> {
        let fp = self
            .fp_enumerate_physical_device_queue_family_performance_counters_by_region_arm
            .expect("vkEnumeratePhysicalDeviceQueueFamilyPerformanceCountersByRegionARM is not loaded");
        let err = (fp)(
            physical_device,
            queue_family_index,
            p_counter_count,
            p_counters,
            p_counter_descriptions,
        );
        match err {
            vk::Result::SUCCESS | vk::Result::INCOMPLETE => Ok(err),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_set_compute_occupancy_priority_nv(
        &self,
        command_buffer: vk::CommandBuffer,
        p_parameters: &vk::ComputeOccupancyPriorityParametersNV,
    ) {
        let fp = self
            .fp_cmd_set_compute_occupancy_priority_nv
            .expect("vkCmdSetComputeOccupancyPriorityNV is not loaded");
        (fp)(command_buffer, p_parameters)
    }
    pub unsafe fn write_sampler_descriptors_ext(
        &self,
        p_samplers: &[vk::SamplerCreateInfo],
        p_descriptors: &[vk::HostAddressRangeEXT],
    ) -> Result<()> {
        let fp = self
            .fp_write_sampler_descriptors_ext
            .expect("vkWriteSamplerDescriptorsEXT is not loaded");
        let sampler_count = p_samplers.len() as u32;
        assert_eq!(sampler_count, p_descriptors.len() as u32);
        let err = (fp)(self.handle, sampler_count, p_samplers.as_ptr(), p_descriptors.as_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn write_resource_descriptors_ext(
        &self,
        p_resources: &[vk::ResourceDescriptorInfoEXT],
        p_descriptors: &[vk::HostAddressRangeEXT],
    ) -> Result<()> {
        let fp = self
            .fp_write_resource_descriptors_ext
            .expect("vkWriteResourceDescriptorsEXT is not loaded");
        let resource_count = p_resources.len() as u32;
        assert_eq!(resource_count, p_descriptors.len() as u32);
        let err = (fp)(
            self.handle,
            resource_count,
            p_resources.as_ptr(),
            p_descriptors.as_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn cmd_bind_sampler_heap_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_bind_info: &vk::BindHeapInfoEXT,
    ) {
        let fp = self
            .fp_cmd_bind_sampler_heap_ext
            .expect("vkCmdBindSamplerHeapEXT is not loaded");
        (fp)(command_buffer, p_bind_info)
    }
    pub unsafe fn cmd_bind_resource_heap_ext(
        &self,
        command_buffer: vk::CommandBuffer,
        p_bind_info: &vk::BindHeapInfoEXT,
    ) {
        let fp = self
            .fp_cmd_bind_resource_heap_ext
            .expect("vkCmdBindResourceHeapEXT is not loaded");
        (fp)(command_buffer, p_bind_info)
    }
    pub unsafe fn cmd_push_data_ext(&self, command_buffer: vk::CommandBuffer, p_push_data_info: &vk::PushDataInfoEXT) {
        let fp = self.fp_cmd_push_data_ext.expect("vkCmdPushDataEXT is not loaded");
        (fp)(command_buffer, p_push_data_info)
    }
    pub unsafe fn register_custom_border_color_ext(
        &self,
        p_border_color: &vk::SamplerCustomBorderColorCreateInfoEXT,
        request_index: bool,
    ) -> Result<u32> {
        let fp = self
            .fp_register_custom_border_color_ext
            .expect("vkRegisterCustomBorderColorEXT is not loaded");
        let mut p_index = MaybeUninit::<_>::uninit();
        let err = (fp)(
            self.handle,
            p_border_color,
            if request_index { vk::TRUE } else { vk::FALSE },
            p_index.as_mut_ptr(),
        );
        match err {
            vk::Result::SUCCESS => Ok(p_index.assume_init()),
            _ => Err(err),
        }
    }
    pub unsafe fn unregister_custom_border_color_ext(&self, index: u32) {
        let fp = self
            .fp_unregister_custom_border_color_ext
            .expect("vkUnregisterCustomBorderColorEXT is not loaded");
        (fp)(self.handle, index)
    }
    pub unsafe fn get_image_opaque_capture_data_ext(
        &self,
        p_images: &[vk::Image],
        p_datas: &mut [vk::HostAddressRangeEXT],
    ) -> Result<()> {
        let fp = self
            .fp_get_image_opaque_capture_data_ext
            .expect("vkGetImageOpaqueCaptureDataEXT is not loaded");
        let image_count = p_images.len() as u32;
        assert_eq!(image_count, p_datas.len() as u32);
        let err = (fp)(self.handle, image_count, p_images.as_ptr(), p_datas.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_image_opaque_capture_data_ext_single(
        &self,
        p_images: &vk::Image,
    ) -> Result<vk::HostAddressRangeEXT> {
        let mut p_datas = Default::default();
        self.get_image_opaque_capture_data_ext(slice::from_ref(p_images), slice::from_mut(&mut p_datas))
            .map(|_| p_datas)
    }
    pub unsafe fn get_physical_device_descriptor_size_ext(
        &self,
        physical_device: vk::PhysicalDevice,
        descriptor_type: vk::DescriptorType,
    ) -> vk::DeviceSize {
        let fp = self
            .fp_get_physical_device_descriptor_size_ext
            .expect("vkGetPhysicalDeviceDescriptorSizeEXT is not loaded");
        (fp)(physical_device, descriptor_type)
    }
    pub unsafe fn get_tensor_opaque_capture_data_arm(
        &self,
        p_tensors: &[vk::TensorARM],
        p_datas: &mut [vk::HostAddressRangeEXT],
    ) -> Result<()> {
        let fp = self
            .fp_get_tensor_opaque_capture_data_arm
            .expect("vkGetTensorOpaqueCaptureDataARM is not loaded");
        let tensor_count = p_tensors.len() as u32;
        assert_eq!(tensor_count, p_datas.len() as u32);
        let err = (fp)(self.handle, tensor_count, p_tensors.as_ptr(), p_datas.as_mut_ptr());
        match err {
            vk::Result::SUCCESS => Ok(()),
            _ => Err(err),
        }
    }
    pub unsafe fn get_tensor_opaque_capture_data_arm_single(
        &self,
        p_tensors: &vk::TensorARM,
    ) -> Result<vk::HostAddressRangeEXT> {
        let mut p_datas = Default::default();
        self.get_tensor_opaque_capture_data_arm(slice::from_ref(p_tensors), slice::from_mut(&mut p_datas))
            .map(|_| p_datas)
    }
}
